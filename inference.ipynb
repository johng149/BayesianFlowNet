{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0387e05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Num processes: 1\n",
      "Using fsdp: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.11/site-packages/datasets/load.py:1461: FutureWarning: The repository for karpathy/tiny_shakespeare contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/karpathy/tiny_shakespeare\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 2637824 parameters\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tracemalloc import start\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import TorchDynamoPlugin\n",
    "\n",
    "from src.datasets.discrete_helper import collate_fn\n",
    "from src.datasets.shakespeare.shakespeare import ShakespeareDataset\n",
    "from src.inference.discrete_inference import bayesian_inference, dis_t\n",
    "from src.nn.models.discrete_model import DiscreteModel\n",
    "from src.tokenizers.byt5.byt5_tokenizer import ByT5Tokenizer as Tokenizer\n",
    "from src.training.checkpoint import CheckpointManager, CheckpointMetadata\n",
    "from src.training.training import TrainingContext, train_discrete_model\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "accelerator = Accelerator(cpu=True)\n",
    "print(f\"Using device: {accelerator.device}\")\n",
    "print(f\"Num processes: {accelerator.num_processes}\")\n",
    "print(\n",
    "    f\"Using fsdp: {hasattr(accelerator.state, 'fsdp_plugin') and accelerator.state.fsdp_plugin is not None}\"\n",
    ")\n",
    "tokenizer = Tokenizer()\n",
    "max_seq_len = 56\n",
    "batch_size = 256\n",
    "train_ds = ShakespeareDataset(tokenizer=tokenizer, max_length=max_seq_len)\n",
    "train_dl = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=3\n",
    ")\n",
    "test_ds = ShakespeareDataset(tokenizer=tokenizer, max_length=max_seq_len)\n",
    "test_dl = torch.utils.data.DataLoader(\n",
    "    test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=3\n",
    ")\n",
    "\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_seq_len\": max_seq_len,\n",
    "    \"K\": tokenizer.vocab_size(),\n",
    "    \"hidden_dim\": 256,\n",
    "    \"num_heads\": 8,\n",
    "    \"layers\": 3,\n",
    "}\n",
    "model = DiscreteModel(**model_kwargs)\n",
    "\n",
    "print(\n",
    "    f\"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters\"\n",
    ")\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    \"lr\": 1e-5,\n",
    "}\n",
    "opt = torch.optim.Adam(\n",
    "    model.parameters(), **optimizer_kwargs  # pyright: ignore[reportArgumentType]\n",
    ")\n",
    "\n",
    "metadata = CheckpointMetadata(\n",
    "    model_kwargs=model_kwargs,\n",
    "    optimizer_kwargs=optimizer_kwargs,\n",
    "    is_fsdp=hasattr(accelerator.state, \"fsdp_plugin\")\n",
    "    and accelerator.state.fsdp_plugin is not None,\n",
    "    num_accelerators=accelerator.num_processes,\n",
    ")\n",
    "\n",
    "checkpoint_name = \"shakespeare_dynamo_f32\"\n",
    "\n",
    "checkpoint_dir = f\"./checkpoint/{checkpoint_name}\"\n",
    "\n",
    "checkpoint_manager = CheckpointManager()\n",
    "checkpoint_manager.prepare(model, opt, accelerator, metadata)\n",
    "checkpoint_manager.load(checkpoint_dir, error_if_not_exists=False)\n",
    "start_epoch = (\n",
    "    checkpoint_manager.metadata.current_epoch if checkpoint_manager.metadata else 0\n",
    ")\n",
    "\n",
    "model, opt = checkpoint_manager.model, checkpoint_manager.optimizer\n",
    "train_dl, test_dl = accelerator.prepare(train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "799872f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = iter(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01688434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Result ---\n",
      "Expected Sequence:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me sp\n",
      "--------------------\n",
      "Generated Sequence:\n",
      "First Citizen:\n",
      "Before we proceeanHfu0\u000eheV,\u0005heacme&}p\n",
      "--- End of Result ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.datasets.discrete_helper import beta_t, theta, y_distribution\n",
    "\n",
    "while True:\n",
    "    # Get a batch of data from the test set\n",
    "    try:\n",
    "        test_data = next(test_iter)\n",
    "    except StopIteration:\n",
    "        test_iter = iter(test_dl)\n",
    "        test_data = next(test_iter)\n",
    "\n",
    "    # Ask user for number of iterations\n",
    "    try:\n",
    "        num_iterations_str = input(\"Enter number of inference iterations (e.g., 100): \")\n",
    "        if not num_iterations_str:\n",
    "            print(\"Defaulting to 100 iterations.\")\n",
    "            num_iterations = 100\n",
    "        else:\n",
    "            num_iterations = int(num_iterations_str)\n",
    "        if num_iterations <= 0:\n",
    "            raise ValueError(\"Number of iterations must be positive.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Invalid input: {e}. Please enter a positive integer.\")\n",
    "        continue\n",
    "\n",
    "    # Perform inference on the first item of the batch\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        x = test_data[\"x\"][:1]  # Take the first sample\n",
    "        t = test_data[\"t\"][:1]\n",
    "        beta_1 = test_data[\"beta_1\"][:1]\n",
    "        \n",
    "        _, seq_len, K = x.shape\n",
    "        total_iterations = torch.ones_like(t) * num_iterations\n",
    "\n",
    "        # Mask to keep the first half of the sequence\n",
    "        indices = torch.arange(seq_len, device=x.device)\n",
    "        mask = (indices < (seq_len // 2)).unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "        # Recreate input from x at t=0\n",
    "        beta_0 = beta_t(beta_1, t * 0)\n",
    "        model_input_acc = theta(y_distribution(beta_0, K, x))\n",
    "\n",
    "        # Prepare x for conditional generation (set 0s to -inf for logits)\n",
    "        x_zero = x.clone().float()\n",
    "        x_zero[x_zero == 0] = float(\"-inf\")\n",
    "\n",
    "        # Iterative generation loop\n",
    "        for i in range(1, num_iterations + 1):\n",
    "            current_iteration = torch.ones_like(t) * i\n",
    "            t_curr = dis_t(current_iteration, total_iterations)\n",
    "            output = model(model_input_acc, t_curr)\n",
    "            \n",
    "            # Apply mask: use original for the first half, model output for the second\n",
    "            output = torch.where(mask, x_zero, output)\n",
    "            \n",
    "            model_input_acc = bayesian_inference(\n",
    "                model_input_acc, output, current_iteration, total_iterations, beta_1\n",
    "            )\n",
    "\n",
    "        # Decode and display results\n",
    "        original_ids = x.squeeze()\n",
    "        generated_ids = model_input_acc.squeeze()\n",
    "\n",
    "        expected_sequence = tokenizer.decode(original_ids)\n",
    "        generated_sequence = tokenizer.decode(generated_ids)\n",
    "\n",
    "        print(\"\\n--- Inference Result ---\")\n",
    "        print(f\"Expected Sequence:\\n{expected_sequence}\")\n",
    "        print(\"-\" * 20)\n",
    "        print(f\"Generated Sequence:\\n{generated_sequence}\")\n",
    "        print(\"--- End of Result ---\\n\")\n",
    "\n",
    "    # Ask to continue\n",
    "    another_run = input(\"Perform another inference? (y/n): \").lower()\n",
    "    if another_run != 'y':\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1b5e3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "         [   -inf,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n",
       "         ...,\n",
       "         [-3.6965, 15.1197, -2.9202,  ..., 15.1056, 15.1407, 15.1035],\n",
       "         [-3.7855, 15.1616, -2.9738,  ..., 15.1814, 15.1272, 15.1405],\n",
       "         [-3.4867, 13.4419, -2.7438,  ..., 15.0606, 13.6649, 14.2573]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7709f822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
