{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a80cf303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"./\"))\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e4c8ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "547dc0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_t(beta_1: Tensor, t: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        beta_1: Maximum possible accuracy (reached when t=1) of shape (batch_size,).\n",
    "        t: A tensor representing the time step, where 1 corresponds to maximum accuracy of shape (batch_size,).\n",
    "    Returns:\n",
    "        Beta value at given time step t\n",
    "    \"\"\"\n",
    "    assert beta_1.ndim == 1, \"beta_1 should be a 1D tensor\"\n",
    "    assert t.ndim == 1, \"t should be a 1D tensor\"\n",
    "    assert beta_1.shape == t.shape, \"beta_1 and t should have the same shape\"\n",
    "    assert torch.all(t >= 0), \"t must be at least 0\"\n",
    "    assert torch.all(t <= 1), \"t must be at most 1\"\n",
    "    return beta_1 * (t ** 2)\n",
    "\n",
    "def y_distribution(beta: Tensor, K: int, kron_x: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        beta: Tensor of accuracy values for each batch of shape (batch_size,).\n",
    "        K: Number of classes (usually vocabulary size etc.)\n",
    "        kron_x: One-hot encoded input tensor of shape (batch_size, seq_len, K).\n",
    "    Returns:\n",
    "        Noisy version of kron_x with the amount of noise controlled\n",
    "        by beta. The shape of the output tensor is the same as kron_x, i.e., (batch_size, seq_len, K).\n",
    "    \"\"\"\n",
    "    beta = beta.view(-1, 1, 1) # allows for broadcasting with reach appropriate batch in kron_x\n",
    "    mean = beta * (K * kron_x - 1)\n",
    "    variance = beta * K\n",
    "    epsilon = torch.normal(0, 1, kron_x.shape, device=kron_x.device)\n",
    "    return mean + variance * epsilon # I know the name `variance` suggests it should be squared, but this works just fine\n",
    "\n",
    "def theta(y: Tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y: Tensor of shape (batch_size, seq_len, K) representing the noisy version of kron_x.\n",
    "    Returns:\n",
    "        Tensor representing the scaled softmax of y, which is the input to the model.\n",
    "    \"\"\"\n",
    "    assert y.ndim == 3, \"y should be a 3D tensor of shape (batch_size, seq_len, K)\"\n",
    "    theta = F.softmax(y, dim=-1)\n",
    "    theta = 2 * theta - 1  # scale to [-1, 1]\n",
    "    return theta\n",
    "\n",
    "def sample_t(batch_size, min_t=1e-6):\n",
    "   return torch.clamp(torch.FloatTensor(batch_size).uniform_(0,1), min=min_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dc1c965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from src.tokenizers.discrete_synthetic.discrete_synthetic_tokenizer import DiscreteSyntheticTokenizer\n",
    "import random\n",
    "\n",
    "class DiscreteSyntheticDataset(Dataset):\n",
    "    def __init__(self, tokenizer: DiscreteSyntheticTokenizer, length: int = 32, tokenized_length: int = 32, mini: int = 0, maxi: int = 100, min_t: float = 1e-6, beta_1: float = 4.0):\n",
    "        self.length = length\n",
    "        self.tokenized_length = tokenized_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        self.min_t = min_t\n",
    "        self.beta_1 = torch.tensor([beta_1])\n",
    "\n",
    "    def generate_sequence(self):\n",
    "        start = random.randint(self.mini, self.maxi - self.length)\n",
    "        end = start + self.length\n",
    "        acc = \"\"\n",
    "        for i in range(start, end+1):\n",
    "            for c in str(i):\n",
    "                acc += \" \" + c\n",
    "            acc += \" ,\"\n",
    "        tokenized = self.tokenizer.encode(acc)\n",
    "        return tokenized[:self.tokenized_length - random.randint(1, 2)] # add jitter to mimic real data variability\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 10000\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = F.one_hot(self.generate_sequence(), num_classes=self.tokenizer.vocab_size())\n",
    "        t = sample_t(1, self.min_t)\n",
    "        beta = beta_t(self.beta_1, t)\n",
    "        # y = y_distribution(beta, self.tokenizer.vocab_size(), seq)\n",
    "        # theta_y = theta(y)\n",
    "        return {\n",
    "            \"x\": seq,\n",
    "            \"t\": t,\n",
    "            \"beta\": beta,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "53eb734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tokenizers.discrete_synthetic.discrete_synthetic_tokenizer import DiscreteSyntheticTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e163c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = DiscreteSyntheticTokenizer()\n",
    "ds = DiscreteSyntheticDataset(tk, length=4, tokenized_length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f0296ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3e515756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collage_fn(batch):\n",
    "    \"\"\"\n",
    "    We expect batch to be a list of dictionaries with keys 'x', 't', and 'beta'.\n",
    "    \"\"\"\n",
    "\n",
    "    # first, we handle truncating the `x` tensors as needed\n",
    "    x = [item['x'] for item in batch]\n",
    "    min_length = min(tensor.shape[0] for tensor in x)\n",
    "    x = [tensor[:min_length] for tensor in x]\n",
    "\n",
    "    # and now we stack the x tensors and concatenate the t and beta tensors\n",
    "    x = torch.stack(x, dim=0)\n",
    "    t = torch.cat([item['t'] for item in batch], dim=0)\n",
    "    beta = torch.cat([item['beta'] for item in batch], dim=0)\n",
    "\n",
    "    # and now we use this to calculate the theta\n",
    "    y = y_distribution(beta, x.shape[-1], x)\n",
    "    theta_y = theta(y)\n",
    "\n",
    "    return {\n",
    "        \"x\": x,\n",
    "        \"t\": t,\n",
    "        \"theta\": theta_y\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0ed1852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=2, shuffle=True, collate_fn=collage_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "241db78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "14110664",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6b920f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "          [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]]),\n",
       " 't': tensor([0.5281, 0.3546]),\n",
       " 'theta': tensor([[[-0.9997, -1.0000, -1.0000, -1.0000, -1.0000, -0.7456, -0.9963,\n",
       "           -1.0000,  0.7416, -1.0000, -1.0000],\n",
       "          [-1.0000, -0.9826, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "           -1.0000, -1.0000, -1.0000,  0.9826]],\n",
       " \n",
       "         [[-1.0000,  0.7658, -1.0000, -0.9959, -0.8931, -1.0000, -0.9996,\n",
       "           -0.8772, -1.0000, -1.0000, -1.0000],\n",
       "          [-1.0000, -0.9991, -1.0000, -1.0000, -0.9997, -1.0000, -1.0000,\n",
       "           -0.9997, -1.0000, -1.0000,  0.9985]]])}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
