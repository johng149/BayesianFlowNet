{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08194304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f37b22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(\n",
    "    beta_1: Tensor,\n",
    "    t: Tensor,\n",
    "    target: Tensor,\n",
    "    model_output_probs: Tensor | None = None,\n",
    "    model_output_logits: Tensor | None = None,\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        beta_1: Maximum possible accuracy (reached when t=1) of shape (batch_size,).\n",
    "        t: A tensor representing the time step (batch_size,).\n",
    "        target: Target tensor of shape (batch_size, seq_len, K).\n",
    "        model_output_probs: Model output probabilities of shape (batch_size, seq_len, K). If None, model_output_logits must be provided.\n",
    "        model_output_logits: Model output logits of shape (batch_size, seq_len, K). If None, model_output_probs must be provided.\n",
    "    Returns:\n",
    "        Loss value\n",
    "\n",
    "    Must provide either model_output_probs or model_output_logits, but not both.\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        model_output_probs is None or model_output_logits is None\n",
    "    ), \"Must provide either model_output_probs or model_output_logits, but not both\"\n",
    "    assert (\n",
    "        model_output_probs is None or model_output_probs.shape == target.shape\n",
    "    ), \"model_output_probs must have the same shape as target if provided\"\n",
    "    assert (\n",
    "        model_output_logits is None or model_output_logits.shape == target.shape\n",
    "    ), \"model_output_logits must have the same shape as target if provided\"\n",
    "\n",
    "    batch_size, seq_len, K = target.shape\n",
    "    model_output = (\n",
    "        model_output_probs\n",
    "        if model_output_probs is not None\n",
    "        else torch.softmax(model_output_logits, dim=-1)\n",
    "    )\n",
    "    result = torch.sum(\n",
    "        K * beta_1 * t * torch.sum((target - model_output) ** 2) / (batch_size**2)\n",
    "    )\n",
    "    return result / seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25472a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_t(beta_1: Tensor, t: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        beta_1: Maximum possible accuracy (reached when t=1) of shape (batch_size,).\n",
    "        t: A tensor representing the time step, where 1 corresponds to maximum accuracy of shape (batch_size,).\n",
    "    Returns:\n",
    "        Beta value at given time step t\n",
    "    \"\"\"\n",
    "    assert beta_1.ndim == 1, \"beta_1 should be a 1D tensor\"\n",
    "    assert t.ndim == 1, \"t should be a 1D tensor\"\n",
    "    assert beta_1.shape == t.shape, \"beta_1 and t should have the same shape\"\n",
    "    assert torch.all(t >= 0), \"t must be at least 0\"\n",
    "    assert torch.all(t <= 1), \"t must be at most 1\"\n",
    "    return beta_1 * (t**2)\n",
    "\n",
    "\n",
    "def y_distribution(beta: Tensor, K: int, kron_x: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        beta: Tensor of accuracy values for each batch of shape (batch_size,).\n",
    "        K: Number of classes (usually vocabulary size etc.)\n",
    "        kron_x: One-hot encoded input tensor of shape (batch_size, seq_len, K).\n",
    "    Returns:\n",
    "        Noisy version of kron_x with the amount of noise controlled\n",
    "        by beta. The shape of the output tensor is the same as kron_x, i.e., (batch_size, seq_len, K).\n",
    "    \"\"\"\n",
    "    beta = beta.view(\n",
    "        -1, 1, 1\n",
    "    )  # allows for broadcasting with reach appropriate batch in kron_x\n",
    "    mean = beta * (K * kron_x - 1)\n",
    "    variance = beta * K\n",
    "    epsilon = torch.normal(0, 1, kron_x.shape, device=kron_x.device)\n",
    "    return mean + (variance**0.5) * epsilon\n",
    "\n",
    "\n",
    "def theta(y: Tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        y: Tensor of shape (batch_size, seq_len, K) representing the noisy version of kron_x.\n",
    "    Returns:\n",
    "        Tensor representing the scaled softmax of y, which is the input to the model.\n",
    "    \"\"\"\n",
    "    assert y.ndim == 3, \"y should be a 3D tensor of shape (batch_size, seq_len, K)\"\n",
    "    theta = F.softmax(y, dim=-1)\n",
    "    theta = 2 * theta - 1  # scale to [-1, 1]\n",
    "    return theta\n",
    "\n",
    "\n",
    "def sample_t(batch_size, min_t=1e-6):\n",
    "    return torch.clamp(torch.FloatTensor(batch_size).uniform_(0, 1), min=min_t)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    This collate function will truncate all sequences to the minimum length of\n",
    "    the sequences in the batch\n",
    "\n",
    "    Args:\n",
    "        batch: List of dictionaries, each containing 'x', 't', and 'beta'.\n",
    "    Returns:\n",
    "        A dictionary with keys 'x', 't', 'beta_1' and 'theta', where 'x' is a tensor of shape\n",
    "        (batch_size, seq_len, K), 't' is a tensor of shape (batch_size,), 'beta_1'\n",
    "        is a tensor of shape (batch_size,), and 'theta' is the transformed version of 'x'.\n",
    "    \"\"\"\n",
    "    x = [item[\"x\"] for item in batch]\n",
    "    min_length = min(seq.shape[0] for seq in x)\n",
    "    x = [tensor[:min_length] for tensor in x]\n",
    "\n",
    "    x = torch.stack(x, dim=0)  # Shape: (batch_size, seq_len, K)\n",
    "    t = torch.cat([item[\"t\"] for item in batch], dim=0)  # Shape: (batch_size,)\n",
    "    beta = torch.cat([item[\"beta\"] for item in batch], dim=0)\n",
    "    beta_1 = torch.cat(\n",
    "        [item[\"beta_1\"] for item in batch], dim=0\n",
    "    )  # Shape: (batch_size,)\n",
    "\n",
    "    y = y_distribution(beta, x.shape[-1], x)  # Shape: (batch_size, seq_len, K)\n",
    "    theta_tensor = theta(y)  # Shape: (batch_size, seq_len, K)\n",
    "\n",
    "    return {\"x\": x, \"t\": t, \"beta_1\": beta_1, \"theta\": theta_tensor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "668cb2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerBase:\n",
    "    def vocab_size(self) -> int:\n",
    "        raise NotImplementedError(\"This method should be implemented by subclasses.\")\n",
    "\n",
    "    def encode(self, text: str) -> Tensor:\n",
    "        raise NotImplementedError(\"This method should be implemented by subclasses.\")\n",
    "\n",
    "    def decode(self, tokens: Tensor) -> str:\n",
    "        raise NotImplementedError(\"This method should be implemented by subclasses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f32f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteSyntheticTokenizer(TokenizerBase):\n",
    "    # only tokenizes strings like \" 8 , 9 , 1 0 , 1 1 , 1 2 ,\"\n",
    "    # this is intended to be used only with the discrete synthetic dataset\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vocab = {\",\": 10}\n",
    "        for i in range(10):\n",
    "            key = str(i)\n",
    "            value = i\n",
    "            self.vocab[key] = value\n",
    "\n",
    "        self.anti_vocab = {}\n",
    "        for k in self.vocab:\n",
    "            self.anti_vocab[self.vocab[k]] = k\n",
    "\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def encode(self, text: str) -> Tensor:\n",
    "        splits = text.split()\n",
    "        res = [self.vocab.get(s, 0) for s in splits]\n",
    "        return torch.tensor(res, dtype=torch.long)\n",
    "\n",
    "    def decode(self, tokens: Tensor) -> str:\n",
    "        assert tokens.ndim == 2, \"tokens should be a 2D tensor of shape (seq_len, K)\"\n",
    "        seq_len, K = tokens.shape\n",
    "        cur_seq = []\n",
    "        for i in range(seq_len):\n",
    "            one_hot_encoding = tokens[i]\n",
    "            value = torch.argmax(one_hot_encoding)\n",
    "            cur_seq.append(self.anti_vocab.get(value.item(), \"\"))\n",
    "        return \" \".join(cur_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7d2c3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteSyntheticDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: DiscreteSyntheticTokenizer,\n",
    "        length: int = 32,\n",
    "        tokenized_length: int = 32,\n",
    "        mini: int = 0,\n",
    "        maxi: int = 100,\n",
    "        beta_1: float = 4.0,\n",
    "        min_t: float = 1e-6,\n",
    "    ):\n",
    "        self.length = length\n",
    "        self.tokenized_length = tokenized_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mini = mini\n",
    "        self.maxi = maxi\n",
    "        self.min_t = min_t\n",
    "        self.beta_1 = torch.tensor([beta_1])\n",
    "\n",
    "    def generate_sequence(self):\n",
    "        start = random.randint(self.mini, self.maxi - self.length)\n",
    "        end = start + self.length\n",
    "        acc = \"\"\n",
    "        for i in range(start, end + 1):\n",
    "            for c in str(i):\n",
    "                acc += \" \" + c\n",
    "            acc += \" ,\"\n",
    "        tokenized = self.tokenizer.encode(acc)\n",
    "        return tokenized[: self.tokenized_length]\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10000\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = F.one_hot(\n",
    "            self.generate_sequence(), num_classes=self.tokenizer.vocab_size()\n",
    "        )\n",
    "        t = sample_t(1, self.min_t)\n",
    "        beta = beta_t(self.beta_1, t)\n",
    "        return {\"x\": seq, \"t\": t, \"beta\": beta, \"beta_1\": self.beta_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a280419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_seq_len: int,\n",
    "        K: int,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        layers: int = 3,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisble by num_heads\"\n",
    "        self.emb = nn.Parameter(torch.randn(K, hidden_dim))\n",
    "        self.pos_emb = nn.Parameter(torch.randn(max_seq_len, hidden_dim))\n",
    "        self.time_vec = nn.Parameter(torch.randn(1, hidden_dim))\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.TransformerEncoderLayer(\n",
    "                    hidden_dim,\n",
    "                    num_heads,\n",
    "                    hidden_dim * 4,\n",
    "                    dropout,\n",
    "                    batch_first=True,\n",
    "                    bias=False,\n",
    "                )\n",
    "                for i in range(layers)\n",
    "            ]\n",
    "        )\n",
    "        self.classifier = nn.Parameter(torch.randn(hidden_dim, K))\n",
    "\n",
    "    def token_emb(self, x):\n",
    "        return x @ self.emb\n",
    "\n",
    "    def positional_emb(self, x):\n",
    "        return x + self.pos_emb[: x.shape[1]]\n",
    "\n",
    "    def time_emb(self, x, t):\n",
    "        assert t.ndim == 1, \"time vector `t` should be vector of length batch_size\"\n",
    "        # we need to first unsqueeze t to get it from shape (batch_size,)\n",
    "        # to (batch_size, 1) so it is compatible with the time_vec's (1, hidden_dim)\n",
    "        # the result is (batch_size, hidden_dim) however the x is\n",
    "        # (batch_size, seq_len, hidden_dim) so we need a second unsqueeze\n",
    "        return (t.unsqueeze(-1) @ self.time_vec).unsqueeze(-2) + x\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.token_emb(x)\n",
    "        x = self.positional_emb(x)\n",
    "        x = self.time_emb(x, t)\n",
    "        for i, l in enumerate(self.layers):\n",
    "            x = l.forward(x)\n",
    "        return F.relu(x @ self.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2273f3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 597,632\n",
      "Vocab size: 11\n",
      "Dataset size: 10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525572cc171c47eb910ae861053146da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/2:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Average loss: 41.1602\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd30f7733ac2437682dff4c6b76c413e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/2:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Average loss: 40.8119\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = DiscreteSyntheticTokenizer()\n",
    "dataset = DiscreteSyntheticDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    length=16,\n",
    "    tokenized_length=32,\n",
    "    beta_1=4.0\n",
    ")\n",
    "\n",
    "# Create data loader\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=8, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = DiscreteModel(\n",
    "    max_seq_len=32,\n",
    "    K=tokenizer.vocab_size(),\n",
    "    hidden_dim=128,\n",
    "    num_heads=4,\n",
    "    layers=3,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size()}\")\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Training loop for 2 epochs\n",
    "num_epochs = 2\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get batch data\n",
    "        x = batch[\"x\"]  # Target one-hot encoded sequences\n",
    "        t = batch[\"t\"]  # Time steps\n",
    "        beta_1 = batch[\"beta_1\"]  # Beta values\n",
    "        theta_tensor = batch[\"theta\"]  # Input to model (noisy version)\n",
    "        \n",
    "        # Forward pass\n",
    "        model_output_logits = model(theta_tensor, t)\n",
    "        \n",
    "        # Compute loss\n",
    "        batch_loss = loss(\n",
    "            beta_1=beta_1,\n",
    "            t=t,\n",
    "            target=x.float(),\n",
    "            model_output_logits=model_output_logits\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        epoch_loss += batch_loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"loss\": f\"{batch_loss.item():.4f}\"})\n",
    "    \n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    print(f\"Epoch {epoch+1} completed. Average loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
