{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65e9b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nested import as_nested_tensor, nested_tensor, to_padded_tensor\n",
    "from torch.nn import Identity, Linear, Module, Parameter, Sequential\n",
    "from torch.nn.functional import scaled_dot_product_attention as sdpa\n",
    "\n",
    "\n",
    "class ColumnarAttention(Module):\n",
    "    def __init__(\n",
    "        self, hidden_dim: int, num_heads: int, dropout: float = 0.0, columns: int = 128\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.columns = columns\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.q = Parameter(\n",
    "            torch.randn(\n",
    "                columns,\n",
    "                hidden_dim,\n",
    "            )\n",
    "            * 0.02\n",
    "        )\n",
    "        self.k_proj = Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = Linear(hidden_dim, hidden_dim)\n",
    "        self.out_proj = Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.apply(self._init_weight)\n",
    "\n",
    "    def _init_weight(self, module):\n",
    "        if isinstance(module, Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    def prepare_kv(self, x: Tensor, seq_lens: Tensor, is_k: bool) -> Tensor:\n",
    "        x = self.k_proj(x) if is_k else self.v_proj(x)\n",
    "        x = einops.rearrange(\n",
    "            x,\n",
    "            \"1 (batch seq) (heads headdim) -> (batch seq) heads headdim\",\n",
    "            heads=self.num_heads,\n",
    "            batch=len(seq_lens),\n",
    "        )\n",
    "        splits = x.split(seq_lens.tolist(), dim=0)\n",
    "        nest = as_nested_tensor(list(splits))\n",
    "        nest = nest.transpose(1, 2).contiguous()  # batch x heads x seq x headdim\n",
    "        return nest\n",
    "\n",
    "    def forward(self, x, doc_ids: Tensor) -> Tensor:\n",
    "        batch, seq, dim = x.shape\n",
    "        assert batch == 1, \"Batch size must be 1 for packed sequences\"\n",
    "        assert doc_ids.shape == (batch, seq)\n",
    "\n",
    "        seq_lens = torch.bincount(doc_ids.flatten())\n",
    "\n",
    "        k = self.prepare_kv(x, seq_lens, is_k=True)\n",
    "        v = self.prepare_kv(x, seq_lens, is_k=False)\n",
    "\n",
    "        queries = einops.rearrange(\n",
    "            einops.repeat(\n",
    "                self.q,\n",
    "                \"seq dim -> batch seq dim\",\n",
    "                batch=len(seq_lens),\n",
    "            ),\n",
    "            \"batch seq (heads headdim) -> batch seq heads headdim\",\n",
    "            heads=self.num_heads,\n",
    "            batch=len(seq_lens),\n",
    "        )\n",
    "        nested_queries = as_nested_tensor(queries)\n",
    "        nested_queries = nested_queries.transpose(\n",
    "            1, 2\n",
    "        ).contiguous()  # batch x heads x seq x headdim\n",
    "\n",
    "        out = sdpa(query=nested_queries, key=k, value=v, dropout_p=self.dropout)\n",
    "\n",
    "        # note that the padded tensor here is just to make it so it is easier to work downstream,\n",
    "        # because all queries have the same length, there won't be any actual padding\n",
    "        out = to_padded_tensor(\n",
    "            out.contiguous(), padding=0.0\n",
    "        )  # batch x heads x seq x headdim\n",
    "        out = einops.rearrange(\n",
    "            out,\n",
    "            \"batch heads seq headdim -> batch seq (heads headdim)\",\n",
    "        )\n",
    "        out = self.out_proj(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class AntiColumnarAttention(Module):\n",
    "    def __init__(self, hidden_dim: int, num_heads: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "\n",
    "        self.q_proj = Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = Linear(hidden_dim, hidden_dim)\n",
    "        self.out_proj = Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.apply(self._init_weight)\n",
    "\n",
    "    def _init_weight(self, module):\n",
    "        if isinstance(module, Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    def prepare_q(self, x: Tensor, seq_lens: Tensor) -> Tensor:\n",
    "        x = self.q_proj(x)\n",
    "        x = einops.rearrange(\n",
    "            x,\n",
    "            \"1 (batch seq) (heads headdim) -> (batch seq) heads headdim\",\n",
    "            heads=self.num_heads,\n",
    "            batch=len(seq_lens),\n",
    "        )\n",
    "        splits = x.split(seq_lens.tolist(), dim=0)\n",
    "        nest = as_nested_tensor(list(splits))\n",
    "        nest = nest.transpose(1, 2).contiguous()  # batch x heads x seq x headdim\n",
    "        return nest\n",
    "\n",
    "    def prepare_kv(self, x: Tensor, is_k: bool) -> Tensor:\n",
    "        x = self.k_proj(x) if is_k else self.v_proj(x)\n",
    "        # x is (batch, fixed_seq, dim)\n",
    "        x = einops.rearrange(\n",
    "            x,\n",
    "            \"batch seq (heads headdim) -> batch seq heads headdim\",\n",
    "            heads=self.num_heads,\n",
    "        )\n",
    "        # Convert to nested tensor to match q structure\n",
    "        nest = as_nested_tensor(list(x.unbind(0)))\n",
    "        nest = nest.transpose(1, 2).contiguous()  # batch x heads x seq x headdim\n",
    "        return nest\n",
    "\n",
    "    def forward(self, x: Tensor, context: Tensor, doc_ids: Tensor) -> Tensor:\n",
    "        batch, seq, dim = x.shape\n",
    "        assert batch == 1, \"Batch size must be 1 for packed sequences\"\n",
    "        assert doc_ids.shape == (batch, seq)\n",
    "        seq_lens = torch.bincount(doc_ids.flatten())\n",
    "\n",
    "        q = self.prepare_q(x, seq_lens)\n",
    "        k = self.prepare_kv(context, is_k=True)\n",
    "        v = self.prepare_kv(context, is_k=False)\n",
    "\n",
    "        out = sdpa(query=q, key=k, value=v, dropout_p=self.dropout)\n",
    "\n",
    "        # out is batch x heads x seq x headdim (nested)\n",
    "        out = out.transpose(1, 2)  # batch x seq x heads x headdim\n",
    "        out = torch.cat(out.unbind(0), dim=0)  # total_seq x heads x headdim\n",
    "        out = einops.rearrange(\n",
    "            out, \"total_seq heads headdim -> 1 total_seq (heads headdim)\"\n",
    "        )\n",
    "\n",
    "        out = self.out_proj(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PillarMan(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        columns: int = 128,\n",
    "        middle: Module | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.columnar_attn = ColumnarAttention(hidden_dim, num_heads, dropout, columns)\n",
    "        self.anti_columnar_attn = AntiColumnarAttention(hidden_dim, num_heads, dropout)\n",
    "        self.middle = middle if middle is not None else Identity()\n",
    "\n",
    "        self.residual = Sequential(\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weight)\n",
    "\n",
    "    def _init_weight(self, module):\n",
    "        if isinstance(module, Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, original_x: Tensor, seq_idx: Tensor) -> Tensor:\n",
    "        res = self.residual(original_x)\n",
    "        x = self.columnar_attn(original_x, seq_idx)\n",
    "        x = self.middle(x)\n",
    "        x = self.anti_columnar_attn(res, x, seq_idx)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b558b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96710aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PillarMan(\n",
       "  (columnar_attn): ColumnarAttention(\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (anti_columnar_attn): AntiColumnarAttention(\n",
       "    (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (out_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (middle): Identity()\n",
       "  (residual): Sequential(\n",
       "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 16\n",
    "num_heads = 2\n",
    "cols = 6\n",
    "pillarman = PillarMan(\n",
    "    hidden_dim=hidden_size,\n",
    "    num_heads=num_heads,\n",
    "    dropout=0.1,\n",
    "    columns=cols,\n",
    ")\n",
    "pillarman.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "049853c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [1,2,3]\n",
    "xs = [torch.randn(1, seq, hidden_size) for seq in sequences]\n",
    "xs = torch.cat(xs, dim=1)\n",
    "xs.requires_grad_(True)\n",
    "index = 0\n",
    "doc_ids = []\n",
    "for seq in sequences:\n",
    "    doc_ids.extend([index] * seq)\n",
    "    index += 1\n",
    "doc_ids = torch.tensor([doc_ids])\n",
    "doc_ids = doc_ids.to(device)\n",
    "xs = xs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a763a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/torch/nested/__init__.py:117: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  return torch._nested_tensor_from_tensor_list(ts, dtype, None, device, None)\n",
      "/tmp/ipykernel_2850019/1110632680.py:78: UserWarning: Flash Efficient attention on Current AMD GPU is still experimental. Enable it with TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1. (Triggered internally at /pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:287.)\n",
      "  out = sdpa(query=nested_queries, key=k, value=v, dropout_p=self.dropout)\n",
      "/tmp/ipykernel_2850019/1110632680.py:78: UserWarning: Mem Efficient attention on Current AMD GPU is still experimental. Enable it with TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1. (Triggered internally at /pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:338.)\n",
      "  out = sdpa(query=nested_queries, key=k, value=v, dropout_p=self.dropout)\n"
     ]
    }
   ],
   "source": [
    "res = pillarman(xs, doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ae5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = torch.autograd.grad(res.sum(), xs, retain_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdbf8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_xs = xs - 0.1 * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32db55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_target = torch.randn_like(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "551dac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = (some_target - xs).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e67422b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd914a32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesianflownet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
