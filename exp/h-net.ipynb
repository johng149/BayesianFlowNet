{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40eac455",
   "metadata": {},
   "source": [
    "### Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b4303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following section 2.2 of the paper\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import cat, arange\n",
    "from torch.nested import nested_tensor\n",
    "from torch.nn import Module, Linear, Parameter\n",
    "from torch.nn.functional import cosine_similarity, pad\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from einx import multiply\n",
    "from einops import repeat, rearrange\n",
    "\n",
    "from assoc_scan import AssocScan\n",
    "\n",
    "# constants\n",
    "\n",
    "Outputs = namedtuple('Outputs', [\n",
    "    'downsampled',\n",
    "    'upsample_fn',\n",
    "    'weighted_aux_ratio_loss'\n",
    "])\n",
    "\n",
    "Intermediates = namedtuple('Intermediates', [\n",
    "    'mask',\n",
    "    'probs',\n",
    "    'chunk_lens',\n",
    "    'boundary_mask',\n",
    "    'residual',\n",
    "    'gates',\n",
    "    'upsampler_output_scale',\n",
    "    'aux_ratio_loss',\n",
    "    'input_mask',     # Added: needed to handle padding logic in upsampler\n",
    "    'is_packed',      # Added: to track state\n",
    "    'seq_lens',       # Added: original sequence lengths\n",
    "    'new_seq_lens'    # Added: downsampled sequence lengths\n",
    "])\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def exists(v):\n",
    "    return v is not None\n",
    "\n",
    "def default(v, d):\n",
    "    return v if exists(v) else d\n",
    "\n",
    "def straight_through(t, value):\n",
    "    return t + (value - t).detach()\n",
    "\n",
    "def frac_gradient(t, frac = 1.):\n",
    "    if frac == 1:\n",
    "        return\n",
    "\n",
    "    t_grad = t * frac\n",
    "    return straight_through(t_grad, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PackDynamicSequenceChunker(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_queries_keys = None,\n",
    "        boundary_threshold = 0.5,\n",
    "        target_avg_token_length = 6.,       # N in eq(10)\n",
    "        ratio_loss_weight = 3e-2,\n",
    "        handle_residual_proj = False,       # turning this on will automatically handle a projection of the residual and its application in the inverse upsample function\n",
    "        assoc_scan_use_accelerated = False,\n",
    "        learning_rate_difference = 0.75,    # in the paper, they report that as one moves up a hierarchy, the learning rate needs to decrease. we'll default to 0.75 for the rough 2.0 -> 1.5 somewhere in the appendix from level 0 -> 1\n",
    "        straight_through_frac_vecs = True,  # improvisation where F receives gradients through straight-through with sigmoid\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_queries_keys = default(dim_queries_keys, dim)\n",
    "\n",
    "        # linear to queries and keys\n",
    "\n",
    "        self.to_queries_keys = Linear(dim, dim_queries_keys * 2, bias = False)\n",
    "\n",
    "        # start key token, so first token can be segmented / chunked out\n",
    "\n",
    "        self.start_key_token = Parameter(torch.randn(dim_queries_keys) * 1e-2) # presumably, need a start key token for the first token, open an issue if i got it wrong\n",
    "\n",
    "        # threshold to determine boundary\n",
    "\n",
    "        assert 0. < boundary_threshold < 1.\n",
    "\n",
    "        self.boundary_threshold = boundary_threshold\n",
    "\n",
    "        # smoothing related\n",
    "\n",
    "        self.smooth_assoc_scan = AssocScan(use_accelerated = assoc_scan_use_accelerated)\n",
    "\n",
    "        # maybe residual proj\n",
    "\n",
    "        self.handle_residual_proj = handle_residual_proj\n",
    "\n",
    "        if handle_residual_proj:\n",
    "            self.residual_proj = Linear(dim, dim)\n",
    "\n",
    "        # learning rate modulation, appendix C\n",
    "        # the multiplier on the learning rate as one goes from outer to inner of the h-net, and inverse of this value from inner to outer\n",
    "\n",
    "        self.learning_rate_difference = learning_rate_difference\n",
    "\n",
    "        # ratio aux loss related\n",
    "\n",
    "        self.target_avg_token_length = target_avg_token_length\n",
    "\n",
    "        self.straight_through_frac_vecs = straight_through_frac_vecs\n",
    "\n",
    "        self.ratio_loss_weight = ratio_loss_weight\n",
    "\n",
    "        self.register_buffer('zero', torch.tensor(0.), persistent = False)\n",
    "\n",
    "    def upsample(\n",
    "        self,\n",
    "        downsampled,\n",
    "        intermediates: Intermediates,\n",
    "        apply_scale = True\n",
    "    ):\n",
    "        # handle packed sequence input for upsampling\n",
    "        is_packed = downsampled.ndim == 2\n",
    "        \n",
    "        if is_packed:\n",
    "            # If the original input was packed, the downsampled input here should be packed.\n",
    "            # We need to unpack it to (B, N, D) to use the internal logic.\n",
    "            assert exists(intermediates.new_seq_lens), \"Cannot upsample packed sequence without new_seq_lens\"\n",
    "            downsampled_list = list(downsampled.split(intermediates.new_seq_lens.tolist()))\n",
    "            downsampled = pad_sequence(downsampled_list, batch_first=True)\n",
    "\n",
    "        batch, needs_grad, device = downsampled.shape[0], downsampled.requires_grad, downsampled.device\n",
    "\n",
    "        mask = intermediates.mask\n",
    "        gates = intermediates.gates\n",
    "        residual = intermediates.residual\n",
    "\n",
    "        # smoothing module for improved gradients eq(5)\n",
    "\n",
    "        downsampled = self.smooth_assoc_scan(gates, downsampled)\n",
    "\n",
    "        # upsample\n",
    "\n",
    "        downsampled_without_padding = downsampled[mask]\n",
    "        chunk_lens_without_padding = intermediates.chunk_lens[mask]\n",
    "\n",
    "        seq = arange(downsampled_without_padding.shape[0], device = device)\n",
    "\n",
    "        repeated_indices = torch.repeat_interleave(seq, chunk_lens_without_padding, dim = 0)\n",
    "        upsampled = downsampled_without_padding[repeated_indices]\n",
    "\n",
    "        upsampled = rearrange(upsampled, '(b n) d -> b n d', b = batch)\n",
    "\n",
    "        scale = intermediates.upsampler_output_scale\n",
    "\n",
    "        if needs_grad and apply_scale and exists(scale):\n",
    "            upsampled = multiply('b n d, b n', upsampled, scale)\n",
    "\n",
    "        if self.handle_residual_proj:\n",
    "            # We need to use the original residual (input tokens)\n",
    "            # If we are in packed mode, the residual stored in intermediates is likely padded.\n",
    "            # We apply the projection on the padded residual and mask later if needed.\n",
    "            upsampled = upsampled + self.residual_proj(residual)\n",
    "\n",
    "        upsampled = frac_gradient(upsampled, self.learning_rate_difference)\n",
    "        \n",
    "        # If the original input was packed, we must return a packed sequence.\n",
    "        # We use the original sequence lengths from intermediates to repack.\n",
    "        if intermediates.is_packed:\n",
    "            # Mask out padding from the padded upsampled result\n",
    "            # intermediates.input_mask contains the valid locations of the original sequence\n",
    "            if exists(intermediates.input_mask):\n",
    "                upsampled = upsampled[intermediates.input_mask]\n",
    "            else:\n",
    "                # Fallback if mask missing (shouldn't happen in packed flow)\n",
    "                upsampled_list = [upsampled[i, :l] for i, l in enumerate(intermediates.seq_lens)]\n",
    "                upsampled = cat(upsampled_list, dim=0)\n",
    "\n",
    "        return upsampled\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokens, # float[b n d] OR float[total_n d] if packed\n",
    "        seq_lens = None, # Required if tokens is packed\n",
    "        return_intermediates = False,\n",
    "        return_only_chunk_lens = False\n",
    "    ):\n",
    "        # ----------------------------------------------------------------------\n",
    "        # Handle Packed Sequences\n",
    "        # ----------------------------------------------------------------------\n",
    "        is_packed = tokens.ndim == 2\n",
    "        input_mask = None\n",
    "        \n",
    "        if is_packed:\n",
    "            assert exists(seq_lens), 'seq_lens must be provided for packed sequences'\n",
    "            # Unpack to (Batch, MaxLen, Dim) for processing\n",
    "            tokens_list = list(tokens.split(seq_lens.tolist()))\n",
    "            tokens_padded = pad_sequence(tokens_list, batch_first=True)\n",
    "            \n",
    "            # Create mask for padding\n",
    "            # This is critical for cosine_sim and loss masking\n",
    "            max_len = tokens_padded.shape[1]\n",
    "            batch_size = len(seq_lens)\n",
    "            range_tensor = arange(max_len, device=tokens.device).expand(batch_size, max_len)\n",
    "            input_mask = range_tensor < seq_lens.unsqueeze(1)\n",
    "            \n",
    "            # Switch tokens pointer to padded version\n",
    "            tokens = tokens_padded\n",
    "        else:\n",
    "            # Standard batched input\n",
    "            # Create a default mask of all True if not provided\n",
    "            batch, length = tokens.shape[:2]\n",
    "            input_mask = torch.ones((batch, length), device=tokens.device, dtype=torch.bool)\n",
    "            seq_lens = torch.full((batch,), length, device=tokens.device, dtype=torch.long)\n",
    "\n",
    "        batch, length, device = *tokens.shape[:2], tokens.device\n",
    "\n",
    "        residual = tokens\n",
    "\n",
    "        queries, keys = self.to_queries_keys(tokens).chunk(2, dim = -1)\n",
    "\n",
    "        start_keys = repeat(self.start_key_token, 'd -> b 1 d', b = batch)\n",
    "\n",
    "        keys = cat((start_keys, keys), dim = 1)\n",
    "\n",
    "        # each query looks at the previous key\n",
    "        # Handle padding for cosine similarity: avoid NaN by adding epsilon to norms or masking\n",
    "        \n",
    "        keys_shifted = keys[:, :-1]\n",
    "        \n",
    "        # Safe cosine similarity\n",
    "        eps = 1e-6\n",
    "        queries_norm = queries.norm(dim=-1, keepdim=True)\n",
    "        keys_norm = keys_shifted.norm(dim=-1, keepdim=True)\n",
    "        numerator = (queries * keys_shifted).sum(dim=-1)\n",
    "        denominator = (queries_norm * keys_norm).clamp(min=eps).squeeze(-1)\n",
    "        cosine_sim = numerator / denominator\n",
    "\n",
    "        # Mask out cosine sim on padding to avoid garbage boundaries\n",
    "        cosine_sim = cosine_sim.masked_fill(~input_mask, 1.0) # 1.0 sim -> 0.0 prob -> no boundary\n",
    "\n",
    "        probs = (1. - cosine_sim) * 0.5 # cosine sim is -1. to 1., this transforms it to 0. to 1.\n",
    "\n",
    "        boundary_mask = probs > self.boundary_threshold # bool[b n]\n",
    "\n",
    "        boundary_mask[:, 0] = True # first token must always be boundary\n",
    "        \n",
    "        # Ensure padding tokens are never boundaries\n",
    "        boundary_mask = boundary_mask & input_mask\n",
    "\n",
    "        # compute some lengths, per chunk and number of chunks per batch\n",
    "\n",
    "        num_chunks = boundary_mask.long().sum(dim = -1)\n",
    "\n",
    "        boundary_mask_with_end = pad(boundary_mask, (0, 1), value = True)\n",
    "        sel_indices = repeat(arange(boundary_mask_with_end.shape[-1], device = device), 'n -> b n', b = batch)[boundary_mask_with_end]\n",
    "\n",
    "        sel_indices = nested_tensor(sel_indices.split((num_chunks + 1).tolist()), layout = torch.jagged, device = device)\n",
    "\n",
    "        sel_indices = sel_indices.to_padded_tensor(padding = -1)\n",
    "\n",
    "        mask = (sel_indices != -1)[:, 1:]\n",
    "\n",
    "        chunk_lens = sel_indices[:, 1:] - sel_indices[:, :-1]\n",
    "        chunk_lens.masked_fill_(~mask, 0)\n",
    "\n",
    "        # early return chunk lens if using a trained module as a tokenizer\n",
    "\n",
    "        if return_only_chunk_lens:\n",
    "            if is_packed:\n",
    "                # Return packed chunk lenses\n",
    "                return chunk_lens[mask]\n",
    "            return chunk_lens\n",
    "\n",
    "        # downsampling - they show in their experiments that picking out the boundary tokens works just fine\n",
    "\n",
    "        boundary_tokens = tokens[boundary_mask] # pick out boundary tokens\n",
    "\n",
    "        tokens_nt = nested_tensor(boundary_tokens.split(num_chunks.tolist()), layout = torch.jagged, device = device, requires_grad = True)\n",
    "\n",
    "        downsampled_tokens = tokens_nt.to_padded_tensor(padding = 0.)\n",
    "\n",
    "        # smoothing module for improved gradients eq(5)\n",
    "\n",
    "        probs_nt = nested_tensor(probs[boundary_mask].split(num_chunks.tolist()), layout = torch.jagged, device = device, requires_grad = True)\n",
    "\n",
    "        boundary_probs = probs_nt.to_padded_tensor(padding = 0.)\n",
    "\n",
    "        gates = 1. - boundary_probs\n",
    "\n",
    "        downsampled_tokens = multiply('b n d, b n', downsampled_tokens, boundary_probs)\n",
    "\n",
    "        # for the upsampler\n",
    "\n",
    "        confidence = torch.where(boundary_mask, probs, 1. - probs)\n",
    "\n",
    "        # defaults if not training\n",
    "\n",
    "        upsampler_output_scale = None\n",
    "        aux_loss = self.zero\n",
    "        weighted_aux_loss = self.zero\n",
    "\n",
    "        needs_grad = tokens.requires_grad\n",
    "\n",
    "        if needs_grad:\n",
    "            # straight through for 1. multiplier on the expanded processed boundary tokens\n",
    "            \n",
    "            # For scale, we only care about valid tokens\n",
    "            if is_packed:\n",
    "                upsampler_output_scale = straight_through(confidence, 1.) # Padded shape\n",
    "                # We don't mask here because multiply() in upsampler handles shape, \n",
    "                # but valid gradients only come from valid indices.\n",
    "            else:\n",
    "                upsampler_output_scale = straight_through(confidence, 1.)\n",
    "\n",
    "            # auxiliary ratio loss in section 2.3.2, eq (10)\n",
    "\n",
    "            N = self.target_avg_token_length\n",
    "\n",
    "            F = boundary_mask.float()\n",
    "            \n",
    "            # Mask the probs for G calculation so padding doesn't affect mean\n",
    "            probs_masked = probs * input_mask.float()\n",
    "            G = probs_masked.sum(dim = -1) / input_mask.float().sum(dim = -1).clamp(min=1.)\n",
    "\n",
    "            # allow for a soft F to straight through - https://arxiv.org/abs/2505.22074\n",
    "\n",
    "            if self.straight_through_frac_vecs:\n",
    "                F_soft = (probs - self.boundary_threshold).sigmoid()\n",
    "                F = straight_through(F_soft, F)\n",
    "\n",
    "            # Mask F for mean calculation\n",
    "            F = (F * input_mask.float()).sum(dim = -1) / input_mask.float().sum(dim = -1).clamp(min=1.)\n",
    "\n",
    "            aux_ratio_loss = N / (N - 1) * ((N - 1) * F * G + (1. - F) * (1. - G))\n",
    "\n",
    "            aux_loss = aux_ratio_loss.mean()\n",
    "            weighted_aux_loss = aux_loss * self.ratio_loss_weight\n",
    "\n",
    "        # intermediates\n",
    "        \n",
    "        # Calculate new seq lengths for packed return\n",
    "        new_seq_lens = num_chunks\n",
    "        \n",
    "        intermediates = Intermediates(\n",
    "            mask, probs, chunk_lens, boundary_mask, residual, gates, \n",
    "            upsampler_output_scale, aux_loss,\n",
    "            input_mask, is_packed, seq_lens, new_seq_lens\n",
    "        )\n",
    "\n",
    "        # return the upsample function\n",
    "\n",
    "        def upsample(downsampled, apply_scale = True):\n",
    "            return self.upsample(downsampled, intermediates, apply_scale = apply_scale)\n",
    "\n",
    "        # adjust learning rate\n",
    "\n",
    "        downsampled_tokens = frac_gradient(downsampled_tokens, self.learning_rate_difference ** -1)\n",
    "\n",
    "        # handle packed output\n",
    "        \n",
    "        if is_packed:\n",
    "            # We computed downsampled_tokens as (B, N_chunks, D) padded.\n",
    "            # We need to pack it back to (Total_Chunks, D)\n",
    "            # We can use new_seq_lens (num_chunks) to create the mask for valid data\n",
    "            valid_chunks_mask = arange(downsampled_tokens.shape[1], device=device).expand(batch, -1) < new_seq_lens.unsqueeze(1)\n",
    "            downsampled_tokens = downsampled_tokens[valid_chunks_mask]\n",
    "\n",
    "        # returning\n",
    "\n",
    "        outputs = Outputs(downsampled_tokens, upsample, weighted_aux_loss)\n",
    "\n",
    "        if not return_intermediates:\n",
    "            return outputs\n",
    "\n",
    "        return outputs, intermediates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c0c22b",
   "metadata": {},
   "source": [
    "### Encoding + Basic Main Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef783b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# had to modify the file at\n",
    "# lib/python3.12/site-packages/fla/ops/common/chunk_delta_h.py\n",
    "# specifically the `chunk_gated_delta_rule_fwd_h` to make chunk_size smaller\n",
    "# it was 64 but I made it 16 and this allows triton to run without OOM\n",
    "# as well as the `chunk_gated_delta_rule_bwd_dhu` function to make chunk_size smaller\n",
    "# also it seems in this backwards function the BT is not dependent on chunk_size, so\n",
    "# need to change both to be the same value, I chose 16\n",
    "# but there seems to be an issue with the backwards pass. Not a memory issue, but\n",
    "# even worse, a LLVM runtime issue with Triton. We are kinda screwed for now\n",
    "# https://github.com/fla-org/flash-linear-attention/issues/638"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ff5386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fla.layers.kda import KimiDeltaAttention as Attn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34af7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, dim = 2, 128, 32\n",
    "heads = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f73f0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, seq_len, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "929a5864",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = Attn(hidden_size=dim, num_heads=heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f7a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5625730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = AdamW(attn.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e06f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61028f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = attn.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77d8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = attn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0657c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0c4771d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/fla/ops/kda/chunk_intra.py:337:26: error: 'tt.load' op operation destroyed but still has uses\n",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))\n",
      "                         ^\n",
      "/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/fla/ops/kda/chunk_intra.py:347:25: note: - use: %375 = \"arith.mulf\"(<<UNKNOWN SSA VALUE>>, %374) <{fastmath = #arith.fastmath<none>}> : (tensor<16x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 1], order = [1, 0]}>>, tensor<16x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 1], order = [1, 0]}>>) -> tensor<16x64xf32, #ttg.blocked<{sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 1], order = [1, 0]}>>\n",
      "\n",
      "            b_qg = b_q * tl.where(m_j[:, None], exp(b_gk - b_gn[None, :]), 0)\n",
      "                        ^\n",
      "LLVM ERROR: operation destroyed but still has uses\n",
      "#blocked = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [1, 0]}>\n",
      "#blocked1 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [2, 16], warpsPerCTA = [1, 1], order = [1, 0]}>\n",
      "#blocked2 = #ttg.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>\n",
      "#blocked3 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [1, 0]}>\n",
      "#blocked4 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 1], order = [0, 1]}>\n",
      "#blocked5 = #ttg.blocked<{sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 1], order = [0, 1]}>\n",
      "#blocked6 = #ttg.blocked<{sizePerThread = [4, 4], threadsPerWarp = [2, 16], warpsPerCTA = [1, 1], order = [1, 0]}>\n",
      "module attributes {\"ttg.num-ctas\" = 1 : i32, \"ttg.num-warps\" = 1 : i32, ttg.target = \"hip:gfx1100\", \"ttg.threads-per-warp\" = 32 : i32} {\n",
      "  tt.func public @chunk_kda_bwd_kernel_intra(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg4: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg5: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg6: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg7: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg8: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg9: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg10: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg11: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg12: i32, %arg13: i32) attributes {noinline = false} {\n",
      "    %cst = arith.constant dense<64> : tensor<16x1xi64, #blocked>\n",
      "    %cst_0 = arith.constant dense<256> : tensor<1x16xi64, #blocked1>\n",
      "    %cst_1 = arith.constant dense<64> : tensor<1x16xi64, #blocked1>\n",
      "    %cst_2 = arith.constant dense<0> : tensor<1x16xi64, #blocked1>\n",
      "    %cst_3 = arith.constant dense<256> : tensor<16x1xi64, #blocked>\n",
      "    %cst_4 = arith.constant dense<0> : tensor<16xi64, #blocked2>\n",
      "    %cst_5 = arith.constant dense<4> : tensor<16xi64, #blocked2>\n",
      "    %cst_6 = arith.constant dense<128> : tensor<1x64xi64, #blocked3>\n",
      "    %cst_7 = arith.constant dense<0> : tensor<1x64xi64, #blocked3>\n",
      "    %cst_8 = arith.constant dense<0> : tensor<16x1xi64, #blocked>\n",
      "    %cst_9 = arith.constant dense<512> : tensor<16x1xi64, #blocked>\n",
      "    %c15_i32 = arith.constant 15 : i32\n",
      "    %c256_i32 = arith.constant 256 : i32\n",
      "    %cst_10 = arith.constant dense<0.000000e+00> : tensor<16xf32, #blocked2>\n",
      "    %cst_11 = arith.constant dense<0.000000e+00> : tensor<64xf32, #blocked2>\n",
      "    %c512_i32 = arith.constant 512 : i32\n",
      "    %cst_12 = arith.constant dense<512> : tensor<64xi32, #blocked2>\n",
      "    %cst_13 = arith.constant dense<64> : tensor<16xi32, #blocked2>\n",
      "    %cst_14 = arith.constant dense<4> : tensor<16xi32, #blocked2>\n",
      "    %cst_15 = arith.constant dense<0.000000e+00> : tensor<16x64xf32, #blocked3>\n",
      "    %c1_i32 = arith.constant 1 : i32\n",
      "    %c0_i32 = arith.constant 0 : i32\n",
      "    %cst_16 = arith.constant dense<128> : tensor<64xi32, #blocked2>\n",
      "    %c128_i32 = arith.constant 128 : i32\n",
      "    %c16_i32 = arith.constant 16 : i32\n",
      "    %c64_i32 = arith.constant 64 : i32\n",
      "    %c4_i32 = arith.constant 4 : i32\n",
      "    %0 = tt.get_program_id x : i32\n",
      "    %1 = tt.get_program_id y : i32\n",
      "    %2 = tt.get_program_id z : i32\n",
      "    %3 = arith.divsi %2, %c4_i32 : i32\n",
      "    %4 = arith.remsi %2, %c4_i32 : i32\n",
      "    %5 = arith.divsi %0, %c4_i32 : i32\n",
      "    %6 = arith.remsi %0, %c4_i32 : i32\n",
      "    %7 = arith.muli %arg12, %arg13 : i32\n",
      "    %8 = arith.muli %3, %arg13 : i32\n",
      "    %9 = arith.addi %8, %arg13 : i32\n",
      "    %10 = arith.subi %9, %8 : i32\n",
      "    %11 = arith.muli %1, %c64_i32 : i32\n",
      "    %12 = arith.muli %6, %c16_i32 : i32\n",
      "    %13 = arith.addi %11, %12 : i32\n",
      "    %14 = arith.cmpi sge, %13, %10 : i32\n",
      "    cf.cond_br %14, ^bb1, ^bb2\n",
      "  ^bb1:  // pred: ^bb0\n",
      "    tt.return\n",
      "  ^bb2:  // pred: ^bb0\n",
      "    %15 = arith.muli %5, %c64_i32 : i32\n",
      "    %16 = tt.make_range {end = 64 : i32, start = 0 : i32} : tensor<64xi32, #blocked2>\n",
      "    %17 = tt.splat %15 : i32 -> tensor<64xi32, #blocked2>\n",
      "    %18 = arith.addi %17, %16 : tensor<64xi32, #blocked2>\n",
      "    %19 = arith.cmpi slt, %18, %cst_16 : tensor<64xi32, #blocked2>\n",
      "    %20 = arith.muli %8, %c4_i32 : i32\n",
      "    %21 = arith.addi %20, %4 : i32\n",
      "    %22 = arith.muli %21, %c128_i32 : i32\n",
      "    %23 = tt.addptr %arg0, %22 : !tt.ptr<f32>, i32\n",
      "    %24 = tt.addptr %arg1, %22 : !tt.ptr<f32>, i32\n",
      "    %25 = tt.addptr %arg2, %22 : !tt.ptr<f32>, i32\n",
      "    %26 = tt.addptr %arg3, %21 : !tt.ptr<f32>, i32\n",
      "    %27 = arith.muli %21, %c64_i32 : i32\n",
      "    %28 = tt.addptr %arg4, %27 : !tt.ptr<f32>, i32\n",
      "    %29 = tt.addptr %arg5, %27 : !tt.ptr<f32>, i32\n",
      "    %30 = tt.addptr %arg6, %22 : !tt.ptr<f32>, i32\n",
      "    %31 = tt.addptr %arg7, %22 : !tt.ptr<f32>, i32\n",
      "    %32 = tt.addptr %arg8, %22 : !tt.ptr<f32>, i32\n",
      "    %33 = tt.addptr %arg9, %22 : !tt.ptr<f32>, i32\n",
      "    %34 = tt.addptr %arg10, %22 : !tt.ptr<f32>, i32\n",
      "    %35 = arith.muli %5, %7 : i32\n",
      "    %36 = arith.addi %35, %8 : i32\n",
      "    %37 = arith.muli %36, %c4_i32 : i32\n",
      "    %38 = arith.addi %37, %4 : i32\n",
      "    %39 = tt.addptr %arg11, %38 : !tt.ptr<f32>, i32\n",
      "    %40 = arith.extsi %10 : i32 to i64\n",
      "    %41 = arith.extsi %13 : i32 to i64\n",
      "    %42 = arith.extsi %15 : i32 to i64\n",
      "    %43 = tt.splat %25 : !tt.ptr<f32> -> tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %44 = tt.splat %41 : i64 -> tensor<16xi64, #blocked2>\n",
      "    %45 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #blocked2>\n",
      "    %46 = arith.extsi %45 : tensor<16xi32, #blocked2> to tensor<16xi64, #blocked2>\n",
      "    %47 = arith.addi %44, %46 : tensor<16xi64, #blocked2>\n",
      "    %48 = ttg.convert_layout %47 : tensor<16xi64, #blocked2> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>\n",
      "    %49 = tt.expand_dims %48 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>\n",
      "    %50 = ttg.convert_layout %49 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked>\n",
      "    %51 = arith.muli %50, %cst_9 : tensor<16x1xi64, #blocked>\n",
      "    %52 = tt.broadcast %51 : tensor<16x1xi64, #blocked> -> tensor<16x64xi64, #blocked>\n",
      "    %53 = ttg.convert_layout %52 : tensor<16x64xi64, #blocked> -> tensor<16x64xi64, #blocked3>\n",
      "    %54 = tt.splat %42 : i64 -> tensor<64xi64, #blocked2>\n",
      "    %55 = arith.extsi %16 : tensor<64xi32, #blocked2> to tensor<64xi64, #blocked2>\n",
      "    %56 = arith.addi %54, %55 : tensor<64xi64, #blocked2>\n",
      "    %57 = ttg.convert_layout %56 : tensor<64xi64, #blocked2> -> tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>\n",
      "    %58 = tt.expand_dims %57 {axis = 0 : i32} : tensor<64xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x64xi64, #blocked5>\n",
      "    %59 = ttg.convert_layout %58 : tensor<1x64xi64, #blocked5> -> tensor<1x64xi64, #blocked3>\n",
      "    %60 = tt.broadcast %59 : tensor<1x64xi64, #blocked3> -> tensor<16x64xi64, #blocked3>\n",
      "    %61 = arith.addi %53, %60 : tensor<16x64xi64, #blocked3>\n",
      "    %62 = tt.addptr %43, %61 : tensor<16x64x!tt.ptr<f32>, #blocked3>, tensor<16x64xi64, #blocked3>\n",
      "    %63 = arith.cmpi sge, %50, %cst_8 : tensor<16x1xi64, #blocked>\n",
      "    %64 = tt.splat %40 : i64 -> tensor<16x1xi64, #blocked>\n",
      "    %65 = arith.cmpi slt, %50, %64 : tensor<16x1xi64, #blocked>\n",
      "    %66 = arith.andi %63, %65 : tensor<16x1xi1, #blocked>\n",
      "    %67 = tt.broadcast %66 : tensor<16x1xi1, #blocked> -> tensor<16x64xi1, #blocked>\n",
      "    %68 = ttg.convert_layout %67 : tensor<16x64xi1, #blocked> -> tensor<16x64xi1, #blocked3>\n",
      "    %69 = arith.cmpi sge, %59, %cst_7 : tensor<1x64xi64, #blocked3>\n",
      "    %70 = arith.cmpi slt, %59, %cst_6 : tensor<1x64xi64, #blocked3>\n",
      "    %71 = arith.andi %69, %70 : tensor<1x64xi1, #blocked3>\n",
      "    %72 = tt.broadcast %71 : tensor<1x64xi1, #blocked3> -> tensor<16x64xi1, #blocked3>\n",
      "    %73 = arith.andi %68, %72 : tensor<16x64xi1, #blocked3>\n",
      "    %74 = tt.load %62, %73 : tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %75 = tt.splat %26 : !tt.ptr<f32> -> tensor<16x!tt.ptr<f32>, #blocked2>\n",
      "    %76 = arith.muli %47, %cst_5 : tensor<16xi64, #blocked2>\n",
      "    %77 = tt.addptr %75, %76 : tensor<16x!tt.ptr<f32>, #blocked2>, tensor<16xi64, #blocked2>\n",
      "    %78 = arith.cmpi sge, %47, %cst_4 : tensor<16xi64, #blocked2>\n",
      "    %79 = tt.splat %40 : i64 -> tensor<16xi64, #blocked2>\n",
      "    %80 = arith.cmpi slt, %47, %79 : tensor<16xi64, #blocked2>\n",
      "    %81 = arith.andi %78, %80 : tensor<16xi1, #blocked2>\n",
      "    %82 = tt.load %77, %81 : tensor<16x!tt.ptr<f32>, #blocked2>\n",
      "    %83 = arith.cmpi sgt, %6, %c0_i32 : i32\n",
      "    %84:2 = scf.if %83 -> (tensor<16x64xf32, #blocked3>, tensor<16x64xf32, #blocked3>) {\n",
      "      %170 = arith.muli %13, %c512_i32 : i32\n",
      "      %171 = tt.addptr %25, %170 : !tt.ptr<f32>, i32\n",
      "      %172 = tt.splat %171 : !tt.ptr<f32> -> tensor<64x!tt.ptr<f32>, #blocked2>\n",
      "      %173 = tt.addptr %172, %18 : tensor<64x!tt.ptr<f32>, #blocked2>, tensor<64xi32, #blocked2>\n",
      "      %174 = tt.load %173, %19, %cst_11 : tensor<64x!tt.ptr<f32>, #blocked2>\n",
      "      %175 = tt.splat %24 : !tt.ptr<f32> -> tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "      %176 = ttg.convert_layout %174 : tensor<64xf32, #blocked2> -> tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>>\n",
      "      %177 = tt.expand_dims %176 {axis = 0 : i32} : tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x64xf32, #blocked5>\n",
      "      %178 = ttg.convert_layout %177 : tensor<1x64xf32, #blocked5> -> tensor<1x64xf32, #blocked3>\n",
      "      %179 = tt.broadcast %178 : tensor<1x64xf32, #blocked3> -> tensor<16x64xf32, #blocked3>\n",
      "      %180 = tt.splat %28 : !tt.ptr<f32> -> tensor<16x16x!tt.ptr<f32>, #blocked1>\n",
      "      %181 = arith.muli %50, %cst_3 : tensor<16x1xi64, #blocked>\n",
      "      %182 = tt.broadcast %181 : tensor<16x1xi64, #blocked> -> tensor<16x16xi64, #blocked>\n",
      "      %183 = ttg.convert_layout %182 : tensor<16x16xi64, #blocked> -> tensor<16x16xi64, #blocked1>\n",
      "      %184 = tt.broadcast %66 : tensor<16x1xi1, #blocked> -> tensor<16x16xi1, #blocked>\n",
      "      %185 = ttg.convert_layout %184 : tensor<16x16xi1, #blocked> -> tensor<16x16xi1, #blocked1>\n",
      "      %186 = tt.splat %29 : !tt.ptr<f32> -> tensor<16x16x!tt.ptr<f32>, #blocked1>\n",
      "      %187:2 = scf.for %arg14 = %c0_i32 to %6 step %c1_i32 iter_args(%arg15 = %cst_15, %arg16 = %cst_15) -> (tensor<16x64xf32, #blocked3>, tensor<16x64xf32, #blocked3>)  : i32 {\n",
      "        %196 = arith.muli %arg14, %c16_i32 : i32\n",
      "        %197 = arith.addi %11, %196 : i32\n",
      "        %198 = arith.extsi %197 : i32 to i64\n",
      "        %199 = arith.extsi %196 : i32 to i64\n",
      "        %200 = tt.splat %198 : i64 -> tensor<16xi64, #blocked2>\n",
      "        %201 = arith.addi %200, %46 : tensor<16xi64, #blocked2>\n",
      "        %202 = ttg.convert_layout %201 : tensor<16xi64, #blocked2> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>\n",
      "        %203 = tt.expand_dims %202 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>\n",
      "        %204 = ttg.convert_layout %203 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked>\n",
      "        %205 = arith.muli %204, %cst_9 : tensor<16x1xi64, #blocked>\n",
      "        %206 = tt.broadcast %205 : tensor<16x1xi64, #blocked> -> tensor<16x64xi64, #blocked>\n",
      "        %207 = ttg.convert_layout %206 : tensor<16x64xi64, #blocked> -> tensor<16x64xi64, #blocked3>\n",
      "        %208 = arith.addi %207, %60 : tensor<16x64xi64, #blocked3>\n",
      "        %209 = tt.addptr %175, %208 : tensor<16x64x!tt.ptr<f32>, #blocked3>, tensor<16x64xi64, #blocked3>\n",
      "        %210 = arith.cmpi sge, %204, %cst_8 : tensor<16x1xi64, #blocked>\n",
      "        %211 = arith.cmpi slt, %204, %64 : tensor<16x1xi64, #blocked>\n",
      "        %212 = arith.andi %210, %211 : tensor<16x1xi1, #blocked>\n",
      "        %213 = tt.broadcast %212 : tensor<16x1xi1, #blocked> -> tensor<16x64xi1, #blocked>\n",
      "        %214 = ttg.convert_layout %213 : tensor<16x64xi1, #blocked> -> tensor<16x64xi1, #blocked3>\n",
      "        %215 = arith.andi %214, %72 : tensor<16x64xi1, #blocked3>\n",
      "        %216 = tt.load %209, %215 : tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "        %217 = tt.addptr %43, %208 : tensor<16x64x!tt.ptr<f32>, #blocked3>, tensor<16x64xi64, #blocked3>\n",
      "        %218 = tt.load %217, %215 : tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "        %219 = arith.subf %179, %218 : tensor<16x64xf32, #blocked3>\n",
      "        %220 = math.exp %219 : tensor<16x64xf32, #blocked3>\n",
      "        %221 = arith.mulf %216, %220 : tensor<16x64xf32, #blocked3>\n",
      "        %222 = tt.splat %199 : i64 -> tensor<16xi64, #blocked2>\n",
      "        %223 = arith.addi %222, %46 : tensor<16xi64, #blocked2>\n",
      "        %224 = ttg.convert_layout %223 : tensor<16xi64, #blocked2> -> tensor<16xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>\n",
      "        %225 = tt.expand_dims %224 {axis = 0 : i32} : tensor<16xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi64, #blocked5>\n",
      "        %226 = ttg.convert_layout %225 : tensor<1x16xi64, #blocked5> -> tensor<1x16xi64, #blocked1>\n",
      "        %227 = tt.broadcast %226 : tensor<1x16xi64, #blocked1> -> tensor<16x16xi64, #blocked1>\n",
      "        %228 = arith.addi %183, %227 : tensor<16x16xi64, #blocked1>\n",
      "        %229 = tt.addptr %180, %228 : tensor<16x16x!tt.ptr<f32>, #blocked1>, tensor<16x16xi64, #blocked1>\n",
      "        %230 = arith.cmpi sge, %226, %cst_2 : tensor<1x16xi64, #blocked1>\n",
      "        %231 = arith.cmpi slt, %226, %cst_1 : tensor<1x16xi64, #blocked1>\n",
      "        %232 = arith.andi %230, %231 : tensor<1x16xi1, #blocked1>\n",
      "        %233 = tt.broadcast %232 : tensor<1x16xi1, #blocked1> -> tensor<16x16xi1, #blocked1>\n",
      "        %234 = arith.andi %185, %233 : tensor<16x16xi1, #blocked1>\n",
      "        %235 = tt.load %229, %234 : tensor<16x16x!tt.ptr<f32>, #blocked1>\n",
      "        %236 = tt.addptr %186, %228 : tensor<16x16x!tt.ptr<f32>, #blocked1>, tensor<16x16xi64, #blocked1>\n",
      "        %237 = tt.load %236, %234 : tensor<16x16x!tt.ptr<f32>, #blocked1>\n",
      "        %238 = ttg.convert_layout %235 : tensor<16x16xf32, #blocked1> -> tensor<16x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>\n",
      "        %239 = ttg.convert_layout %221 : tensor<16x64xf32, #blocked3> -> tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>\n",
      "        %240 = ttg.convert_layout %arg15 : tensor<16x64xf32, #blocked3> -> tensor<16x64xf32, #blocked6>\n",
      "        %241 = tt.dot %238, %239, %240 : tensor<16x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<16x64xf32, #blocked6>\n",
      "        %242 = ttg.convert_layout %241 : tensor<16x64xf32, #blocked6> -> tensor<16x64xf32, #blocked3>\n",
      "        %243 = ttg.convert_layout %237 : tensor<16x16xf32, #blocked1> -> tensor<16x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>\n",
      "        %244 = ttg.convert_layout %221 : tensor<16x64xf32, #blocked3> -> tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>\n",
      "        %245 = ttg.convert_layout %arg16 : tensor<16x64xf32, #blocked3> -> tensor<16x64xf32, #blocked6>\n",
      "        %246 = tt.dot %243, %244, %245 : tensor<16x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<16x64xf32, #blocked6>\n",
      "        %247 = ttg.convert_layout %246 : tensor<16x64xf32, #blocked6> -> tensor<16x64xf32, #blocked3>\n",
      "        scf.yield %242, %247 : tensor<16x64xf32, #blocked3>, tensor<16x64xf32, #blocked3>\n",
      "      }\n",
      "      %188 = ttg.convert_layout %174 : tensor<64xf32, #blocked2> -> tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>>\n",
      "      %189 = tt.expand_dims %188 {axis = 0 : i32} : tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x64xf32, #blocked5>\n",
      "      %190 = ttg.convert_layout %189 : tensor<1x64xf32, #blocked5> -> tensor<1x64xf32, #blocked3>\n",
      "      %191 = tt.broadcast %190 : tensor<1x64xf32, #blocked3> -> tensor<16x64xf32, #blocked3>\n",
      "      %192 = arith.subf %74, %191 : tensor<16x64xf32, #blocked3>\n",
      "      %193 = math.exp %192 : tensor<16x64xf32, #blocked3>\n",
      "      %194 = arith.mulf %187#0, %193 : tensor<16x64xf32, #blocked3>\n",
      "      %195 = arith.mulf %187#1, %193 : tensor<16x64xf32, #blocked3>\n",
      "      scf.yield %194, %195 : tensor<16x64xf32, #blocked3>, tensor<16x64xf32, #blocked3>\n",
      "    } else {\n",
      "      scf.yield %cst_15, %cst_15 : tensor<16x64xf32, #blocked3>, tensor<16x64xf32, #blocked3>\n",
      "    }\n",
      "    %85 = tt.splat %13 : i32 -> tensor<16xi32, #blocked2>\n",
      "    %86 = arith.addi %85, %45 : tensor<16xi32, #blocked2>\n",
      "    %87 = tt.splat %10 : i32 -> tensor<16xi32, #blocked2>\n",
      "    %88 = arith.cmpi slt, %86, %87 : tensor<16xi32, #blocked2>\n",
      "    %89 = arith.muli %86, %cst_14 : tensor<16xi32, #blocked2>\n",
      "    %90 = arith.muli %89, %cst_13 : tensor<16xi32, #blocked2>\n",
      "    %91 = tt.splat %12 : i32 -> tensor<16xi32, #blocked2>\n",
      "    %92 = arith.addi %90, %91 : tensor<16xi32, #blocked2>\n",
      "    %93 = arith.muli %13, %c512_i32 : i32\n",
      "    %94 = tt.addptr %24, %93 : !tt.ptr<f32>, i32\n",
      "    %95 = tt.splat %94 : !tt.ptr<f32> -> tensor<64x!tt.ptr<f32>, #blocked2>\n",
      "    %96 = tt.addptr %95, %18 : tensor<64x!tt.ptr<f32>, #blocked2>, tensor<64xi32, #blocked2>\n",
      "    %97 = tt.addptr %25, %93 : !tt.ptr<f32>, i32\n",
      "    %98 = tt.splat %97 : !tt.ptr<f32> -> tensor<64x!tt.ptr<f32>, #blocked2>\n",
      "    %99 = tt.addptr %98, %18 : tensor<64x!tt.ptr<f32>, #blocked2>, tensor<64xi32, #blocked2>\n",
      "    %100 = tt.splat %23 : !tt.ptr<f32> -> tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %101 = tt.addptr %100, %61 : tensor<16x64x!tt.ptr<f32>, #blocked3>, tensor<16x64xi64, #blocked3>\n",
      "    %102 = tt.load %101, %73 : tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %103 = tt.splat %24 : !tt.ptr<f32> -> tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %104 = tt.addptr %103, %61 : tensor<16x64x!tt.ptr<f32>, #blocked3>, tensor<16x64xi64, #blocked3>\n",
      "    %105 = tt.load %104, %73 : tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %106 = arith.subi %10, %11 : i32\n",
      "    %107 = arith.subi %106, %12 : i32\n",
      "    %108 = arith.minsi %107, %c16_i32 : i32\n",
      "    %109 = tt.splat %28 : !tt.ptr<f32> -> tensor<16x!tt.ptr<f32>, #blocked2>\n",
      "    %110 = tt.addptr %109, %92 : tensor<16x!tt.ptr<f32>, #blocked2>, tensor<16xi32, #blocked2>\n",
      "    %111 = tt.splat %29 : !tt.ptr<f32> -> tensor<16x!tt.ptr<f32>, #blocked2>\n",
      "    %112 = tt.addptr %111, %92 : tensor<16x!tt.ptr<f32>, #blocked2>, tensor<16xi32, #blocked2>\n",
      "    %113 = ttg.convert_layout %45 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>\n",
      "    %114 = tt.expand_dims %113 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>\n",
      "    %115 = ttg.convert_layout %114 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked>\n",
      "    %116:4 = scf.for %arg14 = %c0_i32 to %108 step %c1_i32 iter_args(%arg15 = %84#0, %arg16 = %84#1, %arg17 = %96, %arg18 = %99) -> (tensor<16x64xf32, #blocked3>, tensor<16x64xf32, #blocked3>, tensor<64x!tt.ptr<f32>, #blocked2>, tensor<64x!tt.ptr<f32>, #blocked2>)  : i32 {\n",
      "      %170 = tt.splat %arg14 : i32 -> tensor<16xi32, #blocked2>\n",
      "      %171 = tt.addptr %110, %170 : tensor<16x!tt.ptr<f32>, #blocked2>, tensor<16xi32, #blocked2>\n",
      "      %172 = tt.load %171, %88, %cst_10 : tensor<16x!tt.ptr<f32>, #blocked2>\n",
      "      %173 = tt.addptr %112, %170 : tensor<16x!tt.ptr<f32>, #blocked2>, tensor<16xi32, #blocked2>\n",
      "      %174 = tt.load %173, %88, %cst_10 : tensor<16x!tt.ptr<f32>, #blocked2>\n",
      "      %175 = tt.load %arg17, %19, %cst_11 : tensor<64x!tt.ptr<f32>, #blocked2>\n",
      "      %176 = tt.load %arg18, %19, %cst_11 : tensor<64x!tt.ptr<f32>, #blocked2>\n",
      "      %177 = tt.splat %arg14 : i32 -> tensor<16x1xi32, #blocked>\n",
      "      %178 = arith.cmpi sge, %115, %177 : tensor<16x1xi32, #blocked>\n",
      "      %179 = ttg.convert_layout %172 : tensor<16xf32, #blocked2> -> tensor<16xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>\n",
      "      %180 = tt.expand_dims %179 {axis = 1 : i32} : tensor<16xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xf32, #blocked4>\n",
      "      %181 = ttg.convert_layout %180 : tensor<16x1xf32, #blocked4> -> tensor<16x1xf32, #blocked>\n",
      "      %182 = ttg.convert_layout %175 : tensor<64xf32, #blocked2> -> tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>>\n",
      "      %183 = tt.expand_dims %182 {axis = 0 : i32} : tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x64xf32, #blocked5>\n",
      "      %184 = ttg.convert_layout %183 : tensor<1x64xf32, #blocked5> -> tensor<1x64xf32, #blocked3>\n",
      "      %185 = tt.broadcast %181 : tensor<16x1xf32, #blocked> -> tensor<16x64xf32, #blocked>\n",
      "      %186 = ttg.convert_layout %185 : tensor<16x64xf32, #blocked> -> tensor<16x64xf32, #blocked3>\n",
      "      %187 = tt.broadcast %184 : tensor<1x64xf32, #blocked3> -> tensor<16x64xf32, #blocked3>\n",
      "      %188 = arith.mulf %186, %187 : tensor<16x64xf32, #blocked3>\n",
      "      %189 = ttg.convert_layout %176 : tensor<64xf32, #blocked2> -> tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>>\n",
      "      %190 = tt.expand_dims %189 {axis = 0 : i32} : tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x64xf32, #blocked5>\n",
      "      %191 = ttg.convert_layout %190 : tensor<1x64xf32, #blocked5> -> tensor<1x64xf32, #blocked3>\n",
      "      %192 = tt.broadcast %191 : tensor<1x64xf32, #blocked3> -> tensor<16x64xf32, #blocked3>\n",
      "      %193 = arith.subf %74, %192 : tensor<16x64xf32, #blocked3>\n",
      "      %194 = math.exp %193 : tensor<16x64xf32, #blocked3>\n",
      "      %195 = arith.mulf %188, %194 : tensor<16x64xf32, #blocked3>\n",
      "      %196 = tt.broadcast %178 : tensor<16x1xi1, #blocked> -> tensor<16x64xi1, #blocked>\n",
      "      %197 = ttg.convert_layout %196 : tensor<16x64xi1, #blocked> -> tensor<16x64xi1, #blocked3>\n",
      "      %198 = arith.select %197, %195, %cst_15 : tensor<16x64xi1, #blocked3>, tensor<16x64xf32, #blocked3>\n",
      "      %199 = arith.addf %arg15, %198 : tensor<16x64xf32, #blocked3>\n",
      "      %200 = ttg.convert_layout %174 : tensor<16xf32, #blocked2> -> tensor<16xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>\n",
      "      %201 = tt.expand_dims %200 {axis = 1 : i32} : tensor<16xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xf32, #blocked4>\n",
      "      %202 = ttg.convert_layout %201 : tensor<16x1xf32, #blocked4> -> tensor<16x1xf32, #blocked>\n",
      "      %203 = tt.broadcast %202 : tensor<16x1xf32, #blocked> -> tensor<16x64xf32, #blocked>\n",
      "      %204 = ttg.convert_layout %203 : tensor<16x64xf32, #blocked> -> tensor<16x64xf32, #blocked3>\n",
      "      %205 = arith.mulf %204, %187 : tensor<16x64xf32, #blocked3>\n",
      "      %206 = arith.mulf %205, %194 : tensor<16x64xf32, #blocked3>\n",
      "      %207 = arith.select %197, %206, %cst_15 : tensor<16x64xi1, #blocked3>, tensor<16x64xf32, #blocked3>\n",
      "      %208 = arith.addf %arg16, %207 : tensor<16x64xf32, #blocked3>\n",
      "      %209 = tt.addptr %arg17, %cst_12 : tensor<64x!tt.ptr<f32>, #blocked2>, tensor<64xi32, #blocked2>\n",
      "      %210 = tt.addptr %arg18, %cst_12 : tensor<64x!tt.ptr<f32>, #blocked2>, tensor<64xi32, #blocked2>\n",
      "      scf.yield %199, %208, %209, %210 : tensor<16x64xf32, #blocked3>, tensor<16x64xf32, #blocked3>, tensor<64x!tt.ptr<f32>, #blocked2>, tensor<64x!tt.ptr<f32>, #blocked2>\n",
      "    }\n",
      "    %117 = arith.mulf %116#1, %105 : tensor<16x64xf32, #blocked3>\n",
      "    %118 = \"tt.reduce\"(%117) <{axis = 1 : i32}> ({\n",
      "    ^bb0(%arg14: f32, %arg15: f32):\n",
      "      %170 = arith.addf %arg14, %arg15 : f32\n",
      "      tt.reduce.return %170 : f32\n",
      "    }) : (tensor<16x64xf32, #blocked3>) -> tensor<16xf32, #ttg.slice<{dim = 1, parent = #blocked3}>>\n",
      "    %119 = ttg.convert_layout %118 : tensor<16xf32, #ttg.slice<{dim = 1, parent = #blocked3}>> -> tensor<16xf32, #blocked2>\n",
      "    %120 = ttg.convert_layout %82 : tensor<16xf32, #blocked2> -> tensor<16xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>\n",
      "    %121 = tt.expand_dims %120 {axis = 1 : i32} : tensor<16xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xf32, #blocked4>\n",
      "    %122 = ttg.convert_layout %121 : tensor<16x1xf32, #blocked4> -> tensor<16x1xf32, #blocked>\n",
      "    %123 = tt.broadcast %122 : tensor<16x1xf32, #blocked> -> tensor<16x64xf32, #blocked>\n",
      "    %124 = ttg.convert_layout %123 : tensor<16x64xf32, #blocked> -> tensor<16x64xf32, #blocked3>\n",
      "    %125 = arith.mulf %116#1, %124 : tensor<16x64xf32, #blocked3>\n",
      "    %126 = arith.mulf %102, %116#0 : tensor<16x64xf32, #blocked3>\n",
      "    %127 = tt.splat %30 : !tt.ptr<f32> -> tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %128 = tt.addptr %127, %61 : tensor<16x64x!tt.ptr<f32>, #blocked3>, tensor<16x64xi64, #blocked3>\n",
      "    %129 = tt.load %128, %73 : tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %130 = arith.addf %116#0, %129 : tensor<16x64xf32, #blocked3>\n",
      "    %131 = tt.splat %31 : !tt.ptr<f32> -> tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %132 = tt.addptr %131, %61 : tensor<16x64x!tt.ptr<f32>, #blocked3>, tensor<16x64xi64, #blocked3>\n",
      "    tt.store %132, %130, %73 : tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %133 = tt.splat %39 : !tt.ptr<f32> -> tensor<16x!tt.ptr<f32>, #blocked2>\n",
      "    %134 = tt.addptr %133, %76 : tensor<16x!tt.ptr<f32>, #blocked2>, tensor<16xi64, #blocked2>\n",
      "    tt.store %134, %119, %81 : tensor<16x!tt.ptr<f32>, #blocked2>\n",
      "    gpu.barrier\n",
      "    %135 = arith.addi %106, %c15_i32 : i32\n",
      "    %136 = arith.divsi %135, %c16_i32 : i32\n",
      "    %137 = arith.minsi %136, %c4_i32 : i32\n",
      "    %138 = arith.subi %137, %c1_i32 : i32\n",
      "    %139 = arith.cmpi slt, %6, %138 : i32\n",
      "    %140 = scf.if %139 -> (tensor<16x64xf32, #blocked3>) {\n",
      "      %170 = arith.addi %13, %c16_i32 : i32\n",
      "      %171 = arith.minsi %170, %10 : i32\n",
      "      %172 = arith.subi %171, %c1_i32 : i32\n",
      "      %173 = arith.muli %172, %c512_i32 : i32\n",
      "      %174 = tt.addptr %25, %173 : !tt.ptr<f32>, i32\n",
      "      %175 = tt.splat %174 : !tt.ptr<f32> -> tensor<64x!tt.ptr<f32>, #blocked2>\n",
      "      %176 = tt.addptr %175, %18 : tensor<64x!tt.ptr<f32>, #blocked2>, tensor<64xi32, #blocked2>\n",
      "      %177 = tt.load %176, %19, %cst_11 : tensor<64x!tt.ptr<f32>, #blocked2>\n",
      "      %178 = arith.addi %6, %c1_i32 : i32\n",
      "      %179 = arith.extsi %12 : i32 to i64\n",
      "      %180 = tt.splat %28 : !tt.ptr<f32> -> tensor<16x16x!tt.ptr<f32>, #blocked1>\n",
      "      %181 = tt.splat %179 : i64 -> tensor<16xi64, #blocked2>\n",
      "      %182 = arith.addi %181, %46 : tensor<16xi64, #blocked2>\n",
      "      %183 = ttg.convert_layout %182 : tensor<16xi64, #blocked2> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>\n",
      "      %184 = tt.expand_dims %183 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>\n",
      "      %185 = ttg.convert_layout %184 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked>\n",
      "      %186 = tt.broadcast %185 : tensor<16x1xi64, #blocked> -> tensor<16x16xi64, #blocked>\n",
      "      %187 = ttg.convert_layout %186 : tensor<16x16xi64, #blocked> -> tensor<16x16xi64, #blocked1>\n",
      "      %188 = arith.cmpi sge, %185, %cst_8 : tensor<16x1xi64, #blocked>\n",
      "      %189 = arith.cmpi slt, %185, %cst : tensor<16x1xi64, #blocked>\n",
      "      %190 = arith.andi %188, %189 : tensor<16x1xi1, #blocked>\n",
      "      %191 = tt.broadcast %190 : tensor<16x1xi1, #blocked> -> tensor<16x16xi1, #blocked>\n",
      "      %192 = ttg.convert_layout %191 : tensor<16x16xi1, #blocked> -> tensor<16x16xi1, #blocked1>\n",
      "      %193 = tt.splat %40 : i64 -> tensor<1x16xi64, #blocked1>\n",
      "      %194 = tt.splat %29 : !tt.ptr<f32> -> tensor<16x16x!tt.ptr<f32>, #blocked1>\n",
      "      %195 = ttg.convert_layout %177 : tensor<64xf32, #blocked2> -> tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>>\n",
      "      %196 = tt.expand_dims %195 {axis = 0 : i32} : tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x64xf32, #blocked5>\n",
      "      %197 = ttg.convert_layout %196 : tensor<1x64xf32, #blocked5> -> tensor<1x64xf32, #blocked3>\n",
      "      %198 = tt.broadcast %197 : tensor<1x64xf32, #blocked3> -> tensor<16x64xf32, #blocked3>\n",
      "      %199 = scf.for %arg14 = %178 to %137 step %c1_i32 iter_args(%arg15 = %cst_15) -> (tensor<16x64xf32, #blocked3>)  : i32 {\n",
      "        %207 = arith.muli %arg14, %c16_i32 : i32\n",
      "        %208 = arith.addi %11, %207 : i32\n",
      "        %209 = arith.extsi %208 : i32 to i64\n",
      "        %210 = tt.splat %209 : i64 -> tensor<16xi64, #blocked2>\n",
      "        %211 = arith.addi %210, %46 : tensor<16xi64, #blocked2>\n",
      "        %212 = arith.muli %211, %cst_5 : tensor<16xi64, #blocked2>\n",
      "        %213 = tt.addptr %75, %212 : tensor<16x!tt.ptr<f32>, #blocked2>, tensor<16xi64, #blocked2>\n",
      "        %214 = arith.cmpi sge, %211, %cst_4 : tensor<16xi64, #blocked2>\n",
      "        %215 = arith.cmpi slt, %211, %79 : tensor<16xi64, #blocked2>\n",
      "        %216 = arith.andi %214, %215 : tensor<16xi1, #blocked2>\n",
      "        %217 = tt.load %213, %216 : tensor<16x!tt.ptr<f32>, #blocked2>\n",
      "        %218 = ttg.convert_layout %211 : tensor<16xi64, #blocked2> -> tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>>\n",
      "        %219 = tt.expand_dims %218 {axis = 1 : i32} : tensor<16xi64, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi64, #blocked4>\n",
      "        %220 = ttg.convert_layout %219 : tensor<16x1xi64, #blocked4> -> tensor<16x1xi64, #blocked>\n",
      "        %221 = arith.muli %220, %cst_9 : tensor<16x1xi64, #blocked>\n",
      "        %222 = tt.broadcast %221 : tensor<16x1xi64, #blocked> -> tensor<16x64xi64, #blocked>\n",
      "        %223 = ttg.convert_layout %222 : tensor<16x64xi64, #blocked> -> tensor<16x64xi64, #blocked3>\n",
      "        %224 = arith.addi %223, %60 : tensor<16x64xi64, #blocked3>\n",
      "        %225 = tt.addptr %100, %224 : tensor<16x64x!tt.ptr<f32>, #blocked3>, tensor<16x64xi64, #blocked3>\n",
      "        %226 = arith.cmpi sge, %220, %cst_8 : tensor<16x1xi64, #blocked>\n",
      "        %227 = arith.cmpi slt, %220, %64 : tensor<16x1xi64, #blocked>\n",
      "        %228 = arith.andi %226, %227 : tensor<16x1xi1, #blocked>\n",
      "        %229 = tt.broadcast %228 : tensor<16x1xi1, #blocked> -> tensor<16x64xi1, #blocked>\n",
      "        %230 = ttg.convert_layout %229 : tensor<16x64xi1, #blocked> -> tensor<16x64xi1, #blocked3>\n",
      "        %231 = arith.andi %230, %72 : tensor<16x64xi1, #blocked3>\n",
      "        %232 = tt.load %225, %231 : tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "        %233 = tt.addptr %103, %224 : tensor<16x64x!tt.ptr<f32>, #blocked3>, tensor<16x64xi64, #blocked3>\n",
      "        %234 = tt.load %233, %231 : tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "        %235 = ttg.convert_layout %217 : tensor<16xf32, #blocked2> -> tensor<16xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>\n",
      "        %236 = tt.expand_dims %235 {axis = 1 : i32} : tensor<16xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xf32, #blocked4>\n",
      "        %237 = ttg.convert_layout %236 : tensor<16x1xf32, #blocked4> -> tensor<16x1xf32, #blocked>\n",
      "        %238 = tt.broadcast %237 : tensor<16x1xf32, #blocked> -> tensor<16x64xf32, #blocked>\n",
      "        %239 = ttg.convert_layout %238 : tensor<16x64xf32, #blocked> -> tensor<16x64xf32, #blocked3>\n",
      "        %240 = arith.mulf %234, %239 : tensor<16x64xf32, #blocked3>\n",
      "        %241 = tt.addptr %43, %224 : tensor<16x64x!tt.ptr<f32>, #blocked3>, tensor<16x64xi64, #blocked3>\n",
      "        %242 = tt.load %241, %231 : tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "        %243 = ttg.convert_layout %211 : tensor<16xi64, #blocked2> -> tensor<16xi64, #ttg.slice<{dim = 0, parent = #blocked5}>>\n",
      "        %244 = tt.expand_dims %243 {axis = 0 : i32} : tensor<16xi64, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x16xi64, #blocked5>\n",
      "        %245 = ttg.convert_layout %244 : tensor<1x16xi64, #blocked5> -> tensor<1x16xi64, #blocked1>\n",
      "        %246 = arith.muli %245, %cst_0 : tensor<1x16xi64, #blocked1>\n",
      "        %247 = tt.broadcast %246 : tensor<1x16xi64, #blocked1> -> tensor<16x16xi64, #blocked1>\n",
      "        %248 = arith.addi %187, %247 : tensor<16x16xi64, #blocked1>\n",
      "        %249 = tt.addptr %180, %248 : tensor<16x16x!tt.ptr<f32>, #blocked1>, tensor<16x16xi64, #blocked1>\n",
      "        %250 = arith.cmpi sge, %245, %cst_2 : tensor<1x16xi64, #blocked1>\n",
      "        %251 = arith.cmpi slt, %245, %193 : tensor<1x16xi64, #blocked1>\n",
      "        %252 = arith.andi %250, %251 : tensor<1x16xi1, #blocked1>\n",
      "        %253 = tt.broadcast %252 : tensor<1x16xi1, #blocked1> -> tensor<16x16xi1, #blocked1>\n",
      "        %254 = arith.andi %192, %253 : tensor<16x16xi1, #blocked1>\n",
      "        %255 = tt.load %249, %254 : tensor<16x16x!tt.ptr<f32>, #blocked1>\n",
      "        %256 = tt.addptr %194, %248 : tensor<16x16x!tt.ptr<f32>, #blocked1>, tensor<16x16xi64, #blocked1>\n",
      "        %257 = tt.load %256, %254 : tensor<16x16x!tt.ptr<f32>, #blocked1>\n",
      "        %258 = tt.splat %208 : i32 -> tensor<16xi32, #blocked2>\n",
      "        %259 = arith.addi %258, %45 : tensor<16xi32, #blocked2>\n",
      "        %260 = arith.cmpi slt, %259, %87 : tensor<16xi32, #blocked2>\n",
      "        %261 = ttg.convert_layout %260 : tensor<16xi1, #blocked2> -> tensor<16xi1, #ttg.slice<{dim = 1, parent = #blocked4}>>\n",
      "        %262 = tt.expand_dims %261 {axis = 1 : i32} : tensor<16xi1, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi1, #blocked4>\n",
      "        %263 = ttg.convert_layout %262 : tensor<16x1xi1, #blocked4> -> tensor<16x1xi1, #blocked>\n",
      "        %264 = arith.subf %242, %198 : tensor<16x64xf32, #blocked3>\n",
      "        %265 = math.exp %264 : tensor<16x64xf32, #blocked3>\n",
      "        %266 = tt.broadcast %263 : tensor<16x1xi1, #blocked> -> tensor<16x64xi1, #blocked>\n",
      "        %267 = ttg.convert_layout %266 : tensor<16x64xi1, #blocked> -> tensor<16x64xi1, #blocked3>\n",
      "        %268 = arith.select %267, %265, %cst_15 : tensor<16x64xi1, #blocked3>, tensor<16x64xf32, #blocked3>\n",
      "        %269 = arith.mulf %232, %268 : tensor<16x64xf32, #blocked3>\n",
      "        %270 = arith.mulf %240, %268 : tensor<16x64xf32, #blocked3>\n",
      "        %271 = ttg.convert_layout %255 : tensor<16x16xf32, #blocked1> -> tensor<16x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>\n",
      "        %272 = ttg.convert_layout %269 : tensor<16x64xf32, #blocked3> -> tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>\n",
      "        %273 = ttg.convert_layout %arg15 : tensor<16x64xf32, #blocked3> -> tensor<16x64xf32, #blocked6>\n",
      "        %274 = tt.dot %271, %272, %273 : tensor<16x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<16x64xf32, #blocked6>\n",
      "        %275 = ttg.convert_layout %274 : tensor<16x64xf32, #blocked6> -> tensor<16x64xf32, #blocked3>\n",
      "        %276 = ttg.convert_layout %257 : tensor<16x16xf32, #blocked1> -> tensor<16x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>>\n",
      "        %277 = ttg.convert_layout %270 : tensor<16x64xf32, #blocked3> -> tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>>\n",
      "        %278 = ttg.convert_layout %275 : tensor<16x64xf32, #blocked3> -> tensor<16x64xf32, #blocked6>\n",
      "        %279 = tt.dot %276, %277, %278 : tensor<16x16xf32, #ttg.dot_op<{opIdx = 0, parent = #blocked6}>> * tensor<16x64xf32, #ttg.dot_op<{opIdx = 1, parent = #blocked6}>> -> tensor<16x64xf32, #blocked6>\n",
      "        %280 = ttg.convert_layout %279 : tensor<16x64xf32, #blocked6> -> tensor<16x64xf32, #blocked3>\n",
      "        scf.yield %280 : tensor<16x64xf32, #blocked3>\n",
      "      }\n",
      "      %200 = ttg.convert_layout %177 : tensor<64xf32, #blocked2> -> tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>>\n",
      "      %201 = tt.expand_dims %200 {axis = 0 : i32} : tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x64xf32, #blocked5>\n",
      "      %202 = ttg.convert_layout %201 : tensor<1x64xf32, #blocked5> -> tensor<1x64xf32, #blocked3>\n",
      "      %203 = tt.broadcast %202 : tensor<1x64xf32, #blocked3> -> tensor<16x64xf32, #blocked3>\n",
      "      %204 = arith.subf %203, %74 : tensor<16x64xf32, #blocked3>\n",
      "      %205 = math.exp %204 : tensor<16x64xf32, #blocked3>\n",
      "      %206 = arith.mulf %199, %205 : tensor<16x64xf32, #blocked3>\n",
      "      scf.yield %206 : tensor<16x64xf32, #blocked3>\n",
      "    } else {\n",
      "      scf.yield %cst_15 : tensor<16x64xf32, #blocked3>\n",
      "    }\n",
      "    %141 = arith.muli %13, %c256_i32 : i32\n",
      "    %142 = arith.addi %141, %12 : i32\n",
      "    %143 = tt.splat %142 : i32 -> tensor<16xi32, #blocked2>\n",
      "    %144 = arith.addi %143, %45 : tensor<16xi32, #blocked2>\n",
      "    %145 = tt.addptr %23, %93 : !tt.ptr<f32>, i32\n",
      "    %146 = tt.splat %145 : !tt.ptr<f32> -> tensor<64x!tt.ptr<f32>, #blocked2>\n",
      "    %147 = tt.addptr %146, %18 : tensor<64x!tt.ptr<f32>, #blocked2>, tensor<64xi32, #blocked2>\n",
      "    %148 = arith.muli %13, %c4_i32 : i32\n",
      "    %149 = tt.addptr %26, %148 : !tt.ptr<f32>, i32\n",
      "    %150 = tt.splat %28 : !tt.ptr<f32> -> tensor<16x!tt.ptr<f32>, #blocked2>\n",
      "    %151 = tt.addptr %150, %144 : tensor<16x!tt.ptr<f32>, #blocked2>, tensor<16xi32, #blocked2>\n",
      "    %152 = tt.splat %29 : !tt.ptr<f32> -> tensor<16x!tt.ptr<f32>, #blocked2>\n",
      "    %153 = tt.addptr %152, %144 : tensor<16x!tt.ptr<f32>, #blocked2>, tensor<16xi32, #blocked2>\n",
      "    %154 = ttg.convert_layout %45 : tensor<16xi32, #blocked2> -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>>\n",
      "    %155 = tt.expand_dims %154 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xi32, #blocked4>\n",
      "    %156 = ttg.convert_layout %155 : tensor<16x1xi32, #blocked4> -> tensor<16x1xi32, #blocked>\n",
      "    %157:5 = scf.for %arg14 = %c0_i32 to %108 step %c1_i32 iter_args(%arg15 = %96, %arg16 = %99, %arg17 = %140, %arg18 = %147, %arg19 = %149) -> (tensor<64x!tt.ptr<f32>, #blocked2>, tensor<64x!tt.ptr<f32>, #blocked2>, tensor<16x64xf32, #blocked3>, tensor<64x!tt.ptr<f32>, #blocked2>, !tt.ptr<f32>)  : i32 {\n",
      "      %170 = arith.muli %arg14, %c256_i32 : i32\n",
      "      %171 = tt.splat %170 : i32 -> tensor<16xi32, #blocked2>\n",
      "      %172 = tt.addptr %151, %171 : tensor<16x!tt.ptr<f32>, #blocked2>, tensor<16xi32, #blocked2>\n",
      "      %173 = tt.load %172 : tensor<16x!tt.ptr<f32>, #blocked2>\n",
      "      %174 = tt.addptr %153, %171 : tensor<16x!tt.ptr<f32>, #blocked2>, tensor<16xi32, #blocked2>\n",
      "      %175 = tt.load %174 : tensor<16x!tt.ptr<f32>, #blocked2>\n",
      "      %176 = tt.load %arg18, %19, %cst_11 : tensor<64x!tt.ptr<f32>, #blocked2>\n",
      "      %177 = tt.load %arg15, %19, %cst_11 : tensor<64x!tt.ptr<f32>, #blocked2>\n",
      "      %178 = tt.load %arg19 : !tt.ptr<f32>\n",
      "      %179 = tt.splat %178 : f32 -> tensor<64xf32, #blocked2>\n",
      "      %180 = arith.mulf %177, %179 : tensor<64xf32, #blocked2>\n",
      "      %181 = tt.load %arg16, %19, %cst_11 : tensor<64x!tt.ptr<f32>, #blocked2>\n",
      "      %182 = tt.splat %arg14 : i32 -> tensor<16x1xi32, #blocked>\n",
      "      %183 = arith.cmpi sle, %156, %182 : tensor<16x1xi32, #blocked>\n",
      "      %184 = ttg.convert_layout %173 : tensor<16xf32, #blocked2> -> tensor<16xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>\n",
      "      %185 = tt.expand_dims %184 {axis = 1 : i32} : tensor<16xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xf32, #blocked4>\n",
      "      %186 = ttg.convert_layout %185 : tensor<16x1xf32, #blocked4> -> tensor<16x1xf32, #blocked>\n",
      "      %187 = ttg.convert_layout %176 : tensor<64xf32, #blocked2> -> tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>>\n",
      "      %188 = tt.expand_dims %187 {axis = 0 : i32} : tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x64xf32, #blocked5>\n",
      "      %189 = ttg.convert_layout %188 : tensor<1x64xf32, #blocked5> -> tensor<1x64xf32, #blocked3>\n",
      "      %190 = tt.broadcast %186 : tensor<16x1xf32, #blocked> -> tensor<16x64xf32, #blocked>\n",
      "      %191 = ttg.convert_layout %190 : tensor<16x64xf32, #blocked> -> tensor<16x64xf32, #blocked3>\n",
      "      %192 = tt.broadcast %189 : tensor<1x64xf32, #blocked3> -> tensor<16x64xf32, #blocked3>\n",
      "      %193 = arith.mulf %191, %192 : tensor<16x64xf32, #blocked3>\n",
      "      %194 = ttg.convert_layout %181 : tensor<64xf32, #blocked2> -> tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>>\n",
      "      %195 = tt.expand_dims %194 {axis = 0 : i32} : tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x64xf32, #blocked5>\n",
      "      %196 = ttg.convert_layout %195 : tensor<1x64xf32, #blocked5> -> tensor<1x64xf32, #blocked3>\n",
      "      %197 = tt.broadcast %196 : tensor<1x64xf32, #blocked3> -> tensor<16x64xf32, #blocked3>\n",
      "      %198 = arith.subf %197, %74 : tensor<16x64xf32, #blocked3>\n",
      "      %199 = math.exp %198 : tensor<16x64xf32, #blocked3>\n",
      "      %200 = arith.mulf %193, %199 : tensor<16x64xf32, #blocked3>\n",
      "      %201 = tt.broadcast %183 : tensor<16x1xi1, #blocked> -> tensor<16x64xi1, #blocked>\n",
      "      %202 = ttg.convert_layout %201 : tensor<16x64xi1, #blocked> -> tensor<16x64xi1, #blocked3>\n",
      "      %203 = arith.select %202, %200, %cst_15 : tensor<16x64xi1, #blocked3>, tensor<16x64xf32, #blocked3>\n",
      "      %204 = arith.addf %arg17, %203 : tensor<16x64xf32, #blocked3>\n",
      "      %205 = ttg.convert_layout %175 : tensor<16xf32, #blocked2> -> tensor<16xf32, #ttg.slice<{dim = 1, parent = #blocked4}>>\n",
      "      %206 = tt.expand_dims %205 {axis = 1 : i32} : tensor<16xf32, #ttg.slice<{dim = 1, parent = #blocked4}>> -> tensor<16x1xf32, #blocked4>\n",
      "      %207 = ttg.convert_layout %206 : tensor<16x1xf32, #blocked4> -> tensor<16x1xf32, #blocked>\n",
      "      %208 = ttg.convert_layout %180 : tensor<64xf32, #blocked2> -> tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>>\n",
      "      %209 = tt.expand_dims %208 {axis = 0 : i32} : tensor<64xf32, #ttg.slice<{dim = 0, parent = #blocked5}>> -> tensor<1x64xf32, #blocked5>\n",
      "      %210 = ttg.convert_layout %209 : tensor<1x64xf32, #blocked5> -> tensor<1x64xf32, #blocked3>\n",
      "      %211 = tt.broadcast %207 : tensor<16x1xf32, #blocked> -> tensor<16x64xf32, #blocked>\n",
      "      %212 = ttg.convert_layout %211 : tensor<16x64xf32, #blocked> -> tensor<16x64xf32, #blocked3>\n",
      "      %213 = tt.broadcast %210 : tensor<1x64xf32, #blocked3> -> tensor<16x64xf32, #blocked3>\n",
      "      %214 = arith.mulf %212, %213 : tensor<16x64xf32, #blocked3>\n",
      "      %215 = arith.mulf %214, %199 : tensor<16x64xf32, #blocked3>\n",
      "      %216 = arith.select %202, %215, %cst_15 : tensor<16x64xi1, #blocked3>, tensor<16x64xf32, #blocked3>\n",
      "      %217 = arith.addf %204, %216 : tensor<16x64xf32, #blocked3>\n",
      "      %218 = tt.addptr %arg18, %cst_12 : tensor<64x!tt.ptr<f32>, #blocked2>, tensor<64xi32, #blocked2>\n",
      "      %219 = tt.addptr %arg15, %cst_12 : tensor<64x!tt.ptr<f32>, #blocked2>, tensor<64xi32, #blocked2>\n",
      "      %220 = tt.addptr %arg16, %cst_12 : tensor<64x!tt.ptr<f32>, #blocked2>, tensor<64xi32, #blocked2>\n",
      "      %221 = tt.addptr %arg19, %c4_i32 : !tt.ptr<f32>, i32\n",
      "      scf.yield %219, %220, %217, %218, %221 : tensor<64x!tt.ptr<f32>, #blocked2>, tensor<64x!tt.ptr<f32>, #blocked2>, tensor<16x64xf32, #blocked3>, tensor<64x!tt.ptr<f32>, #blocked2>, !tt.ptr<f32>\n",
      "    }\n",
      "    %158 = arith.subf %125, %157#2 : tensor<16x64xf32, #blocked3>\n",
      "    %159 = arith.mulf %158, %105 : tensor<16x64xf32, #blocked3>\n",
      "    %160 = arith.addf %126, %159 : tensor<16x64xf32, #blocked3>\n",
      "    %161 = tt.splat %32 : !tt.ptr<f32> -> tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %162 = tt.addptr %161, %61 : tensor<16x64x!tt.ptr<f32>, #blocked3>, tensor<16x64xi64, #blocked3>\n",
      "    %163 = tt.load %162, %73 : tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %164 = arith.addf %125, %163 : tensor<16x64xf32, #blocked3>\n",
      "    %165 = arith.addf %164, %157#2 : tensor<16x64xf32, #blocked3>\n",
      "    %166 = tt.splat %33 : !tt.ptr<f32> -> tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %167 = tt.addptr %166, %61 : tensor<16x64x!tt.ptr<f32>, #blocked3>, tensor<16x64xi64, #blocked3>\n",
      "    tt.store %167, %165, %73 : tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %168 = tt.splat %34 : !tt.ptr<f32> -> tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    %169 = tt.addptr %168, %61 : tensor<16x64x!tt.ptr<f32>, #blocked3>, tensor<16x64xi64, #blocked3>\n",
      "    tt.store %169, %160, %73 : tensor<16x64x!tt.ptr<f32>, #blocked3>\n",
      "    tt.return\n",
      "  }\n",
      "}\n",
      "\n",
      "{-#\n",
      "  external_resources: {\n",
      "    mlir_reproducer: {\n",
      "      pipeline: \"builtin.module(tritongpu-coalesce, tritongpu-remove-layout-conversions, tritongpu-optimize-thread-locality, tritonamdgpu-accelerate-matmul{arch-generation-name=gfx1100 kPack=1 matrix-instruction-size=0}, tritongpu-remove-layout-conversions, tritonamdgpu-optimize-epilogue, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tt.func(tritonamdgpu-hoist-layout-conversions), tritongpu-fuse-nested-loops, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, triton-licm, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritonamdgpu-stream-pipeline{global_prefetch=0 local_prefetch=0 num_stages=4 use_async_copy=false use_pingpong=false}, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, tritongpu-optimize-dot-operands{hoist-layout-conversion=true}, tritongpu-remove-layout-conversions, tritongpu-reduce-data-duplication, tritonamdgpu-reorder-instructions, tritonamdgpu-fold-true-cmpi, canonicalize{  max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}, cse, symbol-dce)\",\n",
      "      disable_threading: false,\n",
      "      verify_each: true\n",
      "    }\n",
      "  }\n",
      "#-}\n",
      "/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/fla/ops/kda/chunk_intra.py:195:0: error: Failures have been detected while processing an MLIR pass pipeline\n",
      "/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/fla/ops/kda/chunk_intra.py:195:0: note: Pipeline failed while executing [`TritonAMDGPUStreamPipeline` on 'builtin.module' operation]: reproducer generated at `std::errs, please share the reproducer above with Triton project.`\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PassManager::run failed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m l = output.sum()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43ml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/torch/autograd/function.py:315\u001b[39m, in \u001b[36mBackwardCFunction.apply\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    310\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mImplementing both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mbackward\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mvjp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m for a custom \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    311\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFunction is not allowed. You should only implement one \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    312\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mof them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    313\u001b[39m     )\n\u001b[32m    314\u001b[39m user_fn = vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function.vjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/fla/utils.py:164\u001b[39m, in \u001b[36minput_guard.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    161\u001b[39m     ctx = contextlib.nullcontext()\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcontiguous_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcontiguous_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:573\u001b[39m, in \u001b[36mcustom_bwd.<locals>.decorate_bwd\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    566\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(bwd)\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_bwd\u001b[39m(*args, **kwargs):\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast(\n\u001b[32m    569\u001b[39m         device_type=device_type,\n\u001b[32m    570\u001b[39m         enabled=args[\u001b[32m0\u001b[39m]._fwd_used_autocast,\n\u001b[32m    571\u001b[39m         dtype=args[\u001b[32m0\u001b[39m]._dtype,\n\u001b[32m    572\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/fla/ops/kda/chunk.py:227\u001b[39m, in \u001b[36mChunkKDAFunction.backward\u001b[39m\u001b[34m(ctx, do, dht)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;129m@input_guard\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;129m@autocast_custom_bwd\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    224\u001b[39m     dht: torch.Tensor,\n\u001b[32m    225\u001b[39m ):\n\u001b[32m    226\u001b[39m     q, q_rstd, k, k_rstd, v, g, beta, Aqk, Akk, initial_state, cu_seqlens = ctx.saved_tensors\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     dq, dk, dv, db, dg, dh0 = \u001b[43mchunk_kda_bwd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mAqk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAqk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mAkk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAkk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdht\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdht\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ctx.use_qk_l2norm_in_kernel:\n\u001b[32m    242\u001b[39m         dq = l2norm_bwd(q, q_rstd, dq)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/fla/ops/kda/chunk.py:160\u001b[39m, in \u001b[36mchunk_kda_bwd\u001b[39m\u001b[34m(q, k, v, g, beta, Aqk, Akk, scale, initial_state, do, dht, cu_seqlens)\u001b[39m\n\u001b[32m    136\u001b[39m dq, dk, dw, dg = chunk_kda_bwd_dqkwg(\n\u001b[32m    137\u001b[39m     q=q,\n\u001b[32m    138\u001b[39m     k=k,\n\u001b[32m   (...)\u001b[39m\u001b[32m    148\u001b[39m     chunk_size=chunk_size,\n\u001b[32m    149\u001b[39m )\n\u001b[32m    150\u001b[39m dk2, dv, db, dg2, dAkk = prepare_wy_repr_bwd(\n\u001b[32m    151\u001b[39m     k=k,\n\u001b[32m    152\u001b[39m     v=v,\n\u001b[32m   (...)\u001b[39m\u001b[32m    158\u001b[39m     cu_seqlens=cu_seqlens,\n\u001b[32m    159\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m dq, dk2, db, dg2 = \u001b[43mchunk_kda_bwd_intra\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43mg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdAqk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdAqk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdAkk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdAkk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdk2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdg2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m dk.add_(dk2)\n\u001b[32m    175\u001b[39m dg.add_(dg2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/fla/ops/kda/chunk_intra.py:511\u001b[39m, in \u001b[36mchunk_kda_bwd_intra\u001b[39m\u001b[34m(q, k, g, beta, dAqk, dAkk, dq, dk, db, dg, cu_seqlens, chunk_size)\u001b[39m\n\u001b[32m    509\u001b[39m dg2 = torch.empty_like(dg, dtype=torch.float)\n\u001b[32m    510\u001b[39m grid = (NK * NC, NT, B * H)\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m \u001b[43mchunk_kda_bwd_kernel_intra\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdAqk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdAqk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdAkk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdAkk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdq2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdq2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdk2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdk2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdg2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mB\u001b[49m\u001b[43m=\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[43mT\u001b[49m\u001b[43m=\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mH\u001b[49m\u001b[43m=\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mK\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBT\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBC\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mBK\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mNC\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m dq = dq2\n\u001b[32m    536\u001b[39m dk = dk2\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:419\u001b[39m, in \u001b[36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) -> T:\n\u001b[32m    414\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[33;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[33;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[33;03m    memorizes the grid.\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m *args, **kwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/triton/runtime/autotuner.py:452\u001b[39m, in \u001b[36mHeuristics.run\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v, heur \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.values.items():\n\u001b[32m    451\u001b[39m     kwargs[v] = heur({**\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.arg_names, args)), **kwargs})\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/triton/runtime/autotuner.py:236\u001b[39m, in \u001b[36mAutotuner.run\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    233\u001b[39m     \u001b[38;5;28mself\u001b[39m.configs_timings = timings\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_results:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     used_cached_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_disk_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpruned_configs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    238\u001b[39m     benchmark()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/triton/runtime/autotuner.py:200\u001b[39m, in \u001b[36mAutotuner.check_disk_cache\u001b[39m\u001b[34m(self, tuning_key, configs, bench_fn)\u001b[39m\n\u001b[32m    197\u001b[39m         \u001b[38;5;28mself\u001b[39m.configs_timings = timings\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[43mbench_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m cache.put(\n\u001b[32m    202\u001b[39m     json.dumps({\n\u001b[32m    203\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   (...)\u001b[39m\u001b[32m    206\u001b[39m         [(config.\u001b[34m__dict__\u001b[39m, timings) \u001b[38;5;28;01mfor\u001b[39;00m config, timings \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.configs_timings.items() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.pre_hook],\n\u001b[32m    207\u001b[39m     }), file_name, binary=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/triton/runtime/autotuner.py:227\u001b[39m, in \u001b[36mAutotuner.run.<locals>.benchmark\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbenchmark\u001b[39m():\n\u001b[32m    226\u001b[39m     bench_start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     timings = {config: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m pruned_configs}\n\u001b[32m    228\u001b[39m     bench_end = time.time()\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mself\u001b[39m.bench_time = bench_end - bench_start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/triton/runtime/autotuner.py:162\u001b[39m, in \u001b[36mAutotuner._bench\u001b[39m\u001b[34m(self, config, *args, **meta)\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mself\u001b[39m.post_hook(full_nargs, exception=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_bench\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (OutOfResources, CompileTimeAssertionFailure, PTXASError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/triton/testing.py:149\u001b[39m, in \u001b[36mdo_bench\u001b[39m\u001b[34m(fn, warmup, rep, grad_to_none, quantiles, return_mode)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m return_mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    147\u001b[39m di = runtime.driver.active.get_device_interface()\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m di.synchronize()\n\u001b[32m    152\u001b[39m cache = runtime.driver.active.get_empty_cache_for_benchmark()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/triton/runtime/autotuner.py:148\u001b[39m, in \u001b[36mAutotuner._bench.<locals>.kernel_call\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28mself\u001b[39m.pre_hook(full_nargs)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcurrent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:733\u001b[39m, in \u001b[36mJITFunction.run\u001b[39m\u001b[34m(self, grid, warmup, *args, **kwargs)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kernel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     options, signature, constexprs, attrs = \u001b[38;5;28mself\u001b[39m._pack_args(backend, kwargs, bound_args, specialization,\n\u001b[32m    731\u001b[39m                                                             options)\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m     kernel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m kernel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    735\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/triton/runtime/jit.py:861\u001b[39m, in \u001b[36mJITFunction._do_compile\u001b[39m\u001b[34m(self, key, signature, device, constexprs, options, attrs, warmup)\u001b[39m\n\u001b[32m    859\u001b[39m     kernel = async_mode.submit(cache_key, async_compile, finalize_compile)\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m861\u001b[39m     kernel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    862\u001b[39m     kernel_cache[key] = kernel\n\u001b[32m    863\u001b[39m     \u001b[38;5;28mself\u001b[39m._call_hook(knobs.runtime.jit_post_compile_hook, key, signature, device, constexprs, options, [attrs],\n\u001b[32m    864\u001b[39m                     warmup)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/triton/compiler/compiler.py:320\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(src, target, options, _env_vars)\u001b[39m\n\u001b[32m    318\u001b[39m     timer.finished_ir_initialization()\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ext, compile_ir \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(stages.items())[first_stage:]:\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     next_module = \u001b[43mcompile_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m     ir_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fn_override_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    323\u001b[39m         \u001b[38;5;66;03m# Users can override kernels at scale by setting `ir_override` in autotune config\u001b[39;00m\n\u001b[32m    324\u001b[39m         \u001b[38;5;66;03m# without TRITON_KERNEL_OVERRIDE\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/triton/backends/amd/compiler.py:452\u001b[39m, in \u001b[36mHIPBackend.add_stages.<locals>.<lambda>\u001b[39m\u001b[34m(src, metadata)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m language == Language.TRITON:\n\u001b[32m    451\u001b[39m     stages[\u001b[33m\"\u001b[39m\u001b[33mttir\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28mself\u001b[39m.make_ttir(src, metadata, options)\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     stages[\u001b[33m\"\u001b[39m\u001b[33mttgir\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_ttgir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m language == Language.GLUON:\n\u001b[32m    454\u001b[39m     stages[\u001b[33m\"\u001b[39m\u001b[33mttgir\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mlambda\u001b[39;00m src, metadata: \u001b[38;5;28mself\u001b[39m.gluon_to_ttgir(src, metadata, options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/triton/backends/amd/compiler.py:259\u001b[39m, in \u001b[36mHIPBackend.make_ttgir\u001b[39m\u001b[34m(mod, metadata, options)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_async_copy:\n\u001b[32m    258\u001b[39m     amd.passes.ttgpuir.add_update_async_wait_count(pm, options.arch)\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m \u001b[43mpm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mod\n",
      "\u001b[31mRuntimeError\u001b[39m: PassManager::run failed"
     ]
    }
   ],
   "source": [
    "l = output.sum()\n",
    "l.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce531a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesianflownet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
