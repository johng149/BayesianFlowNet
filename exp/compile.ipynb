{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ca4ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "# 1. Define a Transformer model\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        self.decoder = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: (batch_size, seq_len, input_dim)\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        # After embedding: (batch_size, seq_len, d_model)\n",
    "        # Transformer expects (seq_len, batch_size, d_model) if batch_first=False (default)\n",
    "        # or (batch_size, seq_len, d_model) if batch_first=True.\n",
    "        # Our layer is batch_first=True.\n",
    "        \n",
    "        # The PositionalEncoding expects (seq_len, batch, dim)\n",
    "        # so we need to permute before and after\n",
    "        src = src.permute(1, 0, 2)\n",
    "        src = self.pos_encoder(src)\n",
    "        src = src.permute(1, 0, 2)\n",
    "\n",
    "        output = self.transformer_encoder(src)\n",
    "        # output shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # We can average the output over the sequence length for classification\n",
    "        output = output.mean(dim=1)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7bef254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01359f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input_batch1: torch.Size([2, 10, 1])\n",
      "Shape of input_batch2: torch.Size([2, 15, 1])\n",
      "Shape of target_batch1: torch.Size([2])\n",
      "Shape of target_batch2: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# 2. Create four hard-coded example inputs in two batches with different sequence lengths\n",
    "# The transformer model expects an input dimension, let's make it 1 for simplicity\n",
    "input_dim = 1\n",
    "\n",
    "# Batch 1 will have sequence length 10\n",
    "input1 = torch.randn(2, 10, input_dim).to(device) # batch of 2\n",
    "target1 = torch.randint(0, 5, (2,)).to(device)\n",
    "\n",
    "# Batch 2 will have sequence length 15\n",
    "input2 = torch.randn(2, 15, input_dim).to(device) # batch of 2\n",
    "target2 = torch.randint(0, 5, (2,)).to(device)\n",
    "\n",
    "input_batch1 = input1\n",
    "target_batch1 = target1\n",
    "input_batch2 = input2\n",
    "target_batch2 = target2\n",
    "\n",
    "print(f\"Shape of input_batch1: {input_batch1.shape}\")\n",
    "print(f\"Shape of input_batch2: {input_batch2.shape}\")\n",
    "print(f\"Shape of target_batch1: {target_batch1.shape}\")\n",
    "print(f\"Shape of target_batch2: {target_batch2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "057ba7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a model and optimizer\n",
    "input_dim = 1\n",
    "d_model = 32\n",
    "nhead = 4\n",
    "num_encoder_layers = 2\n",
    "dim_feedforward = 128\n",
    "num_classes = 5\n",
    "\n",
    "model = TransformerModel(input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, num_classes).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 4. Compile the model\n",
    "compiled_model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff96b4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss before training (batch 1): 1.3857465982437134\n",
      "Loss before training (batch 2): 2.0797219276428223\n"
     ]
    }
   ],
   "source": [
    "# Evaluate before training\n",
    "with torch.no_grad():\n",
    "    output_before1 = compiled_model(input_batch1)\n",
    "    loss_before1 = criterion(output_before1, target_batch1)\n",
    "    output_before2 = compiled_model(input_batch2)\n",
    "    loss_before2 = criterion(output_before2, target_batch2)\n",
    "    print(f\"Loss before training (batch 1): {loss_before1.item()}\")\n",
    "    print(f\"Loss before training (batch 2): {loss_before2.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9792fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train step (batch 1)\n",
      "Loss 1 (batch 1): 1.4687457084655762\n",
      "Loss 1 (batch 1): 1.4687457084655762\n"
     ]
    }
   ],
   "source": [
    "# 5. Two train steps\n",
    "print(\"First train step (batch 1)\")\n",
    "optimizer.zero_grad()\n",
    "output1 = compiled_model(input_batch1)\n",
    "loss1 = criterion(output1, target_batch1)\n",
    "loss1.backward()\n",
    "optimizer.step()\n",
    "print(f\"Loss 1 (batch 1): {loss1.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bef22b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Second train step (batch 2)\n",
      "Loss 2 (batch 2): 2.028371810913086\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSecond train step (batch 2)\")\n",
    "optimizer.zero_grad()\n",
    "output2 = compiled_model(input_batch2)\n",
    "loss2 = criterion(output2, target_batch2)\n",
    "loss2.backward()\n",
    "optimizer.step()\n",
    "print(f\"Loss 2 (batch 2): {loss2.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d267872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after training (batch 1): 1.256601095199585\n",
      "Loss after training (batch 2): 1.7875232696533203\n",
      "\n",
      "Loss for batch 1 went from 1.3857465982437134 to 1.256601095199585, delta: -0.12914550304412842 (lower is better)\n",
      "Loss for batch 2 went from 2.0797219276428223 to 1.7875232696533203, delta: -0.29219865798950195 (lower is better)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate after training\n",
    "with torch.no_grad():\n",
    "    output_after1 = compiled_model(input_batch1)\n",
    "    loss_after1 = criterion(output_after1, target_batch1)\n",
    "    output_after2 = compiled_model(input_batch2)\n",
    "    loss_after2 = criterion(output_after2, target_batch2)\n",
    "    print(f\"Loss after training (batch 1): {loss_after1.item()}\")\n",
    "    print(f\"Loss after training (batch 2): {loss_after2.item()}\")\n",
    "    \n",
    "    print(f\"\\nLoss for batch 1 went from {loss_before1.item()} to {loss_after1.item()}, delta: {loss_after1.item() - loss_before1.item()} (lower is better)\")\n",
    "    print(f\"Loss for batch 2 went from {loss_before2.item()} to {loss_after2.item()}, delta: {loss_after2.item() - loss_before2.item()} (lower is better)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b1b01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
