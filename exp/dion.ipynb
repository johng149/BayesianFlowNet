{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3639367",
   "metadata": {},
   "source": [
    "# Dion Optimizer Test Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b049d8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from dion import Dion\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "551f56f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ebfdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple TransformerBlock\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(model_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(model_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, model_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        return x\n",
    "\n",
    "# Define the main TransformerModel\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_dim, model_dim, num_heads, ff_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.embedding = nn.Embedding(vocab_dim, model_dim)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(model_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "        self.lm_head = nn.Linear(model_dim, vocab_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c0ab094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created.\n",
      "Number of parameter groups: 4\n",
      "Matrix params: 8\n",
      "Vector params: 16\n",
      "Embedding params: 1\n",
      "LM Head params: 2\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "vocab_dim = 100\n",
    "model_dim = 32\n",
    "num_heads = 4\n",
    "ff_dim = 64\n",
    "num_layers = 2\n",
    "lr = 1e-3\n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerModel(vocab_dim, model_dim, num_heads, ff_dim, num_layers).to(device)\n",
    "print(\"Model created.\")\n",
    "\n",
    "# Separate parameters into groups\n",
    "matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim == 2 and 'weight' in n]\n",
    "vector_params = [p for n, p in model.blocks.named_parameters() if p.ndim != 2]\n",
    "embed_params  = [p for p in model.embedding.parameters()]\n",
    "lm_head_params= [p for p in model.lm_head.parameters()]\n",
    "\n",
    "param_groups = [\n",
    "    dict(params=matrix_params),  # will default to \"dion\" algorithm\n",
    "    dict(params=vector_params, algorithm=\"lion\"),\n",
    "    dict(params=embed_params, algorithm=\"lion\", weight_decay=0),\n",
    "    dict(params=lm_head_params, algorithm=\"lion\", lr=lr / math.sqrt(model_dim), weight_decay=0)\n",
    "]\n",
    "\n",
    "print(f\"Number of parameter groups: {len(param_groups)}\")\n",
    "print(f\"Matrix params: {len(matrix_params)}\")\n",
    "print(f\"Vector params: {len(vector_params)}\")\n",
    "print(f\"Embedding params: {len(embed_params)}\")\n",
    "print(f\"LM Head params: {len(lm_head_params)}\")\n",
    "\n",
    "# verify all parameters are included\n",
    "total_param_count = sum(p.numel() for p in model.parameters())\n",
    "included_param_count = sum(p.numel() for group in param_groups for p in group['params'])\n",
    "assert total_param_count == included_param_count, \"Some parameters are missing from the optimizer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dff33b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dion optimizer created.\n",
      "Dion (\n",
      "Parameter Group 0\n",
      "    algorithm: dion\n",
      "    beta1: 0.9\n",
      "    beta2: 0.95\n",
      "    epsilon: 1e-08\n",
      "    lr: 0.001\n",
      "    mu: 0.95\n",
      "    oversample: 1.25\n",
      "    rank_fraction: 1.0\n",
      "    rank_multiple_of: 1\n",
      "    step: 0\n",
      "    weight_decay: 0.1\n",
      "\n",
      "Parameter Group 1\n",
      "    algorithm: lion\n",
      "    beta1: 0.9\n",
      "    beta2: 0.95\n",
      "    epsilon: 1e-08\n",
      "    lr: 0.001\n",
      "    mu: 0.95\n",
      "    oversample: 1.25\n",
      "    rank_fraction: 1.0\n",
      "    rank_multiple_of: 1\n",
      "    step: 0\n",
      "    weight_decay: 0.1\n",
      "\n",
      "Parameter Group 2\n",
      "    algorithm: lion\n",
      "    beta1: 0.9\n",
      "    beta2: 0.95\n",
      "    epsilon: 1e-08\n",
      "    lr: 0.001\n",
      "    mu: 0.95\n",
      "    oversample: 1.25\n",
      "    rank_fraction: 1.0\n",
      "    rank_multiple_of: 1\n",
      "    step: 0\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 3\n",
      "    algorithm: lion\n",
      "    beta1: 0.9\n",
      "    beta2: 0.95\n",
      "    epsilon: 1e-08\n",
      "    lr: 0.00017677669529663688\n",
      "    mu: 0.95\n",
      "    oversample: 1.25\n",
      "    rank_fraction: 1.0\n",
      "    rank_multiple_of: 1\n",
      "    step: 0\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the optimizer\n",
    "optimizer = Dion(\n",
    "    param_groups,\n",
    "    lr=lr,\n",
    "    weight_decay=0.1,\n",
    ")\n",
    "\n",
    "print(\"Dion optimizer created.\")\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0941c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/100], Loss: 4.7768, Weights changed: True\n",
      "Step [20/100], Loss: 4.7174, Weights changed: True\n",
      "Step [30/100], Loss: 4.6965, Weights changed: True\n",
      "Step [40/100], Loss: 4.7253, Weights changed: True\n",
      "Step [50/100], Loss: 4.6936, Weights changed: True\n",
      "Step [60/100], Loss: 4.7259, Weights changed: True\n",
      "Step [70/100], Loss: 4.7118, Weights changed: True\n",
      "Step [80/100], Loss: 4.6719, Weights changed: True\n",
      "Step [90/100], Loss: 4.6928, Weights changed: True\n",
      "Step [60/100], Loss: 4.7259, Weights changed: True\n",
      "Step [70/100], Loss: 4.7118, Weights changed: True\n",
      "Step [80/100], Loss: 4.6719, Weights changed: True\n",
      "Step [90/100], Loss: 4.6928, Weights changed: True\n",
      "Step [100/100], Loss: 4.5925, Weights changed: True\n",
      "Step [100/100], Loss: 4.5925, Weights changed: True\n"
     ]
    }
   ],
   "source": [
    "# Basic training loop\n",
    "batch_size = 4\n",
    "seq_len = 32\n",
    "num_steps = 100\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Generate dummy data with some order\n",
    "    data = torch.randint(0, vocab_dim, (batch_size, seq_len + 1), device=device)\n",
    "    inputs = data[:, :-1]\n",
    "    targets = data[:, 1:]\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Reshape for loss calculation\n",
    "    loss = criterion(outputs.reshape(-1, vocab_dim), targets.reshape(-1))\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Clone model state before optimizer step\n",
    "    model_state_before = {name: param.clone() for name, param in model.named_parameters()}\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compare model state after optimizer step\n",
    "    weights_changed = False\n",
    "    for name, param in model.named_parameters():\n",
    "        if not torch.equal(model_state_before[name], param):\n",
    "            weights_changed = True\n",
    "            break\n",
    "    \n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"Step [{step+1}/{num_steps}], Loss: {loss.item():.4f}, Weights changed: {weights_changed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab533d2",
   "metadata": {},
   "source": [
    "Training test finished."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
