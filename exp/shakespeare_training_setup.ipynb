{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d718ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the project root to the path so we can import from src\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.datasets.shakespeare.shakespeare import ShakespeareDataset\n",
    "from src.tokenizers.character_level.character_level import CharacterLevelTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592a43e",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Define the hyperparameters for the dataset and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e47fff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 256\n",
    "MIN_T = 1e-6\n",
    "NUM_WORKERS = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe59add",
   "metadata": {},
   "source": [
    "## Tokenizer and Dataset\n",
    "Initialize the character-level tokenizer and the Shakespeare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "660b9449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/datasets/load.py:1461: FutureWarning: The repository for karpathy/tiny_shakespeare contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/karpathy/tiny_shakespeare\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1003854\n",
      "Vocab size: 35\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharacterLevelTokenizer()\n",
    "dataset = ShakespeareDataset(tokenizer=tokenizer, max_length=MAX_LENGTH, min_t=MIN_T, train=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4df4ef",
   "metadata": {},
   "source": [
    "## Custom Collate Function\n",
    "Define a collate function that packs the batch into a single sequence and returns document IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d6043b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def packed_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to pack a batch of sequences into a single sequence.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of dictionaries, each containing 'x' (sequence) and 't' (timestep).\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "        - 'x': Packed sequence tensor of shape (total_seq_len,)\n",
    "        - 'doc_ids': Document ID tensor of shape (total_seq_len,)\n",
    "        - 't': Packed timestep tensor of shape (total_seq_len,)\n",
    "    \"\"\"\n",
    "    xs = [item['x'] for item in batch]\n",
    "    ts = [item['t'] for item in batch]\n",
    "    \n",
    "    # Pack x into a single sequence\n",
    "    packed_x = torch.cat(xs, dim=0) # (total_seq_len,)\n",
    "    \n",
    "    # Create document ids\n",
    "    doc_ids = []\n",
    "    for i, x in enumerate(xs):\n",
    "        doc_ids.append(torch.full_like(x, i))\n",
    "    packed_doc_ids = torch.cat(doc_ids, dim=0) # (total_seq_len,)\n",
    "    \n",
    "    # Pack t (expand t for each token in the sequence)\n",
    "    # t is (1,) per sample. We need to repeat it for len(x)\n",
    "    packed_ts = []\n",
    "    for i, (x, t) in enumerate(zip(xs, ts)):\n",
    "        packed_ts.append(t.repeat(len(x)))\n",
    "    packed_t = torch.cat(packed_ts, dim=0)\n",
    "\n",
    "    return {\n",
    "        \"x\": packed_x,\n",
    "        \"doc_ids\": packed_doc_ids,\n",
    "        \"t\": packed_t\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe33be9",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "Create the DataLoader using the custom collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de2a7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=packed_collate_fn,\n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5cda9e",
   "metadata": {},
   "source": [
    "## Verification\n",
    "Run a single batch to verify the shapes and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d781652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: dict_keys(['x', 'doc_ids', 't'])\n",
      "Packed x shape: torch.Size([8192])\n",
      "Doc ids shape: torch.Size([8192])\n",
      "Packed t shape: torch.Size([8192])\n",
      "\n",
      "Sample check:\n",
      "First 10 tokens: tensor([32, 22,  8, 19, 13,  4, 18, 18, 32, 19])\n",
      "First 10 doc ids: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "First 10 t values: tensor([0.5833, 0.5833, 0.5833, 0.5833, 0.5833, 0.5833, 0.5833, 0.5833, 0.5833,\n",
      "        0.5833])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "print(\"Batch keys:\", batch.keys())\n",
    "print(\"Packed x shape:\", batch['x'].shape)\n",
    "print(\"Doc ids shape:\", batch['doc_ids'].shape)\n",
    "print(\"Packed t shape:\", batch['t'].shape)\n",
    "\n",
    "print(\"\\nSample check:\")\n",
    "print(\"First 10 tokens:\", batch['x'][:10])\n",
    "print(\"First 10 doc ids:\", batch['doc_ids'][:10])\n",
    "print(\"First 10 t values:\", batch['t'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bded679",
   "metadata": {},
   "source": [
    "## Model Components\n",
    "Import necessary libraries and define the `PackDynamicSequenceChunker` and Flex Attention utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5847d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch import cat, arange\n",
    "from torch.nested import nested_tensor\n",
    "from torch.nn import Module, Linear, Parameter\n",
    "from torch.nn.functional import cosine_similarity, pad, softmax\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from einx import multiply\n",
    "from einops import repeat, rearrange\n",
    "from mamba_ssm import Mamba2\n",
    "from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
    "from assoc_scan import AssocScan\n",
    "\n",
    "# --- Helpers for Chunker ---\n",
    "\n",
    "Outputs = namedtuple('Outputs', [\n",
    "    'downsampled',\n",
    "    'upsample_fn',\n",
    "    'weighted_aux_ratio_loss'\n",
    "])\n",
    "\n",
    "Intermediates = namedtuple('Intermediates', [\n",
    "    'mask',\n",
    "    'probs',\n",
    "    'chunk_lens',\n",
    "    'boundary_mask',\n",
    "    'residual',\n",
    "    'gates',\n",
    "    'upsampler_output_scale',\n",
    "    'aux_ratio_loss',\n",
    "    'input_mask',     # Added: needed to handle padding logic in upsampler\n",
    "    'is_packed',      # Added: to track state\n",
    "    'seq_lens',       # Added: original sequence lengths\n",
    "    'new_seq_lens'    # Added: downsampled sequence lengths\n",
    "])\n",
    "\n",
    "def exists(v):\n",
    "    return v is not None\n",
    "\n",
    "def default(v, d):\n",
    "    return v if exists(v) else d\n",
    "\n",
    "def straight_through(t, value):\n",
    "    return t + (value - t).detach()\n",
    "\n",
    "def frac_gradient(t, frac = 1.):\n",
    "    if frac == 1:\n",
    "        return\n",
    "\n",
    "    t_grad = t * frac\n",
    "    return straight_through(t_grad, t)\n",
    "\n",
    "# --- PackDynamicSequenceChunker ---\n",
    "\n",
    "class PackDynamicSequenceChunker(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_queries_keys = None,\n",
    "        boundary_threshold = 0.5,\n",
    "        target_avg_token_length = 6.,       # N in eq(10)\n",
    "        ratio_loss_weight = 3e-2,\n",
    "        handle_residual_proj = False,       # turning this on will automatically handle a projection of the residual and its application in the inverse upsample function\n",
    "        assoc_scan_use_accelerated = False,\n",
    "        learning_rate_difference = 0.75,    # in the paper, they report that as one moves up a hierarchy, the learning rate needs to decrease. we'll default to 0.75 for the rough 2.0 -> 1.5 somewhere in the appendix from level 0 -> 1\n",
    "        straight_through_frac_vecs = True,  # improvisation where F receives gradients through straight-through with sigmoid\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_queries_keys = default(dim_queries_keys, dim)\n",
    "\n",
    "        # linear to queries and keys\n",
    "\n",
    "        self.to_queries_keys = Linear(dim, dim_queries_keys * 2, bias = False)\n",
    "\n",
    "        # start key token, so first token can be segmented / chunked out\n",
    "\n",
    "        self.start_key_token = Parameter(torch.randn(dim_queries_keys) * 1e-2) # presumably, need a start key token for the first token, open an issue if i got it wrong\n",
    "\n",
    "        # threshold to determine boundary\n",
    "\n",
    "        assert 0. < boundary_threshold < 1.\n",
    "\n",
    "        self.boundary_threshold = boundary_threshold\n",
    "\n",
    "        # smoothing related\n",
    "\n",
    "        self.smooth_assoc_scan = AssocScan(use_accelerated = assoc_scan_use_accelerated)\n",
    "\n",
    "        # maybe residual proj\n",
    "\n",
    "        self.handle_residual_proj = handle_residual_proj\n",
    "\n",
    "        if handle_residual_proj:\n",
    "            self.residual_proj = Linear(dim, dim)\n",
    "\n",
    "        # learning rate modulation, appendix C\n",
    "        # the multiplier on the learning rate as one goes from outer to inner of the h-net, and inverse of this value from inner to outer\n",
    "\n",
    "        self.learning_rate_difference = learning_rate_difference\n",
    "\n",
    "        # ratio aux loss related\n",
    "\n",
    "        self.target_avg_token_length = target_avg_token_length\n",
    "\n",
    "        self.straight_through_frac_vecs = straight_through_frac_vecs\n",
    "\n",
    "        self.ratio_loss_weight = ratio_loss_weight\n",
    "\n",
    "        self.register_buffer('zero', torch.tensor(0.), persistent = False)\n",
    "\n",
    "    def upsample(\n",
    "        self,\n",
    "        downsampled,\n",
    "        intermediates: Intermediates,\n",
    "        apply_scale = True\n",
    "    ):\n",
    "        # handle packed sequence input for upsampling\n",
    "        is_packed = downsampled.ndim == 2\n",
    "        \n",
    "        if is_packed:\n",
    "            # If the original input was packed, the downsampled input here should be packed.\n",
    "            # We need to unpack it to (B, N, D) to use the internal logic.\n",
    "            assert exists(intermediates.new_seq_lens), \"Cannot upsample packed sequence without new_seq_lens\"\n",
    "            downsampled_list = list(downsampled.split(intermediates.new_seq_lens.tolist()))\n",
    "            downsampled = pad_sequence(downsampled_list, batch_first=True)\n",
    "\n",
    "        batch, needs_grad, device = downsampled.shape[0], downsampled.requires_grad, downsampled.device\n",
    "\n",
    "        mask = intermediates.mask\n",
    "        gates = intermediates.gates\n",
    "        residual = intermediates.residual\n",
    "\n",
    "        # smoothing module for improved gradients eq(5)\n",
    "\n",
    "        downsampled = self.smooth_assoc_scan(gates, downsampled)\n",
    "\n",
    "        # upsample\n",
    "\n",
    "        downsampled_without_padding = downsampled[mask]\n",
    "        chunk_lens_without_padding = intermediates.chunk_lens[mask]\n",
    "\n",
    "        seq = arange(downsampled_without_padding.shape[0], device = device)\n",
    "\n",
    "        repeated_indices = torch.repeat_interleave(seq, chunk_lens_without_padding, dim = 0)\n",
    "        upsampled = downsampled_without_padding[repeated_indices]\n",
    "\n",
    "        upsampled = rearrange(upsampled, '(b n) d -> b n d', b = batch)\n",
    "\n",
    "        scale = intermediates.upsampler_output_scale\n",
    "\n",
    "        if needs_grad and apply_scale and exists(scale):\n",
    "            upsampled = multiply('b n d, b n', upsampled, scale)\n",
    "\n",
    "        if self.handle_residual_proj:\n",
    "            # We need to use the original residual (input tokens)\n",
    "            # If we are in packed mode, the residual stored in intermediates is likely padded.\n",
    "            # We apply the projection on the padded residual and mask later if needed.\n",
    "            upsampled = upsampled + self.residual_proj(residual)\n",
    "\n",
    "        upsampled = frac_gradient(upsampled, self.learning_rate_difference)\n",
    "        \n",
    "        # If the original input was packed, we must return a packed sequence.\n",
    "        # We use the original sequence lengths from intermediates to repack.\n",
    "        if intermediates.is_packed:\n",
    "            # Mask out padding from the padded upsampled result\n",
    "            # intermediates.input_mask contains the valid locations of the original sequence\n",
    "            if exists(intermediates.input_mask):\n",
    "                upsampled = upsampled[intermediates.input_mask]\n",
    "            else:\n",
    "                # Fallback if mask missing (shouldn't happen in packed flow)\n",
    "                upsampled_list = [upsampled[i, :l] for i, l in enumerate(intermediates.seq_lens)]\n",
    "                upsampled = cat(upsampled_list, dim=0)\n",
    "\n",
    "        return upsampled\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokens, # float[b n d] OR float[total_n d] if packed\n",
    "        seq_lens = None, # Required if tokens is packed\n",
    "        return_intermediates = False,\n",
    "        return_only_chunk_lens = False\n",
    "    ):\n",
    "        # ----------------------------------------------------------------------\n",
    "        # Handle Packed Sequences\n",
    "        # ----------------------------------------------------------------------\n",
    "        is_packed = tokens.ndim == 2\n",
    "        input_mask = None\n",
    "        \n",
    "        if is_packed:\n",
    "            assert exists(seq_lens), 'seq_lens must be provided for packed sequences'\n",
    "            # Unpack to (Batch, MaxLen, Dim) for processing\n",
    "            tokens_list = list(tokens.split(seq_lens.tolist()))\n",
    "            tokens_padded = pad_sequence(tokens_list, batch_first=True)\n",
    "            \n",
    "            # Create mask for padding\n",
    "            # This is critical for cosine_sim and loss masking\n",
    "            max_len = tokens_padded.shape[1]\n",
    "            batch_size = len(seq_lens)\n",
    "            range_tensor = arange(max_len, device=tokens.device).expand(batch_size, max_len)\n",
    "            input_mask = range_tensor < seq_lens.unsqueeze(1)\n",
    "            \n",
    "            # Switch tokens pointer to padded version\n",
    "            tokens = tokens_padded\n",
    "        else:\n",
    "            # Standard batched input\n",
    "            # Create a default mask of all True if not provided\n",
    "            batch, length = tokens.shape[:2]\n",
    "            input_mask = torch.ones((batch, length), device=tokens.device, dtype=torch.bool)\n",
    "            seq_lens = torch.full((batch,), length, device=tokens.device, dtype=torch.long)\n",
    "\n",
    "        batch, length, device = *tokens.shape[:2], tokens.device\n",
    "\n",
    "        residual = tokens\n",
    "\n",
    "        queries, keys = self.to_queries_keys(tokens).chunk(2, dim = -1)\n",
    "\n",
    "        start_keys = repeat(self.start_key_token, 'd -> b 1 d', b = batch)\n",
    "\n",
    "        keys = cat((start_keys, keys), dim = 1)\n",
    "\n",
    "        # each query looks at the previous key\n",
    "        # Handle padding for cosine similarity: avoid NaN by adding epsilon to norms or masking\n",
    "        \n",
    "        keys_shifted = keys[:, :-1]\n",
    "        \n",
    "        # Safe cosine similarity\n",
    "        eps = 1e-6\n",
    "        queries_norm = queries.norm(dim=-1, keepdim=True)\n",
    "        keys_norm = keys_shifted.norm(dim=-1, keepdim=True)\n",
    "        numerator = (queries * keys_shifted).sum(dim=-1)\n",
    "        denominator = (queries_norm * keys_norm).clamp(min=eps).squeeze(-1)\n",
    "        cosine_sim = numerator / denominator\n",
    "\n",
    "        # Mask out cosine sim on padding to avoid garbage boundaries\n",
    "        cosine_sim = cosine_sim.masked_fill(~input_mask, 1.0) # 1.0 sim -> 0.0 prob -> no boundary\n",
    "\n",
    "        probs = (1. - cosine_sim) * 0.5 # cosine sim is -1. to 1., this transforms it to 0. to 1.\n",
    "\n",
    "        boundary_mask = probs > self.boundary_threshold # bool[b n]\n",
    "\n",
    "        boundary_mask[:, 0] = True # first token must always be boundary\n",
    "        \n",
    "        # Ensure padding tokens are never boundaries\n",
    "        boundary_mask = boundary_mask & input_mask\n",
    "\n",
    "        # compute some lengths, per chunk and number of chunks per batch\n",
    "\n",
    "        num_chunks = boundary_mask.long().sum(dim = -1)\n",
    "\n",
    "        boundary_mask_with_end = pad(boundary_mask, (0, 1), value = True)\n",
    "        sel_indices = repeat(arange(boundary_mask_with_end.shape[-1], device = device), 'n -> b n', b = batch)[boundary_mask_with_end]\n",
    "\n",
    "        sel_indices = nested_tensor(sel_indices.split((num_chunks + 1).tolist()), layout = torch.jagged, device = device)\n",
    "\n",
    "        sel_indices = sel_indices.to_padded_tensor(padding = -1)\n",
    "\n",
    "        mask = (sel_indices != -1)[:, 1:]\n",
    "\n",
    "        chunk_lens = sel_indices[:, 1:] - sel_indices[:, :-1]\n",
    "        chunk_lens.masked_fill_(~mask, 0)\n",
    "\n",
    "        # early return chunk lens if using a trained module as a tokenizer\n",
    "\n",
    "        if return_only_chunk_lens:\n",
    "            if is_packed:\n",
    "                # Return packed chunk lenses\n",
    "                return chunk_lens[mask]\n",
    "            return chunk_lens\n",
    "\n",
    "        # downsampling - they show in their experiments that picking out the boundary tokens works just fine\n",
    "\n",
    "        boundary_tokens = tokens[boundary_mask] # pick out boundary tokens\n",
    "\n",
    "        tokens_nt = nested_tensor(boundary_tokens.split(num_chunks.tolist()), layout = torch.jagged, device = device, requires_grad = True)\n",
    "\n",
    "        downsampled_tokens = tokens_nt.to_padded_tensor(padding = 0.)\n",
    "\n",
    "        # smoothing module for improved gradients eq(5)\n",
    "\n",
    "        probs_nt = nested_tensor(probs[boundary_mask].split(num_chunks.tolist()), layout = torch.jagged, device = device, requires_grad = True)\n",
    "\n",
    "        boundary_probs = probs_nt.to_padded_tensor(padding = 0.)\n",
    "\n",
    "        gates = 1. - boundary_probs\n",
    "\n",
    "        downsampled_tokens = multiply('b n d, b n', downsampled_tokens, boundary_probs)\n",
    "\n",
    "        # for the upsampler\n",
    "\n",
    "        confidence = torch.where(boundary_mask, probs, 1. - probs)\n",
    "\n",
    "        # defaults if not training\n",
    "\n",
    "        upsampler_output_scale = None\n",
    "        aux_loss = self.zero\n",
    "        weighted_aux_loss = self.zero\n",
    "\n",
    "        needs_grad = tokens.requires_grad\n",
    "\n",
    "        if needs_grad:\n",
    "            # straight through for 1. multiplier on the expanded processed boundary tokens\n",
    "            \n",
    "            # For scale, we only care about valid tokens\n",
    "            if is_packed:\n",
    "                upsampler_output_scale = straight_through(confidence, 1.) # Padded shape\n",
    "                # We don't mask here because multiply() in upsampler handles shape, \n",
    "                # but valid gradients only come from valid indices.\n",
    "            else:\n",
    "                upsampler_output_scale = straight_through(confidence, 1.)\n",
    "\n",
    "            # auxiliary ratio loss in section 2.3.2, eq (10)\n",
    "\n",
    "            N = self.target_avg_token_length\n",
    "\n",
    "            F = boundary_mask.float()\n",
    "            \n",
    "            # Mask the probs for G calculation so padding doesn't affect mean\n",
    "            probs_masked = probs * input_mask.float()\n",
    "            G = probs_masked.sum(dim = -1) / input_mask.float().sum(dim = -1).clamp(min=1.)\n",
    "\n",
    "            # allow for a soft F to straight through - https://arxiv.org/abs/2505.22074\n",
    "\n",
    "            if self.straight_through_frac_vecs:\n",
    "                F_soft = (probs - self.boundary_threshold).sigmoid()\n",
    "                F = straight_through(F_soft, F)\n",
    "\n",
    "            # Mask F for mean calculation\n",
    "            F = (F * input_mask.float()).sum(dim = -1) / input_mask.float().sum(dim = -1).clamp(min=1.)\n",
    "\n",
    "            aux_ratio_loss = N / (N - 1) * ((N - 1) * F * G + (1. - F) * (1. - G))\n",
    "\n",
    "            aux_loss = aux_ratio_loss.mean()\n",
    "            weighted_aux_loss = aux_loss * self.ratio_loss_weight\n",
    "\n",
    "        # intermediates\n",
    "        \n",
    "        # Calculate new seq lengths for packed return\n",
    "        new_seq_lens = num_chunks\n",
    "        \n",
    "        intermediates = Intermediates(\n",
    "            mask, probs, chunk_lens, boundary_mask, residual, gates, \n",
    "            upsampler_output_scale, aux_loss,\n",
    "            input_mask, is_packed, seq_lens, new_seq_lens\n",
    "        )\n",
    "\n",
    "        # return the upsample function\n",
    "\n",
    "        def upsample(downsampled, apply_scale = True):\n",
    "            return self.upsample(downsampled, intermediates, apply_scale = apply_scale)\n",
    "\n",
    "        # adjust learning rate\n",
    "\n",
    "        downsampled_tokens = frac_gradient(downsampled_tokens, self.learning_rate_difference ** -1)\n",
    "\n",
    "        # handle packed output\n",
    "        \n",
    "        if is_packed:\n",
    "            # We computed downsampled_tokens as (B, N_chunks, D) padded.\n",
    "            # We need to pack it back to (Total_Chunks, D)\n",
    "            # We can use new_seq_lens (num_chunks) to create the mask for valid data\n",
    "            valid_chunks_mask = arange(downsampled_tokens.shape[1], device=device).expand(batch, -1) < new_seq_lens.unsqueeze(1)\n",
    "            downsampled_tokens = downsampled_tokens[valid_chunks_mask]\n",
    "\n",
    "        # returning\n",
    "\n",
    "        outputs = Outputs(downsampled_tokens, upsample, weighted_aux_loss)\n",
    "\n",
    "        if not return_intermediates:\n",
    "            return outputs\n",
    "\n",
    "        return outputs, intermediates\n",
    "\n",
    "# --- Flex Attention Utils ---\n",
    "\n",
    "def causal(b, h, q_idx, kv_idx):\n",
    "    return q_idx >= kv_idx\n",
    "\n",
    "def generate_doc_mask_mod(mask_mod, document_id):\n",
    "    # can feed in another mask modifier function such as `causal`\n",
    "\n",
    "    # Get unique document IDs and their counts\n",
    "    _, counts = torch.unique_consecutive(document_id, return_counts=True)\n",
    "    # Create cumulative counts (offsets)\n",
    "    offsets = torch.cat([torch.tensor([0], device=document_id.device), counts.cumsum(0)[:-1]])\n",
    "    def doc_mask_wrapper(b, h, q_idx, kv_idx):\n",
    "        same_doc = document_id[q_idx] == document_id[kv_idx]\n",
    "        q_logical = q_idx - offsets[document_id[q_idx]]\n",
    "        kv_logical = kv_idx - offsets[document_id[kv_idx]]\n",
    "        inner_mask = mask_mod(b, h, q_logical, kv_logical)\n",
    "        return same_doc & inner_mask\n",
    "    return doc_mask_wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8febee",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "Define the `TransformerBlock` using Flex Attention and the `HybridModel` combining Mamba, H-Net Chunker, and Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba18d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Module):\n",
    "    def __init__(self, dim, heads, dim_head):\n",
    "        super().__init__()\n",
    "        self.norm1 = torch.nn.RMSNorm(dim)\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        \n",
    "        self.to_qkv = Linear(dim, heads * dim_head * 3, bias=False)\n",
    "        self.to_out = Linear(heads * dim_head, dim, bias=False)\n",
    "        \n",
    "        self.norm2 = torch.nn.RMSNorm(dim)\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            Linear(dim, dim * 4),\n",
    "            torch.nn.GELU(),\n",
    "            Linear(dim * 4, dim)\n",
    "        )\n",
    "        self.flex = torch.compile(flex_attention)\n",
    "\n",
    "    def forward(self, x, block_mask):\n",
    "        # x: (1, SeqLen, Dim) - treating packed as batch 1\n",
    "        B, S, D = x.shape\n",
    "        \n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        qkv = self.to_qkv(x) # (B, S, 3 * H * Dh)\n",
    "        q, k, v = rearrange(qkv, 'b s (t h d) -> t b h s d', t=3, h=self.heads, d=self.dim_head)\n",
    "        \n",
    "        # Flex Attention\n",
    "        out = self.flex(q, k, v, block_mask=block_mask) # (B, S, H, D)\n",
    "        \n",
    "        out = rearrange(out, 'b h s d -> b s (h d)')\n",
    "        out = self.to_out(out)\n",
    "        \n",
    "        x = residual + out\n",
    "        \n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "class HybridModel(Module):\n",
    "    def __init__(self, dim, vocab_size, depth=4):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, dim)\n",
    "        \n",
    "        # Pre-chunk Mamba\n",
    "        self.mamba_pre = Mamba2(\n",
    "            d_model=dim,\n",
    "            headdim=dim//16 if dim >= 16 else 4,\n",
    "            d_state=16,\n",
    "            d_conv=4,\n",
    "            expand=2\n",
    "        )\n",
    "        \n",
    "        # Chunker\n",
    "        self.chunker = PackDynamicSequenceChunker(dim=dim)\n",
    "        \n",
    "        # Main Transformer (Flex Attention)\n",
    "        self.transformer_blocks = torch.nn.ModuleList([\n",
    "            TransformerBlock(dim, heads=4, dim_head=dim//4)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Post-chunk Mamba\n",
    "        self.mamba_post = Mamba2(\n",
    "            d_model=dim,\n",
    "            headdim=dim//16 if dim >= 16 else 4,\n",
    "            d_state=16,\n",
    "            d_conv=4,\n",
    "            expand=2\n",
    "        )\n",
    "        \n",
    "        self.head = Linear(dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, doc_ids):\n",
    "        # x: (Total_L,)\n",
    "        # doc_ids: (Total_L,)\n",
    "        \n",
    "        total_l = x.shape[0]\n",
    "\n",
    "        # Embedding\n",
    "        x = self.embedding(x) # (Total_L, Dim)\n",
    "        \n",
    "        # Mamba Pre\n",
    "        # Treat as batch 1, but use seq_idx for packed logic\n",
    "        x_unsqueezed = x.unsqueeze(0) # (1, Total_L, Dim)\n",
    "        seq_idx = doc_ids.unsqueeze(0).int() # (1, Total_L)\n",
    "        \n",
    "        x = self.mamba_pre(x_unsqueezed, seq_idx=seq_idx) # (1, Total_L, Dim)\n",
    "        x = x.squeeze(0) # (Total_L, Dim)\n",
    "        \n",
    "        # Chunker Downsample\n",
    "        # Need seq_lens\n",
    "        unique_doc_ids, counts = torch.unique_consecutive(doc_ids, return_counts=True)\n",
    "        seq_lens = counts\n",
    "        \n",
    "        outputs, intermediates = self.chunker(x, seq_lens=seq_lens, return_intermediates=True)\n",
    "        x_down = outputs.downsampled # (Total_Chunks, Dim)\n",
    "        \n",
    "        # Transformer\n",
    "        # Reconstruct doc_ids for downsampled sequence\n",
    "        with torch.no_grad():\n",
    "            new_seq_lens = intermediates.new_seq_lens # (Batch_Size,)\n",
    "            doc_ids_down = torch.repeat_interleave(unique_doc_ids, new_seq_lens)\n",
    "        \n",
    "        # Create mask for Flex Attention\n",
    "        x_down_unsqueezed = x_down.unsqueeze(0) # (1, Total_Chunks, Dim)\n",
    "        \n",
    "        # Generate mask\n",
    "        mask_mod = generate_doc_mask_mod(causal, doc_ids_down)\n",
    "        block_mask = create_block_mask(mask_mod, B=None, H=None, Q_LEN=x_down_unsqueezed.shape[1], KV_LEN=x_down_unsqueezed.shape[1], device=x.device)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            x_down_unsqueezed = block(x_down_unsqueezed, block_mask)\n",
    "            \n",
    "        x_down = x_down_unsqueezed.squeeze(0)\n",
    "        \n",
    "        # Chunker Upsample\n",
    "        x_up = outputs.upsample_fn(x_down) # (Total_L, Dim)\n",
    "        \n",
    "        # Mamba Post\n",
    "        x_up_unsqueezed = x_up.unsqueeze(0)\n",
    "        x_final = self.mamba_post(x_up_unsqueezed, seq_idx=seq_idx)\n",
    "        x_final = x_final.squeeze(0)\n",
    "        \n",
    "        # Head\n",
    "        logits = self.head(x_final)\n",
    "        return logits, outputs.weighted_aux_ratio_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63172170",
   "metadata": {},
   "source": [
    "## Model Verification\n",
    "Instantiate the model and run a forward pass with the sample batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a82ab42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([8192])\n",
      "Doc IDs shape: torch.Size([8192])\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "DIM = 64\n",
    "VOCAB_SIZE = tokenizer.vocab_size()\n",
    "DEPTH = 2\n",
    "\n",
    "# Instantiate Model\n",
    "model = HybridModel(dim=DIM, vocab_size=VOCAB_SIZE, depth=DEPTH)\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Get batch from previous step\n",
    "x = batch['x'].to('cuda')\n",
    "doc_ids = batch['doc_ids'].to('cuda')\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Doc IDs shape: {doc_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85518a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/torch/__init__.py:1551: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return _C._get_float32_matmul_precision()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([8192, 35])\n",
      "Aux Loss: 0.05101136863231659\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass\n",
    "logits, aux_loss = model(x, doc_ids)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Aux Loss: {aux_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9567a6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0510, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1394d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3bb37",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Set up the optimizer and the training loop. The loop iterates through the dataloader, handles document boundaries for the loss, and updates the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33f7b749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c206d0feb04ac78d7f57a5b9570024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss 3.6454 (CE: 3.5938, Aux: 0.0516)\n",
      "Step 10: Loss 3.1206 (CE: 3.0875, Aux: 0.0331)\n",
      "Step 20: Loss 3.0829 (CE: 3.0531, Aux: 0.0298)\n",
      "Step 30: Loss 3.0995 (CE: 3.0691, Aux: 0.0304)\n",
      "Step 40: Loss 3.0979 (CE: 3.0683, Aux: 0.0296)\n",
      "Step 50: Loss 3.0527 (CE: 3.0211, Aux: 0.0316)\n",
      "Step 60: Loss 3.0443 (CE: 3.0116, Aux: 0.0326)\n",
      "Step 70: Loss 2.9868 (CE: 2.9515, Aux: 0.0353)\n",
      "Step 80: Loss 2.9394 (CE: 2.9045, Aux: 0.0349)\n",
      "Step 90: Loss 2.9026 (CE: 2.8656, Aux: 0.0370)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none') # We need to mask manually\n",
    "\n",
    "# Training Loop\n",
    "NUM_STEPS = 100\n",
    "PRINT_EVERY = 10\n",
    "\n",
    "model.train()\n",
    "iter_dataloader = iter(dataloader)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for step in tqdm(range(NUM_STEPS)):\n",
    "    try:\n",
    "        batch = next(iter_dataloader)\n",
    "    except StopIteration:\n",
    "        iter_dataloader = iter(dataloader)\n",
    "        batch = next(iter_dataloader)\n",
    "        \n",
    "    x = batch['x'].to('cuda')\n",
    "    doc_ids = batch['doc_ids'].to('cuda')\n",
    "    \n",
    "    # Prepare inputs and targets\n",
    "    # We predict the next token\n",
    "    input_ids = x[:-1]\n",
    "    target_ids = x[1:]\n",
    "    input_doc_ids = doc_ids[:-1]\n",
    "    target_doc_ids = doc_ids[1:]\n",
    "    \n",
    "    # Forward pass\n",
    "    logits, aux_loss = model(input_ids, input_doc_ids)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    ce_loss = criterion(logits, target_ids)\n",
    "    \n",
    "    # Mask loss at document boundaries\n",
    "    # If input_doc_ids[i] != target_doc_ids[i], it means target is from a new doc\n",
    "    # We shouldn't predict across documents\n",
    "    valid_mask = (input_doc_ids == target_doc_ids).float()\n",
    "    \n",
    "    masked_ce_loss = (ce_loss * valid_mask).sum() / valid_mask.sum()\n",
    "    \n",
    "    total_loss = masked_ce_loss + aux_loss\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % PRINT_EVERY == 0:\n",
    "        print(f\"Step {step}: Loss {total_loss.item():.4f} (CE: {masked_ce_loss.item():.4f}, Aux: {aux_loss.item():.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesianflownet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
