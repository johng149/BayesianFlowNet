{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3d718ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the project root to the path so we can import from src\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from src.datasets.shakespeare.shakespeare import ShakespeareDataset\n",
    "from src.tokenizers.character_level.character_level import CharacterLevelTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592a43e",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Define the hyperparameters for the dataset and dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e47fff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "MAX_LENGTH = 256\n",
    "MIN_T = 1e-6\n",
    "NUM_WORKERS = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe59add",
   "metadata": {},
   "source": [
    "## Tokenizer and Dataset\n",
    "Initialize the character-level tokenizer and the Shakespeare dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "660b9449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/datasets/load.py:1461: FutureWarning: The repository for karpathy/tiny_shakespeare contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/karpathy/tiny_shakespeare\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1003854\n",
      "Vocab size: 35\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CharacterLevelTokenizer()\n",
    "dataset = ShakespeareDataset(tokenizer=tokenizer, max_length=MAX_LENGTH, min_t=MIN_T, train=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4df4ef",
   "metadata": {},
   "source": [
    "## Custom Collate Function\n",
    "Define a collate function that packs the batch into a single sequence and returns document IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d6043b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def packed_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to pack a batch of sequences into a single sequence.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of dictionaries, each containing 'x' (sequence) and 't' (timestep).\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "        - 'x': Packed sequence tensor of shape (total_seq_len,)\n",
    "        - 'doc_ids': Document ID tensor of shape (total_seq_len,)\n",
    "        - 't': Packed timestep tensor of shape (total_seq_len,)\n",
    "    \"\"\"\n",
    "    xs = [item['x'] for item in batch]\n",
    "    ts = [item['t'] for item in batch]\n",
    "    \n",
    "    # Pack x into a single sequence\n",
    "    packed_x = torch.cat(xs, dim=0) # (total_seq_len,)\n",
    "    \n",
    "    # Create document ids\n",
    "    doc_ids = []\n",
    "    for i, x in enumerate(xs):\n",
    "        doc_ids.append(torch.full_like(x, i))\n",
    "    packed_doc_ids = torch.cat(doc_ids, dim=0) # (total_seq_len,)\n",
    "    \n",
    "    # Pack t (expand t for each token in the sequence)\n",
    "    # t is (1,) per sample. We need to repeat it for len(x)\n",
    "    packed_ts = []\n",
    "    for i, (x, t) in enumerate(zip(xs, ts)):\n",
    "        packed_ts.append(t.repeat(len(x)))\n",
    "    packed_t = torch.cat(packed_ts, dim=0)\n",
    "\n",
    "    return {\n",
    "        \"x\": packed_x,\n",
    "        \"doc_ids\": packed_doc_ids,\n",
    "        \"t\": packed_t\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe33be9",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "Create the DataLoader using the custom collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de2a7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=packed_collate_fn,\n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5cda9e",
   "metadata": {},
   "source": [
    "## Verification\n",
    "Run a single batch to verify the shapes and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d781652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: dict_keys(['x', 'doc_ids', 't'])\n",
      "Packed x shape: torch.Size([32768])\n",
      "Doc ids shape: torch.Size([32768])\n",
      "Packed t shape: torch.Size([32768])\n",
      "\n",
      "Sample check:\n",
      "First 10 tokens: tensor([10,  8, 11, 11, 32, 12, 24, 18,  4, 11])\n",
      "First 10 doc ids: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "First 10 t values: tensor([0.8537, 0.8537, 0.8537, 0.8537, 0.8537, 0.8537, 0.8537, 0.8537, 0.8537,\n",
      "        0.8537])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "print(\"Batch keys:\", batch.keys())\n",
    "print(\"Packed x shape:\", batch['x'].shape)\n",
    "print(\"Doc ids shape:\", batch['doc_ids'].shape)\n",
    "print(\"Packed t shape:\", batch['t'].shape)\n",
    "\n",
    "print(\"\\nSample check:\")\n",
    "print(\"First 10 tokens:\", batch['x'][:10])\n",
    "print(\"First 10 doc ids:\", batch['doc_ids'][:10])\n",
    "print(\"First 10 t values:\", batch['t'][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bded679",
   "metadata": {},
   "source": [
    "## Model Components\n",
    "Import necessary libraries and define the `PackDynamicSequenceChunker` and Flex Attention utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5847d9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch import cat, arange\n",
    "from torch.nested import nested_tensor\n",
    "from torch.nn import Module, Linear, Parameter\n",
    "from torch.nn.functional import cosine_similarity, pad, softmax\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from einx import multiply\n",
    "from einops import repeat, rearrange\n",
    "from mamba_ssm import Mamba2\n",
    "from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
    "from assoc_scan import AssocScan\n",
    "# following section 2.2 of the paper\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import cat, arange\n",
    "from torch.nested import nested_tensor\n",
    "from torch.nn import Module, Linear, Parameter\n",
    "from torch.nn.functional import cosine_similarity, pad\n",
    "\n",
    "from einx import multiply\n",
    "from einops import repeat, rearrange\n",
    "\n",
    "from assoc_scan import AssocScan\n",
    "\n",
    "# constants\n",
    "\n",
    "Outputs = namedtuple('Outputs', [\n",
    "    'downsampled',\n",
    "    'upsample_fn',\n",
    "    'weighted_aux_ratio_loss'\n",
    "])\n",
    "\n",
    "Intermediates = namedtuple('Intermediates', [\n",
    "    'mask',\n",
    "    'probs',\n",
    "    'chunk_lens',\n",
    "    'boundary_mask',\n",
    "    'residual',\n",
    "    'gates',\n",
    "    'upsampler_output_scale',\n",
    "    'aux_ratio_loss',\n",
    "    'new_seq_lens'\n",
    "])\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def exists(v):\n",
    "    return v is not None\n",
    "\n",
    "def default(v, d):\n",
    "    return v if exists(v) else d\n",
    "\n",
    "def straight_through(t, value):\n",
    "    return t + (value - t).detach()\n",
    "\n",
    "def frac_gradient(t, frac = 1.):\n",
    "    if frac == 1:\n",
    "        return\n",
    "\n",
    "    t_grad = t * frac\n",
    "    return straight_through(t_grad, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PackDynamicSequenceChunker(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_queries_keys = None,\n",
    "        boundary_threshold = 0.5,\n",
    "        target_avg_token_length = 6.,       # N in eq(10)\n",
    "        ratio_loss_weight = 3e-2,\n",
    "        handle_residual_proj = False,       # turning this on will automatically handle a projection of the residual and its application in the inverse upsample function\n",
    "        assoc_scan_use_accelerated = False,\n",
    "        learning_rate_difference = 0.75,    # in the paper, they report that as one moves up a hierarchy, the learning rate needs to decrease. we'll default to 0.75 for the rough 2.0 -> 1.5 somewhere in the appendix from level 0 -> 1\n",
    "        straight_through_frac_vecs = True,  # improvisation where F receives gradients through straight-through with sigmoid\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_queries_keys = default(dim_queries_keys, dim)\n",
    "\n",
    "        # linear to queries and keys\n",
    "\n",
    "        self.to_queries_keys = Linear(dim, dim_queries_keys * 2, bias = False)\n",
    "\n",
    "        # start key token, so first token can be segmented / chunked out\n",
    "\n",
    "        self.start_key_token = Parameter(torch.randn(dim_queries_keys) * 1e-2) # presumably, need a start key token for the first token, open an issue if i got it wrong\n",
    "\n",
    "        # threshold to determine boundary\n",
    "\n",
    "        assert 0. < boundary_threshold < 1.\n",
    "\n",
    "        self.boundary_threshold = boundary_threshold\n",
    "\n",
    "        # smoothing related\n",
    "\n",
    "        self.smooth_assoc_scan = AssocScan(use_accelerated = assoc_scan_use_accelerated)\n",
    "\n",
    "        # maybe residual proj\n",
    "\n",
    "        self.handle_residual_proj = handle_residual_proj\n",
    "\n",
    "        if handle_residual_proj:\n",
    "            self.residual_proj = Linear(dim, dim)\n",
    "\n",
    "        # learning rate modulation, appendix C\n",
    "        # the multiplier on the learning rate as one goes from outer to inner of the h-net, and inverse of this value from inner to outer\n",
    "\n",
    "        self.learning_rate_difference = learning_rate_difference\n",
    "\n",
    "        # ratio aux loss related\n",
    "\n",
    "        self.target_avg_token_length = target_avg_token_length\n",
    "\n",
    "        self.straight_through_frac_vecs = straight_through_frac_vecs\n",
    "\n",
    "        self.ratio_loss_weight = ratio_loss_weight\n",
    "\n",
    "        self.register_buffer('zero', torch.tensor(0.), persistent = False)\n",
    "\n",
    "    def upsample(\n",
    "        self,\n",
    "        downsampled,\n",
    "        intermediates: Intermediates,\n",
    "        apply_scale = True\n",
    "    ):\n",
    "        batch, needs_grad, device = downsampled.shape[0], downsampled.requires_grad, downsampled.device\n",
    "\n",
    "        mask = intermediates.mask\n",
    "        gates = intermediates.gates\n",
    "        residual = intermediates.residual\n",
    "\n",
    "        # smoothing module for improved gradients eq(5)\n",
    "\n",
    "        downsampled = self.smooth_assoc_scan(gates, downsampled)\n",
    "\n",
    "        # upsample\n",
    "\n",
    "        downsampled_without_padding = downsampled[mask]\n",
    "        chunk_lens_without_padding = intermediates.chunk_lens[mask]\n",
    "\n",
    "        seq = arange(downsampled_without_padding.shape[0], device = device)\n",
    "\n",
    "        repeated_indices = torch.repeat_interleave(seq, chunk_lens_without_padding, dim = 0)\n",
    "        upsampled = downsampled_without_padding[repeated_indices]\n",
    "\n",
    "        upsampled = rearrange(upsampled, '(b n) d -> b n d', b = batch)\n",
    "\n",
    "        scale = intermediates.upsampler_output_scale\n",
    "\n",
    "        if needs_grad and apply_scale and exists(scale):\n",
    "            upsampled = multiply('b n d, b n', upsampled, scale)\n",
    "\n",
    "        if self.handle_residual_proj:\n",
    "            upsampled = upsampled + self.residual_proj(residual)\n",
    "\n",
    "        upsampled = frac_gradient(upsampled, self.learning_rate_difference)\n",
    "\n",
    "        return upsampled\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokens, # float[b n d] or float[total_n d] if seq_lens is specified,\n",
    "        seq_lens: Tensor | None = None,\n",
    "        return_intermediates = False,\n",
    "        return_only_chunk_lens = False\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            if seq_lens is not None:\n",
    "                total_lens = seq_lens.sum().item()\n",
    "                document_ids = torch.repeat_interleave(\n",
    "                    torch.arange(len(seq_lens), device=seq_lens.device), seq_lens\n",
    "                )\n",
    "\n",
    "                # a sequence position with 1 in probs_mask is the position of the first\n",
    "                # token of a new document, which means it must be a chunk start with\n",
    "                # probability 1\n",
    "                packed_probs_mask = torch.zeros_like(document_ids)\n",
    "                packed_probs_mask[1:] = document_ids[:-1] != document_ids[1:]\n",
    "\n",
    "                # however, since the sequence position is the start of a new document,\n",
    "                # we must prevent the associative scan from reading from the token before \n",
    "                # it. To do this, we reverse probs_mask, so the sequence position that used\n",
    "                # to be 1 becomes 0 and the positions that used to be 0 become 1.\n",
    "                # this means that at the start of each new document, the token cannot\n",
    "                # read from the token before it\n",
    "                packed_gate_mask = -1 * (packed_probs_mask - 1)\n",
    "                tokens = tokens.unsqueeze(0)\n",
    "            else:\n",
    "                packed_probs_mask = None\n",
    "                packed_gate_mask = None\n",
    "                document_ids = None\n",
    "\n",
    "        batch, length, device = *tokens.shape[:2], tokens.device\n",
    "\n",
    "        residual = tokens\n",
    "\n",
    "        queries, keys = self.to_queries_keys(tokens).chunk(2, dim = -1)\n",
    "\n",
    "        start_keys = repeat(self.start_key_token, 'd -> b 1 d', b = batch)\n",
    "\n",
    "        keys = cat((start_keys, keys), dim = 1)\n",
    "\n",
    "        if packed_probs_mask is not None:\n",
    "            # when packed, the keys end up being compared incorrectly at this current stage\n",
    "            # for example, suppose we have two documents of lengths 2 and 2.\n",
    "            # if passed individually, each document's first token will compare against the start key token\n",
    "            # however, when packed, the 3rd token (first token of second document)\n",
    "            # will compare against the key of the 2nd token, resulting in a wrong cosine_similarity\n",
    "            # which later impacts the probability\n",
    "            # at first I thought this would be fine because we hard set the probability, however\n",
    "            # now I recall that in the associative scan smoothing, this probability term is involved\n",
    "            # beyond the gate itself, which would result in an incorrect calculation, so\n",
    "            # we need to make all those keys that are at the start of a new document\n",
    "            # equal to the start key token\n",
    "            \n",
    "            # first, we start by adding a 1 to the right side of the packed_probs_mask, this is to account\n",
    "            # for the fact that when calculating cosine similarity, we use `keys[:, :-1]`, so it is shifted\n",
    "            # so the placement of the start key token needs to be shifted as well\n",
    "            packed_probs_mask_with_start = pad(packed_probs_mask, (0, 1), value = 0)\n",
    "\n",
    "            # and now, for all sequence positions where packed_probs_mask_with_start is 1,\n",
    "            # we set the corresponding keys to the start key token\n",
    "            keys[:, packed_probs_mask_with_start == 1] = start_keys\n",
    "\n",
    "\n",
    "        # each query looks at the previous key to determine if distance is greater than some threshold for determining a boundary exists (they use 0.5 as threshold)\n",
    "\n",
    "        cosine_sim  = cosine_similarity(queries, keys[:, :-1], dim = -1)\n",
    "\n",
    "        probs = (1. - cosine_sim) * 0.5 # cosine sim is -1. to 1., this transforms it to 0. to 1.\n",
    "\n",
    "        boundary_mask = probs > self.boundary_threshold # bool[b n]\n",
    "\n",
    "        boundary_mask[:, 0] = True # first token must always be boundary\n",
    "\n",
    "        if packed_probs_mask is not None:\n",
    "            # at all positions where the packed_probs_masking is 1, it means it is the start\n",
    "            # of a new document. We must force these positions to be boundaries\n",
    "            # previously I tried doing it by setting probs to 1, but that\n",
    "            # will cause issues later down the line because downsampling tensor is multiplied\n",
    "            # by the probs, so we must directly set the boundary mask instead\n",
    "            boundary_mask = torch.where(packed_probs_mask == 1, True, boundary_mask)\n",
    "\n",
    "        # compute some lengths, per chunk and number of chunks per batch\n",
    "\n",
    "        num_chunks = boundary_mask.long().sum(dim = -1)\n",
    "\n",
    "        boundary_mask_with_end = pad(boundary_mask, (0, 1), value = True)\n",
    "        sel_indices = repeat(arange(boundary_mask_with_end.shape[-1], device = device), 'n -> b n', b = batch)[boundary_mask_with_end]\n",
    "\n",
    "        sel_indices = nested_tensor(sel_indices.split((num_chunks + 1).tolist()), layout = torch.jagged, device = device)\n",
    "\n",
    "        sel_indices = sel_indices.to_padded_tensor(padding = -1)\n",
    "\n",
    "        mask = (sel_indices != -1)[:, 1:]\n",
    "\n",
    "        chunk_lens = sel_indices[:, 1:] - sel_indices[:, :-1]\n",
    "        chunk_lens.masked_fill_(~mask, 0)\n",
    "\n",
    "        # early return chunk lens if using a trained module as a tokenizer\n",
    "\n",
    "        if return_only_chunk_lens:\n",
    "            return chunk_lens\n",
    "\n",
    "        # downsampling - they show in their experiments that picking out the boundary tokens works just fine\n",
    "\n",
    "        boundary_tokens = tokens[boundary_mask] # pick out boundary tokens\n",
    "\n",
    "        tokens_nt = nested_tensor(boundary_tokens.split(num_chunks.tolist()), layout = torch.jagged, device = device, requires_grad = True)\n",
    "\n",
    "        downsampled_tokens = tokens_nt.to_padded_tensor(padding = 0.)\n",
    "\n",
    "        # smoothing module for improved gradients eq(5)\n",
    "\n",
    "        probs_nt = nested_tensor(probs[boundary_mask].split(num_chunks.tolist()), layout = torch.jagged, device = device, requires_grad = True)\n",
    "\n",
    "        boundary_probs = probs_nt.to_padded_tensor(padding = 0.)\n",
    "\n",
    "        gates = 1. - boundary_probs\n",
    "\n",
    "        if packed_gate_mask is not None:\n",
    "            # at all positions where the packed_gate_masking is 0, it means it is the start\n",
    "            # of a new document. We must prevent associative scan from allowing\n",
    "            # this starting token from reading into the past document\n",
    "            # also, gradients cannot propagate through this to modify this gating, as it is\n",
    "            # fixed by the document sequence\n",
    "            packed_gate_mask_nt = nested_tensor(packed_gate_mask.unsqueeze(0)[boundary_mask].split(num_chunks.tolist()), layout = torch.jagged, device = device, requires_grad = False)\n",
    "            packed_gate_masking = packed_gate_mask_nt.to_padded_tensor(padding = 1.0)\n",
    "            gates = gates * packed_gate_masking\n",
    "\n",
    "        downsampled_tokens = multiply('b n d, b n', downsampled_tokens, boundary_probs)\n",
    "\n",
    "\n",
    "        # for the upsampler\n",
    "\n",
    "        confidence = torch.where(boundary_mask, probs, 1. - probs)\n",
    "\n",
    "        # defaults if not training\n",
    "\n",
    "        upsampler_output_scale = None\n",
    "        aux_loss = self.zero\n",
    "        weighted_aux_loss = self.zero\n",
    "\n",
    "        needs_grad = tokens.requires_grad\n",
    "\n",
    "        if needs_grad:\n",
    "            # straight through for 1. multiplier on the expanded processed boundary tokens\n",
    "\n",
    "            upsampler_output_scale = straight_through(confidence, 1.)\n",
    "\n",
    "            # auxiliary ratio loss in section 2.3.2, eq (10)\n",
    "            # lets follow their notation\n",
    "\n",
    "            N = self.target_avg_token_length\n",
    "\n",
    "            F = boundary_mask.float()\n",
    "            G = probs.mean(dim = -1)\n",
    "\n",
    "            # allow for a soft F to straight through - https://arxiv.org/abs/2505.22074\n",
    "\n",
    "            if self.straight_through_frac_vecs:\n",
    "                F_soft = (probs - self.boundary_threshold).sigmoid()\n",
    "                F = straight_through(F_soft, F)\n",
    "\n",
    "            F = F.mean(dim = -1)\n",
    "\n",
    "            aux_ratio_loss = N / (N - 1) * ((N - 1) * F * G + (1. - F) * (1. - G))\n",
    "\n",
    "            aux_loss = aux_ratio_loss.mean()\n",
    "            weighted_aux_loss = aux_loss * self.ratio_loss_weight\n",
    "\n",
    "        # intermediates\n",
    "        if document_ids is not None:\n",
    "            # this minlength should not be necessary as the boundaries should \n",
    "            # guarantee that each document has at least one chunk\n",
    "            new_seq_lens = torch.bincount(document_ids, weights=boundary_mask.squeeze(0).long(), minlength=len(seq_lens)).long()\n",
    "        else:\n",
    "            new_seq_lens = num_chunks\n",
    "\n",
    "        intermediates = Intermediates(mask, probs, chunk_lens, boundary_mask, residual, gates, upsampler_output_scale, aux_loss, new_seq_lens)\n",
    "\n",
    "        # return the upsample function\n",
    "\n",
    "        def upsample(downsampled, apply_scale = True):\n",
    "            downsampled_input = downsampled.unsqueeze(0) if downsampled.ndim == 2 else downsampled\n",
    "            upsampled = self.upsample(downsampled_input, intermediates, apply_scale = apply_scale)\n",
    "            return upsampled.squeeze(0) if downsampled.ndim == 2 else upsampled\n",
    "\n",
    "        # adjust learning rate\n",
    "\n",
    "        downsampled_tokens = frac_gradient(downsampled_tokens, self.learning_rate_difference ** -1)\n",
    "\n",
    "        if packed_probs_mask is not None:\n",
    "            downsampled_tokens = downsampled_tokens.squeeze(0)\n",
    "\n",
    "        # returning\n",
    "\n",
    "        outputs = Outputs(downsampled_tokens, upsample, weighted_aux_loss)\n",
    "\n",
    "        if not return_intermediates:\n",
    "            return outputs\n",
    "\n",
    "        return outputs, intermediates\n",
    "\n",
    "\n",
    "    \n",
    "# --- Flex Attention Utils ---\n",
    "\n",
    "def causal(b, h, q_idx, kv_idx):\n",
    "    return q_idx >= kv_idx\n",
    "\n",
    "def generate_doc_mask_mod(mask_mod, document_id):\n",
    "    # can feed in another mask modifier function such as `causal`\n",
    "\n",
    "    # Get unique document IDs and their counts\n",
    "    _, counts = torch.unique_consecutive(document_id, return_counts=True)\n",
    "    # Create cumulative counts (offsets)\n",
    "    offsets = torch.cat([torch.tensor([0], device=document_id.device), counts.cumsum(0)[:-1]])\n",
    "    def doc_mask_wrapper(b, h, q_idx, kv_idx):\n",
    "        same_doc = document_id[q_idx] == document_id[kv_idx]\n",
    "        q_logical = q_idx - offsets[document_id[q_idx]]\n",
    "        kv_logical = kv_idx - offsets[document_id[kv_idx]]\n",
    "        inner_mask = mask_mod(b, h, q_logical, kv_logical)\n",
    "        return same_doc & inner_mask\n",
    "    return doc_mask_wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8febee",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "Define the `TransformerBlock` using Flex Attention and the `HybridModel` combining Mamba, H-Net Chunker, and Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba18d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Module):\n",
    "    def __init__(self, dim, heads, dim_head):\n",
    "        super().__init__()\n",
    "        self.norm1 = torch.nn.RMSNorm(dim)\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        \n",
    "        self.to_qkv = Linear(dim, heads * dim_head * 3, bias=False)\n",
    "        self.to_out = Linear(heads * dim_head, dim, bias=False)\n",
    "        \n",
    "        self.norm2 = torch.nn.RMSNorm(dim)\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            Linear(dim, dim * 4),\n",
    "            torch.nn.GELU(),\n",
    "            Linear(dim * 4, dim)\n",
    "        )\n",
    "        self.flex = torch.compile(flex_attention)\n",
    "\n",
    "    def forward(self, x, block_mask):\n",
    "        # x: (1, SeqLen, Dim) - treating packed as batch 1\n",
    "        B, S, D = x.shape\n",
    "        \n",
    "        residual = x\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        qkv = self.to_qkv(x) # (B, S, 3 * H * Dh)\n",
    "        q, k, v = rearrange(qkv, 'b s (t h d) -> t b h s d', t=3, h=self.heads, d=self.dim_head)\n",
    "        \n",
    "        # Flex Attention\n",
    "        out = self.flex(q, k, v, block_mask=block_mask) # (B, S, H, D)\n",
    "        \n",
    "        out = rearrange(out, 'b h s d -> b s (h d)')\n",
    "        out = self.to_out(out)\n",
    "        \n",
    "        x = residual + out\n",
    "        \n",
    "        residual = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = residual + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "class HybridModel(Module):\n",
    "    def __init__(self, dim, vocab_size, depth=4):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, dim)\n",
    "        \n",
    "        # Pre-chunk Mamba\n",
    "        self.mamba_pre = Mamba2(\n",
    "            d_model=dim,\n",
    "            headdim=dim//16 if dim >= 16 else 4,\n",
    "            d_state=16,\n",
    "            d_conv=4,\n",
    "            expand=2\n",
    "        )\n",
    "        \n",
    "        # Chunker\n",
    "        self.chunker = PackDynamicSequenceChunker(dim=dim)\n",
    "        \n",
    "        # Main Transformer (Flex Attention)\n",
    "        self.transformer_blocks = torch.nn.ModuleList([\n",
    "            TransformerBlock(dim, heads=4, dim_head=dim//4)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        \n",
    "        # Post-chunk Mamba\n",
    "        self.mamba_post = Mamba2(\n",
    "            d_model=dim,\n",
    "            headdim=dim//16 if dim >= 16 else 4,\n",
    "            d_state=16,\n",
    "            d_conv=4,\n",
    "            expand=2\n",
    "        )\n",
    "        \n",
    "        self.head = Linear(dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, doc_ids):\n",
    "        # x: (Total_L,)\n",
    "        # doc_ids: (Total_L,)\n",
    "        \n",
    "        total_l = x.shape[0]\n",
    "\n",
    "        # Embedding\n",
    "        x = self.embedding(x) # (Total_L, Dim)\n",
    "        \n",
    "        # Mamba Pre\n",
    "        # Treat as batch 1, but use seq_idx for packed logic\n",
    "        x_unsqueezed = x.unsqueeze(0) # (1, Total_L, Dim)\n",
    "        seq_idx = doc_ids.unsqueeze(0).int() # (1, Total_L)\n",
    "        \n",
    "        x = self.mamba_pre(x_unsqueezed, seq_idx=seq_idx) # (1, Total_L, Dim)\n",
    "        x = x.squeeze(0) # (Total_L, Dim)\n",
    "        \n",
    "        # Chunker Downsample\n",
    "        # Need seq_lens\n",
    "        unique_doc_ids, counts = torch.unique_consecutive(doc_ids, return_counts=True)\n",
    "        seq_lens = counts\n",
    "        \n",
    "        outputs, intermediates = self.chunker(x, seq_lens=seq_lens, return_intermediates=True)\n",
    "        x_down = outputs.downsampled # (Total_Chunks, Dim)\n",
    "        \n",
    "        # Transformer\n",
    "        # Reconstruct doc_ids for downsampled sequence\n",
    "        with torch.no_grad():\n",
    "            new_seq_lens = intermediates.new_seq_lens # (Batch_Size,)\n",
    "            doc_ids_down = torch.repeat_interleave(unique_doc_ids, new_seq_lens)\n",
    "        \n",
    "        # Create mask for Flex Attention\n",
    "        x_down_unsqueezed = x_down.unsqueeze(0) # (1, Total_Chunks, Dim)\n",
    "        \n",
    "        # Generate mask\n",
    "        mask_mod = generate_doc_mask_mod(causal, doc_ids_down)\n",
    "        block_mask = create_block_mask(mask_mod, B=None, H=None, Q_LEN=x_down_unsqueezed.shape[1], KV_LEN=x_down_unsqueezed.shape[1], device=x.device)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            x_down_unsqueezed = block(x_down_unsqueezed, block_mask)\n",
    "            \n",
    "        x_down = x_down_unsqueezed.squeeze(0)\n",
    "        \n",
    "        # Chunker Upsample\n",
    "        x_up = outputs.upsample_fn(x_down) # (Total_L, Dim)\n",
    "        \n",
    "        # Mamba Post\n",
    "        x_up_unsqueezed = x_up.unsqueeze(0)\n",
    "        x_final = self.mamba_post(x_up_unsqueezed, seq_idx=seq_idx)\n",
    "        x_final = x_final.squeeze(0)\n",
    "        \n",
    "        # Head\n",
    "        logits = self.head(x_final)\n",
    "        return logits, outputs.weighted_aux_ratio_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63172170",
   "metadata": {},
   "source": [
    "## Model Verification\n",
    "Instantiate the model and run a forward pass with the sample batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a82ab42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32768])\n",
      "Doc IDs shape: torch.Size([32768])\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "DIM = 64\n",
    "VOCAB_SIZE = tokenizer.vocab_size()\n",
    "DEPTH = 2\n",
    "\n",
    "# Instantiate Model\n",
    "model = HybridModel(dim=DIM, vocab_size=VOCAB_SIZE, depth=DEPTH)\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Get batch from previous step\n",
    "x = batch['x'].to('cuda')\n",
    "doc_ids = batch['doc_ids'].to('cuda')\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Doc IDs shape: {doc_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85518a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/torch/__init__.py:1551: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return _C._get_float32_matmul_precision()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([32768, 35])\n",
      "Aux Loss: 0.055008966475725174\n"
     ]
    }
   ],
   "source": [
    "# Forward Pass\n",
    "logits, aux_loss = model(x, doc_ids)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Aux Loss: {aux_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9567a6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0550, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1394d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3bb37",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Set up the optimizer and the training loop. The loop iterates through the dataloader, handles document boundaries for the loss, and updates the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7b749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9b00d692584a6e8cb03548074ac4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss 3.6436 (CE: 3.5889, Aux: 0.0547)\n",
      "Step 1000: Loss 1.9931 (CE: 1.9368, Aux: 0.0564)\n",
      "Step 2000: Loss 1.9263 (CE: 1.8706, Aux: 0.0556)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='none') # We need to mask manually\n",
    "\n",
    "# Training Loop\n",
    "NUM_STEPS = 10000\n",
    "PRINT_EVERY = 1000\n",
    "\n",
    "model.train()\n",
    "iter_dataloader = iter(dataloader)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for step in tqdm(range(NUM_STEPS)):\n",
    "    try:\n",
    "        batch = next(iter_dataloader)\n",
    "    except StopIteration:\n",
    "        iter_dataloader = iter(dataloader)\n",
    "        batch = next(iter_dataloader)\n",
    "        \n",
    "    x = batch['x'].to('cuda')\n",
    "    doc_ids = batch['doc_ids'].to('cuda')\n",
    "    \n",
    "    # Prepare inputs and targets\n",
    "    # We predict the next token\n",
    "    input_ids = x[:-1]\n",
    "    target_ids = x[1:]\n",
    "    input_doc_ids = doc_ids[:-1]\n",
    "    target_doc_ids = doc_ids[1:]\n",
    "    \n",
    "    # Forward pass\n",
    "    logits, aux_loss = model(input_ids, input_doc_ids)\n",
    "    \n",
    "    # Calculate Loss\n",
    "    ce_loss = criterion(logits, target_ids)\n",
    "    \n",
    "    # Mask loss at document boundaries\n",
    "    # If input_doc_ids[i] != target_doc_ids[i], it means target is from a new doc\n",
    "    # We shouldn't predict across documents\n",
    "    valid_mask = (input_doc_ids == target_doc_ids).float()\n",
    "    \n",
    "    masked_ce_loss = (ce_loss * valid_mask).sum() / valid_mask.sum()\n",
    "    \n",
    "    total_loss = masked_ce_loss + aux_loss\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % PRINT_EVERY == 0:\n",
    "        print(f\"Step {step}: Loss {total_loss.item():.4f} (CE: {masked_ce_loss.item():.4f}, Aux: {aux_loss.item():.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e91fb4e",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Generate text using the trained model to qualitatively assess performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c700367f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Generating from: 'ROMEO:'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1127 20:19:12.059000 263834 torch/_dynamo/convert_frame.py:1358] [0/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W1127 20:19:12.059000 263834 torch/_dynamo/convert_frame.py:1358] [0/8]    function: 'flex_attention' (/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/torch/nn/attention/flex_attention.py:1449)\n",
      "W1127 20:19:12.059000 263834 torch/_dynamo/convert_frame.py:1358] [0/8]    last reason: 0/7: tensor 'block_mask.q_indices' size mismatch at index 2. expected 1, actual 2\n",
      "W1127 20:19:12.059000 263834 torch/_dynamo/convert_frame.py:1358] [0/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1127 20:19:12.059000 263834 torch/_dynamo/convert_frame.py:1358] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html\n",
      "/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/torch/nn/attention/flex_attention.py:1687: UserWarning: flex_attention called without torch.compile() - this will use an unfused implementation that materializes the full scores matrix instead of generating a fused kernel.\n",
      "\n",
      "SOLUTION: Use torch.compile(flex_attention)(...)\n",
      "\n",
      "If you want to debug your score_mod/mask_mod, you can set:\n",
      "torch.nn.attention.flex_attention._FLEX_ATTENTION_DISABLE_COMPILE_DEBUG = True\n",
      "\n",
      "This will allow you to use print statements or breakpoints. Note: This doesn't work with the backwards pass and may produce incorrect results.\n",
      "  _warn_once(\n",
      "/media/john/Tertiary/Projects/ML/BayesianFlowNet/.venv/lib/python3.12/site-packages/torch/nn/attention/flex_attention.py:1687: UserWarning: flex_attention called without torch.compile() - this will use an unfused implementation that materializes the full scores matrix instead of generating a fused kernel.\n",
      "\n",
      "SOLUTION: Use torch.compile(flex_attention)(...)\n",
      "\n",
      "If you want to debug your score_mod/mask_mod, you can set:\n",
      "torch.nn.attention.flex_attention._FLEX_ATTENTION_DISABLE_COMPILE_DEBUG = True\n",
      "\n",
      "This will allow you to use print statements or breakpoints. Note: This doesn't work with the backwards pass and may produce incorrect results.\n",
      "  _warn_once(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romeo:<UNK><UNK>ens t assw r dnth hr lldoeley n rtitetne aangsnefteniu<UNK>ecetes ,<UNK>snemotor uatur i .<UNK>pzbmrtsli: yo h yoyseusuenbf<UNK>lnoddller p;leiid our oteuyovole,<UNK>l'meniod<UNK>so a rdnm kin pete ei 'sn mebitsuaem,<UNK>fou'ebltwn: m<UNK>d fyishrn wodhnrali?<UNK>l<UNK>fmele, moly pm tho h  fteuwbl ekbu dsmicu d;at yo lley o nehes ogrf zchsp her:<UNK>venh mas yo:<UNK>ih thlt s' f; in, ;orle wa or son:<UNK>n whidait aacnneot har yoy p a t tktt'ys'ay k eseor, an sudr meat cgnefniesireilo gto hald lstlcre.'aeebunst.,<UNK>hor hsdtul , ellilr.<UNK>onnr .nts\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, start_text=\"ROMEO:\", max_length=200, temperature=1.0):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Encode start text\n",
    "    input_ids = tokenizer.encode(start_text).to(device)\n",
    "    \n",
    "    # We need to track doc_ids, assuming single document for generation\n",
    "    doc_ids = torch.zeros_like(input_ids).to(device)\n",
    "    \n",
    "    generated_ids = input_ids.clone()\n",
    "    \n",
    "    print(f\"Generating from: '{start_text}'\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass\n",
    "            logits, _ = model(generated_ids, doc_ids)\n",
    "            \n",
    "            # Get logits for the last token\n",
    "            next_token_logits = logits[-1, :] / temperature\n",
    "            \n",
    "            # Sample\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append\n",
    "            generated_ids = torch.cat([generated_ids, next_token], dim=0)\n",
    "            doc_ids = torch.cat([doc_ids, torch.zeros(1, device=device, dtype=doc_ids.dtype)], dim=0)\n",
    "            \n",
    "    # Decode\n",
    "    return tokenizer._decode(generated_ids)\n",
    "\n",
    "# Test generation\n",
    "print(\"-\" * 50)\n",
    "print(generate_text(model, tokenizer, start_text=\"ROMEO:\", max_length=500))\n",
    "print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesianflownet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
