from sqlite3 import OperationalError

import torch
from tqdm.auto import tqdm

from src.nn.models.discrete_model import DiscreteModel
from src.training.checkpoint import CheckpointManager
from src.training.discrete_loss import (  # alpha_variance_loss,
    divergence_loss,
    format_loss,
    loss,
    variance_loss,
)
from src.training.gradient_surgery import gradient_surgery


def train_discrete_model(
    model: DiscreteModel,
    body_optimizer,
    schedule_optimizer,
    train_dl,
    starting_epoch,
    epochs,
    accelerator,
    checkpoint_manager: CheckpointManager,
    save_dir: str,
    folds: int,
    grad_clip_norm=None,
    save_every: int = 100,
    variance_loss_strength: float = 0.8,
    divergence_loss_strength: float = 0.8,
    # alpha_linearity_loss_strength: float = 0.2,
    skip_schedule_optim: bool = False,
):
    """
    Args:
        model: Model to train, it should output two Tensors in its forward function and an attribute called learnable_beta
            which points to a `LearnableBetaScheduleNI` instance, as well as a function beta_1 and scaling
        body_optimizer: Optimizer to use for model save for the learned beta schedule weights
        schedule_optimizer: Optimizer to use for the schedule of the model
        train_dl: Training data loader
        starting_epoch: Epoch to start training from
        epochs: Number of epochs to train for
        accelerator: Accelerator to use for training
        checkpoint_manager: Checkpoint manager to use for saving/loading checkpoints
        save_dir: Directory to save checkpoints
        folds: The number of folds to expect to be generated by the dataloader per batch. The folds are used to
            estimate the variance of the loss across each unique ground truth `x`. Assumes batch size to be
            `unique_x * folds`
        grad_clip_norm: Gradient clipping norm
        save_every: Save checkpoint every X epochs
        variance_loss_strength: Strength of the variance loss
        divergence_loss_strength: Strength of the divergence loss
    """
    epoch = starting_epoch
    # debug_data_past_epoch = {}
    # debug_data_current_epoch = {}
    # with torch.autograd.detect_anomaly():
    try:
        model.train()
        pbar = tqdm(
            range(starting_epoch, starting_epoch + epochs),
            desc="Training Discrete Model",
        )
        train_iter = iter(train_dl)
        for epoch in pbar:
            try:
                ground_truth = next(train_iter)
            except StopIteration:
                train_iter = iter(train_dl)
                ground_truth = next(train_iter)
            x = ground_truth["x"]  # batch_size, seq_len, K
            t = ground_truth["t"]
            output, alpha = model.forward(x, t)
            formatted_loss = format_loss(
                alpha, x, model_output_logits=output, folds=folds
            )
            l_infty_loss = loss(formatted_loss)
            var_loss = variance_loss(formatted_loss) * variance_loss_strength
            # alpha_var_loss = alpha_variance_loss(alpha) * alpha_linearity_loss_strength
            div_loss = (
                divergence_loss(x, model.learnable_beta, folds)
                * divergence_loss_strength
            )
            l = l_infty_loss + var_loss + div_loss  # + alpha_var_loss
            # debug_data_current_epoch = {
            #     "x": x,
            #     "t": t,
            #     "output": output,
            #     "alpha": alpha,
            #     "formatted_loss": formatted_loss,
            #     "l_infty_loss": l_infty_loss,
            #     "var_loss": var_loss,
            #     "div_loss": div_loss,
            #     "l": l,
            #     "model_state_dict": model.state_dict(),
            # }

            if (
                l_infty_loss.isnan().any()
                or var_loss.isnan().any()
                or div_loss.isnan().any()
            ):
                raise RuntimeError("NaN detected in loss components")

            # optimizer.zero_grad()
            # accelerator.backward(l)

            # if we get here, then loss was fine and no NaN detected in gradients either
            # debug_data_past_epoch = debug_data_current_epoch

            body_optimizer.zero_grad()
            schedule_optimizer.zero_grad()
            accelerator.backward(l)
            if accelerator.sync_gradients and grad_clip_norm is not None:
                accelerator.clip_grad_norm_(model.parameters(), grad_clip_norm)
            body_optimizer.step()
            if not skip_schedule_optim:
                schedule_optimizer.step()

            accelerator.log(
                {
                    "loss": l.item(),
                    "l_infty_loss": l_infty_loss.item(),
                    "var_loss": var_loss.item(),
                    "div_loss": div_loss.item(),
                    # "alpha_var_loss": alpha_var_loss.item(),
                    "beta_1": model.beta_1(x.shape[-1], device=accelerator.device),
                },
                step=epoch,
            )
            pbar_description = f"Loss: {l.item():.4f}"
            if epoch % save_every == 0:
                pbar_description += " - Saving checkpoint"
                pbar.set_description(pbar_description)
                checkpoint_manager.save(save_dir, epoch)
            else:
                pbar.set_description(pbar_description)
        accelerator.end_training()

        checkpoint_manager.save(save_dir, epoch)
    except KeyboardInterrupt:
        print("Training interrupted. Saving checkpoint...")
        checkpoint_manager.save(save_dir, epoch)
        accelerator.end_training()
        raise KeyboardInterrupt("Training interrupted by user.")
    except RuntimeError as e:
        print(f"Runtime error occurred: {e}")
        # torch.save(debug_data_past_epoch, "debug_data_past_epoch.pt")
        # torch.save(debug_data_current_epoch, "debug_data_current_epoch.pt")
