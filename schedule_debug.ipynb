{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6fbcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from src.datasets.shakespeare.shakespeare import ShakespeareDataset\n",
    "from torch.nn import functional as F\n",
    "from src.training.discrete_loss import (\n",
    "    #alpha_variance_loss,\n",
    "    divergence_loss,\n",
    "    format_loss,\n",
    "    loss,\n",
    "    variance_loss,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf4d8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /media/john/Tertiary/Data/huggingface/modules/datasets_modules/datasets/karpathy--tiny_shakespeare/b5b13969f09fe8707337f6cb296314fbe06960bd9a868dca39e713e163d27b5e (last modified on Sun Jul 20 14:43:33 2025) since it couldn't be found locally at karpathy/tiny_shakespeare, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model...\n",
      "Starting checkpoint loading process...\n",
      "Attempting to load checkpoint from epoch 11\n",
      "Finished loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from matplotlib import pyplot as plt\n",
    "from safetensors.torch import load_file\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.datasets.discrete_helper import collate_fn\n",
    "from src.inference.discrete_inference import bayesian_inference, dis_t\n",
    "from src.nn.layers.learnable_schedule import LearnableBetaScheduleNI\n",
    "from src.nn.models.discrete_model import DiscreteModel\n",
    "from src.tokenizers.ascii.ascii_tokenizer import ASCIITokenizer as Tokenizer\n",
    "from src.training.checkpoint import CheckpointManager, CheckpointMetadata\n",
    "from src.training.training import train_discrete_model\n",
    "\n",
    "accelerator = Accelerator(project_dir=\"./runs/shakespeare\")\n",
    "tokenizer = Tokenizer()\n",
    "max_seq_len = 32\n",
    "folds = 8\n",
    "dataset = ShakespeareDataset(\n",
    "    tokenizer=tokenizer, max_length=max_seq_len, folds=folds,\n",
    ")\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_seq_len\": max_seq_len,\n",
    "    \"K\": tokenizer.vocab_size(),\n",
    "    \"hidden_dim\": 512,\n",
    "    \"num_heads\": 8,\n",
    "    \"layers\": 5,\n",
    "    # beta_1 from https://arxiv.org/html/2407.20294v2 equation 5\n",
    "    \"reference_beta_1\": 20.4054 / tokenizer.vocab_size(),\n",
    "    \"learner_weight\": 1.0,\n",
    "    \"freeze_body\": False,\n",
    "}\n",
    "model = DiscreteModel(**model_kwargs)\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    \"lr\": 3e-5,\n",
    "}\n",
    "body_opt = torch.optim.Adam(\n",
    "    model.body.parameters(), **optimizer_kwargs  # pyright: ignore[reportArgumentType]\n",
    ")\n",
    "schedule_opt = torch.optim.Adam(\n",
    "    model.learnable_beta.parameters(),\n",
    "    **optimizer_kwargs,  # pyright: ignore[reportArgumentType]\n",
    ")\n",
    "\n",
    "metadata = CheckpointMetadata(\n",
    "    model_kwargs=model_kwargs,\n",
    "    optimizer_kwargs=optimizer_kwargs,\n",
    "    is_fsdp=hasattr(accelerator.state, \"fsdp_plugin\")\n",
    "    and accelerator.state.fsdp_plugin is not None,\n",
    "    num_accelerators=accelerator.num_processes,\n",
    ")\n",
    "\n",
    "checkpoint_dir = \"./checkpoint/shakespeare_ASCII_shannon\"\n",
    "checkpoint_manager = CheckpointManager()\n",
    "print(\"Preparing model...\")\n",
    "checkpoint_manager.prepare(model, body_opt, schedule_opt, accelerator, metadata)\n",
    "print(\"Starting checkpoint loading process...\")\n",
    "checkpoint_manager.load(checkpoint_dir, error_if_not_exists=True)\n",
    "print(\"Finished loading checkpoint\")\n",
    "\n",
    "model, opt = checkpoint_manager.model, checkpoint_manager.body_optimizer\n",
    "\n",
    "assert model is not None\n",
    "assert isinstance(model, DiscreteModel)\n",
    "\n",
    "schedule: LearnableBetaScheduleNI = model.learnable_beta\n",
    "\n",
    "assert isinstance(schedule, LearnableBetaScheduleNI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e2e8e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schedule.forward(torch.tensor([0.0], device=\"cuda\"), tokenizer.vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e548a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
